
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>A Frequentist Approach to Bayesian Inference</title>
    <meta name="description" content="">
    <meta name="author" content="Jason Liszka">

    <!-- Enable responsive viewport -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="/assets/themes/twitter/bootstrap/css/bootstrap.2.2.2.min.css" rel="stylesheet">
    <link href="/assets/themes/twitter/css/style.css?body=1" rel="stylesheet" type="text/css" media="all">
    <link href="/assets/themes/twitter/css/pygment.css" rel="stylesheet" type="text/css" media="all">

    <!-- Le fav and touch icons -->
  <!-- Update these with your own images
    <link rel="shortcut icon" href="images/favicon.ico">
    <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
  -->

    <!-- atom & rss feed -->
    <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
    <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </head>

  <body>
    <div class="navbar">
      <div class="navbar-inner">
        <div class="container-narrow">
          <a class="brand" href="/">A Gentleman and a Scala</a>
          <ul class="nav">
            
            
            


  
    
      
      	
      	<li><a href="/archive.html">Archive</a></li>
      	
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
      	
      	<li><a href="/tags.html">Tags</a></li>
      	
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  



          </ul>
        </div>
      </div>
    </div>

    <div class="container-narrow">

      <div class="content">
        
<div class="page-header">
  <h1>A Frequentist Approach to Bayesian Inference </h1>
</div>

<div class="row-fluid post-full">
  <div class="span12">
    <div class="date">
      <span>11 August 2013</span>
    </div>
    <div class="content">
      <p>Say you have a biased coin, but you don&#8217;t know what the &#8220;true&#8221; bias is. You flip the coin 10 times and observe 8 heads. What can you say now about the true bias?</p>

<p>This sounds like a classic problem in Bayesian inference. You might think that a problem like this would not be amenable to a frequentist approach. In fact, I&#8217;ll show the contrary, that a frequentist approach can be quite illuminating, especially if you (like me) are a little wobbly on your Bayesian inference.</p>

<h3 id='the_experiment'>The experiment</h3>

<p>The frequentist approach to this problem is to run an experiment consisting of a large number of trials. A single trial will look like this:</p>

<ol>
<li>Choose a bias at random</li>

<li>Flip a coin with that bias 10 times</li>
</ol>

<p>After you run this experiment, say, 10,000 times, you look at all the times you got 8 heads, and see what biases were involved in those trials. The percent of the time each bias comes up in this subset of trials gives you the probability (the &#8220;posterior probability&#8221;) that that bias is the &#8220;true&#8221; bias.</p>

<h3 id='the_prior'>The prior</h3>

<p>When you start to code this up, one question jumps out: In step 1, when you choose a bias &#8220;at random,&#8221; what distribution do you draw it from?</p>

<p>The only answer that makes sense is the uniform distribution between 0 and 1, reflecting no particular prior knowledge about what the true bias is. Later on we&#8217;ll see what happens to the posterior distribution when you start with different priors.</p>

<h3 id='the_code'>The code</h3>

<p>First, a case class that represents the outcome of one trial:</p>
<div class='highlight'><pre><code class='scala'><span class='k'>case</span> <span class='k'>class</span> <span class='nc'>Trial</span><span class='o'>(</span><span class='n'>bias</span><span class='k'>:</span> <span class='kt'>Double</span><span class='o'>,</span> <span class='n'>flips</span><span class='k'>:</span> <span class='kt'>List</span><span class='o'>[</span><span class='kt'>Boolean</span><span class='o'>])</span>
</code></pre></div>
<p>And the experiment itself:</p>
<div class='highlight'><pre><code class='scala'><span class='k'>val</span> <span class='n'>experiment</span><span class='k'>:</span> <span class='kt'>Distribution</span><span class='o'>[</span><span class='kt'>Trial</span><span class='o'>]</span> <span class='k'>=</span> <span class='o'>{</span>
  <span class='k'>for</span> <span class='o'>{</span>
    <span class='n'>bias</span> <span class='k'>&lt;-</span> <span class='n'>uniform</span>
    <span class='n'>flips</span> <span class='k'>&lt;-</span> <span class='n'>bernoulli</span><span class='o'>(</span><span class='n'>bias</span><span class='o'>).</span><span class='n'>repeat</span><span class='o'>(</span><span class='mi'>10</span><span class='o'>)</span>
  <span class='o'>}</span> <span class='k'>yield</span> <span class='nc'>Trial</span><span class='o'>(</span><span class='n'>bias</span><span class='o'>,</span> <span class='n'>flips</span><span class='o'>)</span>
<span class='o'>}</span>
</code></pre></div>
<p>The bias is drawn from the uniform distribution and feeds into a bernoulli distribution, which represents a coin flip, with <code>true</code> corresponding to heads and <code>false</code> corresponding to tails.</p>

<p>Now let&#8217;s analyze the experiment. Recall we only care about the time we got 8 heads.</p>
<div class='highlight'><pre><code class='scala'><span class='k'>val</span> <span class='n'>posterior</span><span class='k'>:</span> <span class='kt'>Distribution</span><span class='o'>[</span><span class='kt'>Double</span><span class='o'>]</span> <span class='k'>=</span> <span class='o'>{</span>
  <span class='n'>experiment</span><span class='o'>.</span><span class='n'>given</span><span class='o'>(</span><span class='k'>_</span><span class='o'>.</span><span class='n'>flips</span><span class='o'>.</span><span class='n'>count</span><span class='o'>(</span><span class='k'>_</span> <span class='o'>==</span> <span class='kc'>true</span><span class='o'>)</span> <span class='o'>==</span> <span class='mi'>8</span><span class='o'>).</span><span class='n'>map</span><span class='o'>(</span><span class='k'>_</span><span class='o'>.</span><span class='n'>bias</span><span class='o'>)</span>
<span class='o'>}</span>
</code></pre></div>
<p>Let&#8217;s see what it looks like:</p>

<pre><code>scala&gt; posterior.bucketedHist(0, 1, 20)
0.00  0.00% 
0.05  0.00% 
0.10  0.00% 
0.15  0.00% 
0.20  0.01% 
0.25  0.03% 
0.30  0.09% 
0.35  0.18% 
0.40  0.63% 
0.45  1.37% #
0.50  2.32% ##
0.55  3.85% ###
0.60  6.69% ######
0.65  9.35% #########
0.70 12.73% ############
0.75 15.49% ###############
0.80 16.91% ################
0.85 15.08% ###############
0.90 10.60% ##########
0.95  4.48% ####
1.00  0.19% </code></pre>

<p>That looks pretty good! It&#8217;s clear that 0.8 is the most likely bias (in technical terms, the &#8220;maximum likelihood estimate&#8221;).</p>

<p>Alright, now suppose I flip the same coin 10 more times and get only 6 heads this time. I should be able to model it the same way, only using <code>posterior</code> as my new prior.</p>
<div class='highlight'><pre><code class='scala'><span class='k'>val</span> <span class='n'>experiment2</span> <span class='k'>=</span> <span class='o'>{</span>
  <span class='k'>for</span> <span class='o'>{</span>
    <span class='n'>bias</span> <span class='k'>&lt;-</span> <span class='n'>posterior</span>
    <span class='n'>flips</span> <span class='k'>&lt;-</span> <span class='n'>bernoulli</span><span class='o'>(</span><span class='n'>bias</span><span class='o'>).</span><span class='n'>repeat</span><span class='o'>(</span><span class='mi'>10</span><span class='o'>)</span>
  <span class='o'>}</span> <span class='k'>yield</span> <span class='nc'>Trial</span><span class='o'>(</span><span class='n'>bias</span><span class='o'>,</span> <span class='n'>flips</span><span class='o'>)</span>
<span class='o'>}</span>
<span class='k'>val</span> <span class='n'>posterior2</span><span class='k'>:</span> <span class='kt'>Distribution</span><span class='o'>[</span><span class='kt'>Double</span><span class='o'>]</span> <span class='k'>=</span> <span class='o'>{</span>
  <span class='n'>experiment2</span><span class='o'>.</span><span class='n'>given</span><span class='o'>(</span><span class='k'>_</span><span class='o'>.</span><span class='n'>flips</span><span class='o'>.</span><span class='n'>count</span><span class='o'>(</span><span class='k'>_</span> <span class='o'>==</span> <span class='kc'>true</span><span class='o'>)</span> <span class='o'>==</span> <span class='mi'>6</span><span class='o'>).</span><span class='n'>map</span><span class='o'>(</span><span class='k'>_</span><span class='o'>.</span><span class='n'>bias</span><span class='o'>)</span>
<span class='o'>}</span>
</code></pre></div>
<pre><code>scala&gt; p2.bucketedHist(0, 1, 20)
0.00  0.00% 
0.05  0.00% 
0.10  0.00% 
0.15  0.00% 
0.20  0.00% 
0.25  0.00% 
0.30  0.00% 
0.35  0.18% 
0.40  0.55% 
0.45  1.84% #
0.50  4.25% ####
0.55  7.79% #######
0.60 12.91% ############
0.65 17.86% #################
0.70 19.41% ###################
0.75 17.66% #################
0.80 11.37% ###########
0.85  4.98% ####
0.90  1.14% #
0.95  0.06% 
1.00  0.00% </code></pre>

<p>Great, exactly what you would expect. Playing around with it, you can see that as you do more trials and observe more outcomes, the distribution gets narrower.</p>

<p>To make that easier I&#8217;m going to abstract this out to a method.</p>
<div class='highlight'><pre><code class='scala'><span class='k'>trait</span> <span class='nc'>Distribution</span><span class='o'>[</span><span class='kt'>A</span><span class='o'>]</span> <span class='o'>{</span>
  <span class='c1'>// ...</span>
  <span class='k'>def</span> <span class='n'>posterior</span><span class='o'>[</span><span class='kt'>B</span><span class='o'>](</span><span class='n'>experiment</span><span class='k'>:</span> <span class='kt'>A</span> <span class='o'>=&gt;</span> <span class='nc'>Distribution</span><span class='o'>[</span><span class='kt'>B</span><span class='o'>])</span>
                   <span class='o'>(</span><span class='n'>observed</span><span class='k'>:</span> <span class='kt'>B</span> <span class='o'>=&gt;</span> <span class='nc'>Boolean</span><span class='o'>)</span><span class='k'>:</span> <span class='kt'>Distribution</span><span class='o'>[</span><span class='kt'>A</span><span class='o'>]</span> <span class='k'>=</span> <span class='o'>{</span>
    <span class='k'>case</span> <span class='k'>class</span> <span class='nc'>Trial</span><span class='o'>(</span><span class='n'>a</span><span class='k'>:</span> <span class='kt'>A</span><span class='o'>,</span> <span class='n'>outcome</span><span class='k'>:</span> <span class='kt'>B</span><span class='o'>)</span>
    <span class='k'>val</span> <span class='n'>d</span> <span class='k'>=</span> <span class='k'>for</span> <span class='o'>{</span>
      <span class='n'>a</span> <span class='k'>&lt;-</span> <span class='k'>this</span>
      <span class='n'>e</span> <span class='k'>&lt;-</span> <span class='n'>experiment</span><span class='o'>(</span><span class='n'>a</span><span class='o'>)</span>
    <span class='o'>}</span> <span class='k'>yield</span> <span class='nc'>Trial</span><span class='o'>(</span><span class='n'>a</span><span class='o'>,</span> <span class='n'>e</span><span class='o'>)</span>
    <span class='n'>d</span><span class='o'>.</span><span class='n'>given</span><span class='o'>(</span><span class='n'>t</span> <span class='k'>=&gt;</span> <span class='n'>observed</span><span class='o'>(</span><span class='n'>t</span><span class='o'>.</span><span class='n'>outcome</span><span class='o'>)).</span><span class='n'>map</span><span class='o'>(</span><span class='k'>_</span><span class='o'>.</span><span class='n'>a</span><span class='o'>)</span>
  <span class='o'>}</span>
<span class='o'>}</span>
</code></pre></div>
<p>This method treats <code>this</code> as a prior and returns the posterior distribution after running an experiment that depends on values sampled from <code>this</code>. The function <code>observed</code> indicates what outcomes were actually observed and which were not.</p>

<p>So now our coin-flip experiment becomes:</p>
<div class='highlight'><pre><code class='scala'><span class='k'>val</span> <span class='n'>p1</span> <span class='k'>=</span> <span class='n'>uniform</span><span class='o'>.</span><span class='n'>posterior</span><span class='o'>(</span><span class='n'>bias</span> <span class='k'>=&gt;</span> <span class='n'>bernoulli</span><span class='o'>(</span><span class='n'>bias</span><span class='o'>).</span><span class='n'>repeat</span><span class='o'>(</span><span class='mi'>10</span><span class='o'>))(</span><span class='k'>_</span><span class='o'>.</span><span class='n'>count</span><span class='o'>(</span><span class='k'>_</span> <span class='o'>==</span> <span class='kc'>true</span><span class='o'>)</span> <span class='o'>==</span> <span class='mi'>8</span><span class='o'>)</span>
<span class='k'>val</span> <span class='n'>p2</span> <span class='k'>=</span> <span class='n'>p1</span><span class='o'>.</span><span class='n'>posterior</span><span class='o'>(</span><span class='n'>bias</span> <span class='k'>=&gt;</span> <span class='n'>bernoulli</span><span class='o'>(</span><span class='n'>bias</span><span class='o'>).</span><span class='n'>repeat</span><span class='o'>(</span><span class='mi'>10</span><span class='o'>))(</span><span class='k'>_</span><span class='o'>.</span><span class='n'>count</span><span class='o'>(</span><span class='k'>_</span> <span class='o'>==</span> <span class='kc'>true</span><span class='o'>)</span> <span class='o'>==</span> <span class='mi'>6</span><span class='o'>)</span>
</code></pre></div>
<p>We can eyeball that <code>p2</code> gives the same result as flipping a coin 20 times and observing 14 heads:</p>

<pre><code>scala&gt; uniform.posterior(bias =&gt; bernoulli(bias).repeat(20))(_.count(_ == true) == 14).bucketedHist(0, 1, 20)
0.00  0.00% 
0.05  0.00% 
0.10  0.00% 
0.15  0.00% 
0.20  0.00% 
0.25  0.00% 
0.30  0.06% 
0.35  0.13% 
0.40  0.53% 
0.45  1.83% #
0.50  3.88% ###
0.55  8.20% ########
0.60 13.40% #############
0.65 17.61% #################
0.70 19.28% ###################
0.75 18.30% ##################
0.80 11.33% ###########
0.85  4.40% ####
0.90  0.98% 
0.95  0.07% 
1.00  0.00% </code></pre>

<p>And that flipping the coin more times gives a narrower distribution:</p>

<pre><code>scala&gt; uniform.posterior(bias =&gt; bernoulli(bias).repeat(100))(_.count(_ == true) == 72).bucketedHist(0, 1, 20)
0.00  0.00% 
0.05  0.00% 
0.10  0.00% 
0.15  0.00% 
0.20  0.00% 
0.25  0.00% 
0.30  0.00% 
0.35  0.00% 
0.40  0.00% 
0.45  0.00% 
0.50  0.01% 
0.55  0.14% 
0.60  2.35% ##
0.65 15.58% ###############
0.70 39.42% #######################################
0.75 33.53% #################################
0.80  8.60% ########
0.85  0.37% 
0.90  0.00% 
0.95  0.00% 
1.00  0.00% </code></pre>

<p>Pretty neat!</p>

<h3 id='other_priors'>Other priors</h3>

<p>Now let&#8217;s play around with other priors and see what posterior distributions come out.</p>

<p>Suppose we start with some knowledge that coin favors tails over heads. So we know the bias is less than 0.5. We&#8217;ll model this with a uniform distribution between 0 and 0.5.</p>

<pre><code>scala&gt; val prior = uniform.given(_ &lt; 0.5)
prior: Distribution[Double] = &lt;distribution&gt;

scala&gt; prior.posterior(bias =&gt; bernoulli(bias).repeat(10))(_.count(_ == true) == 8).bucketedHist(0, 1, 20, roundDown = true)
0.00  0.00% 
0.05  0.00% 
0.10  0.00% 
0.15  0.01% 
0.20  0.33% 
0.25  1.47% #
0.30  4.42% ####
0.35 11.61% ###########
0.40 26.93% ##########################
0.45 55.23% #######################################################
0.50  0.00% 
0.55  0.00% 
0.60  0.00% 
0.65  0.00% 
0.70  0.00% 
0.75  0.00% 
0.80  0.00% 
0.85  0.00% 
0.90  0.00% 
0.95  0.00% 
1.00  0.00% </code></pre>

<p>Makes sense, all the probabily mass crowds as close to 0.5 as it can. (<code>roundDown = true</code> labels each bucket with the minimum value of the bucket instead of the middle value. So here, the &#8220;0.45&#8221; bucket includes all values between 0.45 and 0.50, whereas normally that bucket would include values between 0.425 and 0.475. I just did this to align the bucket boundaries with where I know the cutoff in the distribution is.)</p>

<p>OK, let&#8217;s say someone tells us that they don&#8217;t know what the bias is, but it is definitely <em>not</em> between 0.7 and 0.8.</p>

<pre><code>scala&gt; val prior = uniform.given(x =&gt; x &lt;= 0.7 || x &gt;= 0.8)
prior: Distribution[Double] = &lt;distribution&gt;

scala&gt; prior.posterior(bias =&gt; bernoulli(bias).repeat(10))(_.count(_ == true) == 8).bucketedHist(0, 1, 20, roundDown = true)
0.00  0.00% 
0.05  0.00% 
0.10  0.00% 
0.15  0.00% 
0.20  0.03% 
0.25  0.02% 
0.30  0.20% 
0.35  0.39% 
0.40  1.21% #
0.45  2.79% ##
0.50  4.67% ####
0.55  7.52% #######
0.60 11.68% ###########
0.65 16.20% ################
0.70  0.00% 
0.75  0.00% 
0.80 23.96% #######################
0.85 18.53% ##################
0.90 10.52% ##########
0.95  2.28% ##
1.00  0.00% </code></pre>

<p>Fun! Makes perfect sense though, the prior distribution isn&#8217;t generating any biases between 0.7 and 0.8, so it&#8217;s not going to show up in the results.</p>
    </div>

    

  
    <ul class="tag_box inline">
      <li><i class="icon-tags"></i></li>
      
      


  
     
    	<li><a href="/tags.html#probability-ref">probability <span>3</span></a></li>
    
  



    </ul>
    

    <hr>
    <div class="navigation">
    
    
      <span class="next"><a href="/2013/08/11/a-frequentist-approach-to-probability.html" title="A Frequentist Approach to Probability">A Frequentist Approach to Probability &rarr;</a></span>
    
      <div style="clear: both"></div>
    </div>
    <hr>
    


  <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_developer = 1;
    var disqus_shortname = 'jliszka'; // required: replace example with your forum shortname
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>




  </div>
</div>


      </div>
      <hr>
      <footer>
        <p>&copy; 2013 Jason Liszka
          with help from <a href="http://jekyllbootstrap.com" target="_blank" title="The Definitive Jekyll Blogging Framework">Jekyll Bootstrap</a>
          and <a href="http://twitter.github.com/bootstrap/" target="_blank">Twitter Bootstrap</a>
        </p>
      </footer>

    </div>

    
  </body>
</html>

