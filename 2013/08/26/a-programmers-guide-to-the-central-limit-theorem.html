
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>A Programmer's Guide to the Central Limit Theorem</title>
    <meta name="description" content="">
    <meta name="author" content="Jason Liszka">

    <!-- Enable responsive viewport -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="/assets/themes/twitter/bootstrap/css/bootstrap.2.2.2.min.css" rel="stylesheet">
    <link href="/assets/themes/twitter/css/style.css?body=1&amp;v=2" rel="stylesheet" type="text/css" media="all">
    <link href="/assets/themes/twitter/css/pygment.css" rel="stylesheet" type="text/css" media="all">

    <!-- Le fav and touch icons -->
  <!-- Update these with your own images
    <link rel="shortcut icon" href="images/favicon.ico">
    <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
  -->

    <!-- atom & rss feed -->
    <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
    <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </head>

  <body>
    <div class="navbar">
      <div class="navbar-inner">
        <div class="container-narrow">
          <div class="row-fluid">
            <div class="span9 offset1">
              <a class="brand" href="/">A Gentleman and a Scala</a>
              <ul class="nav">
                
                
                


  
    
      
      	
      	<li><a href="/archive.html">Archive</a></li>
      	
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
      	
      	<li><a href="/tags.html">Tags</a></li>
      	
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  



              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="container-narrow">

      <div class="content">
        
<div class="row-fluid">
  <div class="span9 offset1 post-content post-spacer"></div>
</div>
<div class="row-fluid post-full">
  <div class="span9 offset1 post-content">
    <div class="page-header">
      <h1>A Programmer's Guide to the Central Limit Theorem </h1>
    </div>
    <div class="date">
      <span>26 August 2013</span>
    </div>
    <div class="content">
      
<p>This post is a continuation of a series of posts about exploring probability distributions through code. The first post
is <a href="/2013/08/12/a-frequentist-approach-to-probability.html">here</a>.</p>

<p>In this post I’m going to look at the Central Limit Theorem.</p>

<h3 id="sample-means">Sample means</h3>

<p>Suppose I have a random variable whose underlying distribution is unknown to me. I take sample of a reasonable size (say 100)
and find the mean of the sample. What can I say about the relationship between the true mean and the mean of the sample?</p>

<p>The most comprehensive answer to this is to look at the distribution of the sample mean.</p>

<div class="highlight"><pre><code class="scala"><span class="k">def</span> <span class="n">sampleMean</span><span class="o">(</span><span class="n">d</span><span class="k">:</span> <span class="kt">Distribution</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">n</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">100</span><span class="o">)</span><span class="k">:</span> <span class="kt">Distribution</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="n">d</span><span class="o">.</span><span class="n">repeat</span><span class="o">(</span><span class="n">n</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">sum</span> <span class="o">/</span> <span class="n">n</span><span class="o">)</span>
<span class="o">}</span></code></pre></div>

<p>This method takes a probability distribution and returns the distribution of means of samples from that distribution. You can
specify the sample size, but by default we’ll use 100.</p>

<p>Let’s try it on some of the distributions we’ve <a href="/2013/08/19/climbing-the-probability-distribution-ladder.html">created</a>.</p>

<!-- more -->

<pre><code>scala&gt; sampleMean(uniform).hist
0.40  0.01% 
0.41  0.06% 
0.42  0.36% 
0.43  0.79% 
0.44  1.63% #
0.45  2.95% ##
0.46  5.18% #####
0.47  8.33% ########
0.48 11.43% ###########
0.49 12.80% ############
0.50 14.22% ##############
0.51 12.47% ############
0.52 10.74% ##########
0.53  8.00% ########
0.54  5.47% #####
0.55  2.78% ##
0.56  1.60% #
0.57  0.70% 
0.58  0.32% 
0.59  0.07% 
0.60  0.06% 
</code></pre>

<p>Not surprising. All the sample means are clustered around the true mean (0.5).</p>

<p>Let’s try a couple more.</p>

<pre><code>scala&gt; sampleMean(exponential(1)).hist
0.60  0.00% 
0.65  0.02% 
0.70  0.16% 
0.75  0.69% 
0.80  2.38% ##
0.85  6.68% ######
0.90 13.12% #############
0.95 17.93% #################
1.00 19.21% ###################
1.05 17.27% #################
1.10 11.26% ###########
1.15  6.53% ######
1.20  3.01% ###
1.25  1.28% #
1.30  0.36% 
1.35  0.07% 
1.40  0.02% 
1.45  0.00% 
1.50  0.01% 

scala&gt; sampleMean(chi2(5)).hist
3.90  0.02% 
4.00  0.08% 
4.10  0.14% 
4.20  0.40% 
4.30  0.95% 
4.40  1.89% #
4.50  3.63% ###
4.60  5.68% #####
4.70  8.52% ########
4.80 10.25% ##########
4.90 12.23% ############
5.00 13.18% #############
5.10 11.19% ###########
5.20 10.37% ##########
5.30  7.61% #######
5.40  5.84% #####
5.50  3.67% ###
5.60  2.04% ##
5.70  1.23% #
5.80  0.64% 
5.90  0.29% 
6.00  0.10% 
6.10  0.03% 
6.20  0.02% 
</code></pre>

<p>OK, starting to see a pattern here. Let’s look at some discrete distributions.</p>

<pre><code>scala&gt; sampleMean(bernoulli(0.8).map(_.toDouble).hist
0.68  0.33% 
0.70  0.85% 
0.72  2.14% ##
0.74  5.29% #####
0.76  9.96% #########
0.78 15.81% ###############
0.80 19.27% ###################
0.82 18.74% ##################
0.84 14.75% ##############
0.86  8.14% ########
0.88  3.32% ###
0.90  1.10% #
0.92  0.21% 
0.94  0.03% 

scala&gt; sampleMean(binomial(0.2, 10).map(_.toDouble)).hist
1.50  0.00% 
1.55  0.03% 
1.60  0.08% 
1.65  0.32% 
1.70  1.00% #
1.75  2.35% ##
1.80  4.61% ####
1.85  7.88% #######
1.90 11.20% ###########
1.95 14.62% ##############
2.00 16.07% ################
2.05 14.82% ##############
2.10 11.42% ###########
2.15  7.43% #######
2.20  4.60% ####
2.25  2.15% ##
2.30  0.97% 
2.35  0.34% 
2.40  0.09% 
2.45  0.02% 
2.50  0.00% 

scala&gt; sampleMean(geometric(0.2).map(_.toDouble)).hist
2.40  0.01% 
2.60  0.09% 
2.80  0.29% 
3.00  1.14% #
3.20  3.59% ###
3.40  7.31% #######
3.60 12.92% ############
3.80 16.75% ################
4.00 17.69% #################
4.20 15.31% ###############
4.40 11.16% ###########
4.60  7.03% #######
4.80  3.84% ###
5.00  1.70% #
5.20  0.79% 
5.40  0.29% 
5.60  0.08% 
5.80  0.00% 
6.00  0.01% 

scala&gt; sampleMean(poisson(5).map(_.toDouble)).hist
4.30  0.15% 
4.40  0.43% 
4.50  1.34% #
4.60  3.42% ###
4.70  7.19% #######
4.80 12.04% ############
4.90 15.49% ###############
5.00 18.02% ##################
5.10 15.82% ###############
5.20 11.99% ###########
5.30  7.37% #######
5.40  4.03% ####
5.50  1.81% #
5.60  0.64% 
5.70  0.16% 
</code></pre>

<p>All of these distributions look vaguely normal and they’re all clustered around the mean of the underlying distribution.</p>

<h3 id="the-central-limit-theorem">The Central Limit Theorem</h3>

<p>Surprise! That little observation was basically a statement of the Central Limit Theorem — means samples of a reasonable
size drawn from any probability
distribution will be normally distributed around the mean of the distribution. The Central Limit Theorem even
tells you how to compute the standard deviation of this distribution: it’s just the standard deviation of the
underlying distribution divided by the square root of the sample size.</p>

<script type="math/tex; mode=display">
%<![CDATA[
\bar{\sigma} = \frac{\sigma}{\sqrt{n}}
%]]>
</script>

<p>This quantity, the standard deviation of the distribution of sample means, is also known as the
<a href="http://en.wikipedia.org/wiki/Standard_error">standard error</a>.
It’s not a terribly suggestive name, but it might help
to think of the “error” as the difference between the sample mean and the true mean.</p>

<p>Terminology aside, the most remarkable fact is that this works no matter what distribution you try it on.</p>

<p>Let’s revisit each of the examples above and see if it pans out.</p>

<pre><code>scala&gt; uniform.ev
res0: Double = 0.49596431533522234

scala&gt; uniform.stdev
res1: Double = 0.290545289200811
</code></pre>

<p>So the Central Limit Theorem would predict that <code>sampleMean(uniform)</code> will have mean 0.5 and stdev <script type="math/tex">0.29 / \sqrt{100} = 0.029</script>.</p>

<pre><code>scala&gt; sampleMean(uniform).ev
res2: Double = 0.49968258747065275

scala&gt; sampleMean(uniform).stdev
res3: Double = 0.028763987024078164
</code></pre>

<p>Wow, OK! Let’s keep going. (I’m going to omit the mean calculations because it seems like an obvious fact. So I’m just
looking to see that the standard error is 1/10th the standard deviation of the underlying distribution.)</p>

<pre><code>scala&gt; exponential(1).stdev
res0: Double = 0.9971584111946743

scala&gt; sampleMean(exponential(1)).stdev
res2: Double = 0.09987372019328666

scala&gt; chi2(5).stdev
res3: Double = 3.1542391941582766

scala&gt; sampleMean(chi2(5)).stdev
res4: Double = 0.3180622311083607

scala&gt; binomial(0.2, 10).map(_.toDouble).stdev
res5: Double = 1.2733502267640227

scala&gt; sampleMean(binomial(0.2, 10).map(_.toDouble)).stdev
res6: Double = 0.12688793641635224

scala&gt; poisson(5).map(_.toDouble).stdev
res7: Double = 2.2423514867210077

scala&gt; sampleMean(poisson(5).map(_.toDouble)).stdev
res8: Double = 0.2251131007715896

scala&gt; geometric(0.2).map(_.toDouble).stdev
res9: Double = 4.439230239579939

scala&gt; sampleMean(geometric(0.2).map(_.toDouble)).stdev
res10: Double = 0.4428929231078312
</code></pre>

<p>And one more with a different sample size:</p>

<pre><code>scala&gt; sampleMean(geometric(0.2).map(_.toDouble), n = 625).stdev
res11: Double = 0.17952533894556522

scala&gt; geometric(0.2).stdev / 25
res12: Double = 0.17756920958319758
</code></pre>

<p>Crazy! OK that’s enough experimental proof for me.</p>

<h3 id="so-what">So what?</h3>

<p>Experimental analysis leans heavily on the Central Limit Theorem. A common question in experimental analysis is whether
a sample is likely to have been drawn from a particular probability distribution. Since you can always treat sample means
as normally distributed, you don’t need to perform a different analysis for every
type of distribution you might encounter. All you need to know is how to work with the normal distribution.</p>

<p>You’ve probably seen <a href="http://en.wikipedia.org/wiki/Standard_deviation">this diagram</a> before:</p>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Standard_deviation_diagram.svg/325px-Standard_deviation_diagram.svg.png" alt="the normal distribution" /></p>

<p>This is what we’re working with. A value drawn from a normal distribution will be within 2 standard deviations of the
mean 96% of the time. Since sample means are normally distributed around the true mean, sample means will be within 2
standard errors of the true mean 96% of the time. If a sample mean is more than 2 standard deviations away from the true
mean, the sample is unlikely to have been drawn from that distribution.</p>

<h3 id="an-example">An example</h3>

<p>Let’s look at an example where we can put the Central Limit Theorem to good use.
Suppose your friend tells you he has a fair coin and offers to play a game.
You pay him $1 to play, and he flips his coin until it comes up heads. He gives
you $1 for every time the coin comes up tails until that happens.</p>

<p>After 100 rounds of this, you notice that you’ve lost $30. Did your friend cheat you?</p>

<p>In standard experimental analysis terms, the null hypothesis is that your friend has a fair coin. You can reject the
null hypothesis if you can show that there is less than, say, a 5% chance of losing $30 after 100 rounds.</p>

<p>You can model the distribution of outcomes for a single round of the game as follows:</p>

<div class="highlight"><pre><code class="scala"><span class="k">val</span> <span class="n">d</span> <span class="k">=</span> <span class="n">geometric</span><span class="o">(</span><span class="mf">0.5</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span> <span class="o">-</span> <span class="mf">1.0</span><span class="o">)</span></code></pre></div>

<p><code>geometric(0.5)</code> models your winnings and <code>- 1.0</code> represents the cost to play the round. The expected value
and standard deviation of this distribution are:</p>

<pre><code>scala&gt; d.ev
res102: Double = -0.0093

scala&gt; d.stdev
res105: Double = 1.406354208583263
</code></pre>

<p>We’ll call that 0 and 1.4. You have a sample of 100 rounds and an average loss of $0.30 per round. What is the probability
that 100 samples from <code>d</code> would have a mean of -0.3? Well, the distribution of sample means has mean 0 and
standard deviation <script type="math/tex">1.4 / \sqrt{100} = 0.14</script>. So your sample mean of -0.3 is more than 2 standard deviations
away from the average sample mean, which we know will happen less than 5% of the time. So we can reject the null hypothesis.</p>

<p>We can also calculate the probability directly against the distribution of sample means.</p>

<pre><code>scala&gt; sampleMean(d, n = 100).pr(_ &lt; -0.3)
res0: Double = 0.0104
</code></pre>

<p>It’s worth pointing out that this is a one-tailed test (I’m not considering the possibility that we’d see a gain of
$0.30 per round) because I have no reason to suspect that my friend has rigged the game in my favor.</p>

<h3 id="another-example">Another example</h3>

<p>Let’s say you have a website, and you want to know whether making your big green
“Sign Up!” button red instead of green would increase the percent of people who click the button.
Historically, you know that 5.8% of visitors to your site click the (green) button.</p>

<p>So one day you make the button red and keep track of the fraction of visitors who click on it. After some period of time
you observe that 53 out of 810 visitors clicked the (red) button. That’s 6.5%, a decent improvement! (Some would say it’s a 12%
improvement; others a 0.7% improvement. Potato, potato.) But is this difference something we’re likely to observe just by
chance, or was making the button red a meaningful change?</p>

<p>We can model the number of clicks as a Bernoulli distribution with a 5.8% success probability.</p>

<pre><code>scala&gt; val d = bernoulli(0.058).map(_.toDouble)
d: Distribution[Double] = &lt;distribution&gt;

scala&gt; d.stdev
res0: Double = 0.24062616233485523
</code></pre>

<p>We want to know the probability of seeing a 6.5% success rate in a sample of 810 visitors. Since we know the standard
deviation of <code>d</code>, we can apply the Central Limit Theorem to find the standard error for a sample of size 810,
just by dividing by <script type="math/tex">\sqrt{810} = 28.5</script>.</p>

<pre><code>scala&gt; d.stdev / math.sqrt(810)
res2: Double = 0.008313053843054703
</code></pre>

<p>Well, OK. Our difference of 0.7% is less than one standard deviation from the mean, which means it’s pretty likely to happen
just by chance. In other words, we can’t reject the null hypothesis that making the button red did not change the rate at
which people click on it.</p>

<p>To illustrate this further, here’s what the distribution of sample means for samples of size 810 looks like:</p>

<pre><code>scala&gt; sampleMean(d, 810).hist
0.0350  0.15% 
0.0375  0.41% 
0.0400  1.20% #
0.0425  2.01% ##
0.0450  3.58% ###
0.0475  5.96% #####
0.0500  7.55% #######
0.0525  9.62% #########
0.0550 11.49% ###########
0.0575 12.08% ############
0.0600 11.23% ###########
0.0625 10.42% ##########
0.0650  8.32% ########
0.0675  5.70% #####
0.0700  3.97% ###
0.0725  2.90% ##
0.0750  1.56% #
0.0775  0.84% 
0.0800  0.47% 
</code></pre>

<p>Seeing 6.5% as the mean of a sample of size 810 is totally within the fat part
of the distribution. Let’s see just how likely a difference of 0.7% really is:</p>

<pre><code>scala&gt; sampleMean(d, 810).pr(c =&gt; c &lt; 0.051 || c &gt; 0.065)
res3: Double = 0.4074
</code></pre>

<p>Pretty likely! (A difference of -0.7% would be just as surprising, so we have to count that too.)</p>

<p>OK, let’s say instead we had observed 530 clicks out of 8100 visitors – it’s the same 6.5% success rate, just with 10 times as many samples.
What does that do to our analysis? Well, first of all, we can expect the standard error
to be much smaller, since we’re dividing by <script type="math/tex">\sqrt{8100} = 90</script> instead of <script type="math/tex">\sqrt{810} = 28.5</script>. </p>

<pre><code>scala&gt; d.stdev / 90
res0: Double = 0.0026411931460171042
</code></pre>

<p>The standard error is 0.26%, putting our 0.7% difference at more than 2.5 standard deviations from the mean, and so in
this case we can reject the null hypothesis and conclude that making the button red was a meaningful change.</p>

<p>To further illustrate this, here is the distribution of sample means for samples of size 8100, on the same scale as before:</p>

<pre><code>scala&gt; sampleMean(d, 8100).bucketedHist(0.035, 0.08, 18)
0.0350  0.00% 
0.0375  0.00% 
0.0400  0.00% 
0.0425  0.00% 
0.0450  0.00% 
0.0475  0.01% 
0.0500  0.44% 
0.0525  4.50% ####
0.0550 19.90% ###################
0.0575 36.42% ####################################
0.0600 28.55% ############################
0.0625  8.70% ########
0.0650  1.41% #
0.0675  0.06% 
0.0700  0.01% 
0.0725  0.00% 
0.0750  0.00% 
0.0775  0.00% 
0.0800  0.00% 

scala&gt; sampleMean(d, 8100).pr(p =&gt; p &lt; 0.051 || p &gt; 0.065)
res4: Double = 0.0062
</code></pre>

<p>The range of the distribution of sample means is much narrower, so much so that a difference of 0.7% (in either direction)
is expected to occur by chance less than 1% of the time.</p>

<h3 id="one-important-exception">One important exception</h3>

<p>It turns out that the Central Limit Theorem doesn’t work with every distribution. This is due to one sneaky fact — sample means
are clustered around the mean of the underlying distribution <em>if it exists</em>. But how can a distribution have no mean?
Well, one common distribution that has no mean is the Pareto distribution. If you tried to calculate it using the usual
methods, it would diverge to infinity.</p>

<p>The means of samples drawn from the Pareto distribution are not normally distributed:</p>

<pre><code>scala&gt; sampleMean(pareto(1)).bucketedHist(0, 20, 20)
 0.0  0.00% 
 1.0  0.00% 
 2.0  0.00% 
 3.0  3.04% ###
 4.0 16.03% ################
 5.0 20.08% ####################
 6.0 16.61% ################
 7.0 12.16% ############
 8.0  8.08% ########
 9.0  5.87% #####
10.0  4.42% ####
11.0  2.90% ##
12.0  2.72% ##
13.0  1.67% #
14.0  1.51% #
15.0  1.31% #
16.0  1.14% #
17.0  0.88% 
18.0  0.83% 
19.0  0.53% 
20.0  0.22% 
</code></pre>

<p>And the standard error is completely meaningless:</p>

<pre><code>scala&gt; sampleMean(pareto(1)).stdev
res0: Double = 157.6098722134558

scala&gt; sampleMean(pareto(1)).stdev
res1: Double = 477.9797744569662
</code></pre>

<p>So the Central Limit Theorem doesn’t apply.</p>

<h3 id="conclusion">Conclusion</h3>

<p>We were able to use the Central Limit Theorem to reason about a samples from various distributions, knowing that
the mean of such a sample is expected to fall within a bell-shaped curve around the mean of the underlying distribution.
This is great because we don’t need special analysis tools for each kind of distribution we might come across. 
No matter what the underlying distribution is, you can always treat sample means as normally distributed.</p>

<p>… unless the underlying distribution has no mean.</p>

<p>We actually run into this all the time at Foursquare. Certain things like, say, the distribution of the number of friends
users have is Pareto-distributed (the vast majority of users have a small number of friends, but some users have thousands of friends).
So if you’re running an experiment that is intended to increase the average number of friends
users have, you’re going to run into trouble. You aren’t going to be able to use standard statistical techniques to analyze
the results of the experiment. Well, actually, you can try, and you’ll get some convincing-looking numbers out, but those
numbers will be completely meaningless!</p>

    </div>

    

  


  <div style="float: right; margin-top: 5px">
  <a href="https://twitter.com/share" class="twitter-share-button" data-url="http://jliszka.github.io/2013/08/26/a-programmers-guide-to-the-central-limit-theorem.html" data-text="A Programmer's Guide to the Central Limit Theorem" data-via="jliszka">Tweet</a>
  <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
</div>




  
    <ul class="tag_box inline">
      <li><i class="icon-tags"></i></li>
      
      


  
     
    	<li><a href="/tags.html#probability-ref">probability <span>8</span></a></li>
    
  



    </ul>
    
    <div style="clear: both"></div>

    <hr>
    <div class="navigation">
    
      <span class="prev"><a href="/2013/08/19/climbing-the-probability-distribution-ladder.html" title="Climbing the probability distribution ladder">&larr; Climbing the probability distribution ladder</a></span>
    
    
      <span class="next"><a href="/2013/09/03/fun-with-bayesian-priors.html" title="Fun with Bayesian Priors">Fun with Bayesian Priors &rarr;</a></span>
    
      <div style="clear: both"></div>
    </div>
    <hr>
    


  <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'jliszka'; // required: replace example with your forum shortname
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>




  </div>

  <div class="span2 sidebar">
    <a href="/"><div id="photo"></div></a>
    <div id="bio">
      <b>Jason Liszka</b><br/>
      <i>Software engineer at Foursquare, CMU alum, Scala fan, new father</i>
    </div>
    <h4>Popular posts</h4>
    <ul>
      <li><a href="/2013/10/01/how-traffic-actually-works.html">How traffic actually works</a></li>
      <li><a href="/2013/08/12/a-frequentist-approach-to-probability.html">A frequentist approach to probability</a></li>
      <li><a href="/2013/10/24/exact-numeric-nth-derivatives.html">Exact numeric nth derivatives</a></li>
      <li><a href="/2013/10/31/infinite-lazy-polynomials.html">Infinite lazy polynomials</a></li>
    </ul>

    <h4>Recent posts</h4>
    <ul>
      
        <li><a href="/2014/06/03/programming-with-futures.html">Programming with futures: patterns and anti-patterns</a></li>
      
        <li><a href="/2014/01/30/good-tech-lead-bad-tech-lead.html">Good Tech Lead, Bad Tech Lead</a></li>
      
        <li><a href="/2013/12/18/bayesian-networks-and-causality.html">Bayesian networks and causality</a></li>
      
        <li><a href="/2013/11/22/unlikely-things-happen-all-the-time.html">Unlikely things happen all the time</a></li>
      
        <li><a href="/2013/11/14/the-foursquare-theorem.html">The Foursquare Theorem</a></li>
      
    </ul>

    <a href="https://twitter.com/jliszka" class="twitter-follow-button" data-show-count="false" data-size="large" data-show-screen-name="false">Follow @jliszka</a>
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>

  </div>
</div>


      </div>
      <hr>
      <footer>
        <p>&copy; 2014 Jason Liszka
          with help from <a href="http://jekyllbootstrap.com" target="_blank" title="The Definitive Jekyll Blogging Framework">Jekyll Bootstrap</a>
          and <a href="http://twitter.github.com/bootstrap/" target="_blank">Twitter Bootstrap</a>
        </p>
      </footer>

    </div>

    


  <script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-42567355-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>



  </body>
</html>

