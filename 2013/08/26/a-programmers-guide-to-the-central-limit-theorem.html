
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>A Programmer's Guide to the Central Limit Theorem</title>
    <meta name="description" content="">
    <meta name="author" content="Jason Liszka">

    <!-- Enable responsive viewport -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="/assets/themes/twitter/bootstrap/css/bootstrap.2.2.2.min.css" rel="stylesheet">
    <link href="/assets/themes/twitter/css/style.css?body=1" rel="stylesheet" type="text/css" media="all">
    <link href="/assets/themes/twitter/css/pygment.css" rel="stylesheet" type="text/css" media="all">

    <!-- Le fav and touch icons -->
  <!-- Update these with your own images
    <link rel="shortcut icon" href="images/favicon.ico">
    <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
  -->

    <!-- atom & rss feed -->
    <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
    <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </head>

  <body>
    <div class="navbar">
      <div class="navbar-inner">
        <div class="container-narrow">
          <a class="brand" href="/">A Gentleman and a Scala</a>
          <ul class="nav">
            
            
            


  
    
      
      	
      	<li><a href="/archive.html">Archive</a></li>
      	
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
      	
      	<li><a href="/tags.html">Tags</a></li>
      	
      
    
  
    
      
    
  
    
      
    
  



          </ul>
        </div>
      </div>
    </div>

    <div class="container-narrow">

      <div class="content">
        
<div class="page-header">
  <h1>A Programmer's Guide to the Central Limit Theorem </h1>
</div>

<div class="row-fluid post-full">
  <div class="span12">
    <div class="date">
      <span>26 August 2013</span>
    </div>
    <div class="content">
      <p>This post is a continuation of a series of posts about exploring probability distributions through code. The first post is <a href='/2013/08/12/a-frequentist-approach-to-probability.html'>here</a>.</p>

<p>In this post I&#8217;m going to look at the Central Limit Theorem.</p>

<h3 id='sample_means'>Sample means</h3>

<p>Suppose I have a random variable whose underlying distribution is unknown to me. I take sample of a reasonable size (say 100) and find the mean of the sample. What can I say about the relationship between the true mean and the mean of the sample?</p>

<p>The most comprehensive answer to this is to look at the distribution of the sample mean.</p>
<div class='highlight'><pre><code class='scala'><span class='k'>def</span> <span class='n'>sampleMean</span><span class='o'>(</span><span class='n'>d</span><span class='k'>:</span> <span class='kt'>Distribution</span><span class='o'>[</span><span class='kt'>Double</span><span class='o'>],</span> <span class='n'>n</span><span class='k'>:</span> <span class='kt'>Int</span> <span class='o'>=</span> <span class='mi'>100</span><span class='o'>)</span><span class='k'>:</span> <span class='kt'>Distribution</span><span class='o'>[</span><span class='kt'>Double</span><span class='o'>]</span> <span class='k'>=</span> <span class='o'>{</span>
  <span class='n'>d</span><span class='o'>.</span><span class='n'>repeat</span><span class='o'>(</span><span class='n'>n</span><span class='o'>).</span><span class='n'>map</span><span class='o'>(</span><span class='k'>_</span><span class='o'>.</span><span class='n'>sum</span> <span class='o'>/</span> <span class='n'>n</span><span class='o'>)</span>
<span class='o'>}</span>
</code></pre></div>
<p>This method takes a probability distribution and returns the distribution of means of samples from that distribution. You can specify the sample size, but by default we&#8217;ll use 100.</p>

<p>Let&#8217;s try it on some of the distributions we&#8217;ve <a href='/2013/08/19/climbing-the-probability-distribution-ladder.html'>created</a>.</p>

<pre><code>scala&gt; sampleMean(uniform).hist
0.40  0.01% 
0.41  0.06% 
0.42  0.36% 
0.43  0.79% 
0.44  1.63% #
0.45  2.95% ##
0.46  5.18% #####
0.47  8.33% ########
0.48 11.43% ###########
0.49 12.80% ############
0.50 14.22% ##############
0.51 12.47% ############
0.52 10.74% ##########
0.53  8.00% ########
0.54  5.47% #####
0.55  2.78% ##
0.56  1.60% #
0.57  0.70% 
0.58  0.32% 
0.59  0.07% 
0.60  0.06% </code></pre>

<p>Not surprising. All the sample means are clustered around the true mean (0.5).</p>

<p>Let&#8217;s try a couple more.</p>

<pre><code>scala&gt; sampleMean(exponential(1)).hist
0.60  0.00% 
0.65  0.02% 
0.70  0.16% 
0.75  0.69% 
0.80  2.38% ##
0.85  6.68% ######
0.90 13.12% #############
0.95 17.93% #################
1.00 19.21% ###################
1.05 17.27% #################
1.10 11.26% ###########
1.15  6.53% ######
1.20  3.01% ###
1.25  1.28% #
1.30  0.36% 
1.35  0.07% 
1.40  0.02% 
1.45  0.00% 
1.50  0.01% 

scala&gt; sampleMean(chi2(5)).hist
3.90  0.02% 
4.00  0.08% 
4.10  0.14% 
4.20  0.40% 
4.30  0.95% 
4.40  1.89% #
4.50  3.63% ###
4.60  5.68% #####
4.70  8.52% ########
4.80 10.25% ##########
4.90 12.23% ############
5.00 13.18% #############
5.10 11.19% ###########
5.20 10.37% ##########
5.30  7.61% #######
5.40  5.84% #####
5.50  3.67% ###
5.60  2.04% ##
5.70  1.23% #
5.80  0.64% 
5.90  0.29% 
6.00  0.10% 
6.10  0.03% 
6.20  0.02% </code></pre>

<p>OK, starting to see a pattern here. Let&#8217;s look at some discrete distributions.</p>

<pre><code>scala&gt; sampleMean(bernoulli(0.8).map(b =&gt; if (b) 1.0 else 0.0)).hist
0.68  0.33% 
0.70  0.85% 
0.72  2.14% ##
0.74  5.29% #####
0.76  9.96% #########
0.78 15.81% ###############
0.80 19.27% ###################
0.82 18.74% ##################
0.84 14.75% ##############
0.86  8.14% ########
0.88  3.32% ###
0.90  1.10% #
0.92  0.21% 
0.94  0.03% 

scala&gt; sampleMean(binomial(0.2, 10).map(_.toDouble)).hist
1.50  0.00% 
1.55  0.03% 
1.60  0.08% 
1.65  0.32% 
1.70  1.00% #
1.75  2.35% ##
1.80  4.61% ####
1.85  7.88% #######
1.90 11.20% ###########
1.95 14.62% ##############
2.00 16.07% ################
2.05 14.82% ##############
2.10 11.42% ###########
2.15  7.43% #######
2.20  4.60% ####
2.25  2.15% ##
2.30  0.97% 
2.35  0.34% 
2.40  0.09% 
2.45  0.02% 
2.50  0.00% 

scala&gt; sampleMean(geometric(0.2).map(_.toDouble)).hist
2.40  0.01% 
2.60  0.09% 
2.80  0.29% 
3.00  1.14% #
3.20  3.59% ###
3.40  7.31% #######
3.60 12.92% ############
3.80 16.75% ################
4.00 17.69% #################
4.20 15.31% ###############
4.40 11.16% ###########
4.60  7.03% #######
4.80  3.84% ###
5.00  1.70% #
5.20  0.79% 
5.40  0.29% 
5.60  0.08% 
5.80  0.00% 
6.00  0.01% 

scala&gt; sampleMean(poisson(5).map(_.toDouble)).hist
4.30  0.15% 
4.40  0.43% 
4.50  1.34% #
4.60  3.42% ###
4.70  7.19% #######
4.80 12.04% ############
4.90 15.49% ###############
5.00 18.02% ##################
5.10 15.82% ###############
5.20 11.99% ###########
5.30  7.37% #######
5.40  4.03% ####
5.50  1.81% #
5.60  0.64% 
5.70  0.16% </code></pre>

<p>All of these distributions look vaguely normal and they&#8217;re all clustered around the mean of the underlying distribution.</p>

<h3 id='the_central_limit_theorem'>The Central Limit Theorem</h3>

<p>Surprise! That little observation was basically a statement of the Central Limit Theorem — means samples of a reasonable size drawn from any probability distribution will be normally distributed around the mean of the distribution. The Central Limit Theorem even tells you how to compute the standard deviation of this distribution: it&#8217;s just the standard deviation of the underlying distribution divided by the square root of the sample size.</p>
<script type='math/tex; mode=display'>
%<![CDATA[
\bar{\sigma} = \frac{\sigma}{\sqrt{n}}
%]]>
</script>
<p>This quantity, the standard deviation of the distribution of sample means, is also known as the <a href='http://en.wikipedia.org/wiki/Standard_error'>standard error</a>. It&#8217;s not a terribly suggestive name, but it might help to think of the &#8220;error&#8221; as the difference between the sample mean and the true mean.</p>

<p>Terminology aside, the most remarkable fact is that this works no matter what distribution you try it on.</p>

<p>Let&#8217;s revisit each of the examples above and see if it pans out.</p>

<pre><code>scala&gt; uniform.ev
res0: Double = 0.49596431533522234

scala&gt; uniform.stdev
res1: Double = 0.290545289200811</code></pre>

<p>So the Central Limit Theorem would predict that <code>sampleMean(uniform)</code> will have mean 0.5 and stdev <script type='math/tex'>0.29 / \sqrt{100} = 0.029</script>.</p>

<pre><code>scala&gt; sampleMean(uniform).ev
res2: Double = 0.49968258747065275

scala&gt; sampleMean(uniform).stdev
res3: Double = 0.028763987024078164</code></pre>

<p>Wow, OK! Let&#8217;s keep going. (I&#8217;m going to omit the mean calculations because it seems like an obvious fact. So I&#8217;m just looking to see that the standard error is 1/10th the standard deviation of the underlying distribution.)</p>

<pre><code>scala&gt; exponential(1).stdev
res0: Double = 0.9971584111946743

scala&gt; sampleMean(exponential(1)).stdev
res2: Double = 0.09987372019328666

scala&gt; chi2(5).stdev
res3: Double = 3.1542391941582766

scala&gt; sampleMean(chi2(5)).stdev
res4: Double = 0.3180622311083607

scala&gt; binomial(0.2, 10).map(_.toDouble).stdev
res5: Double = 1.2733502267640227

scala&gt; sampleMean(binomial(0.2, 10).map(_.toDouble)).stdev
res6: Double = 0.12688793641635224

scala&gt; poisson(5).map(_.toDouble).stdev
res7: Double = 2.2423514867210077

scala&gt; sampleMean(poisson(5).map(_.toDouble)).stdev
res8: Double = 0.2251131007715896

scala&gt; geometric(0.2).map(_.toDouble).stdev
res9: Double = 4.439230239579939

scala&gt; sampleMean(geometric(0.2).map(_.toDouble)).stdev
res10: Double = 0.4428929231078312</code></pre>

<p>And one more with a different sample size:</p>

<pre><code>scala&gt; sampleMean(geometric(0.2).map(_.toDouble), n = 625).stdev
res11: Double = 0.17952533894556522

scala&gt; geometric(0.2).stdev / 25
res12: Double = 0.17756920958319758</code></pre>

<p>Crazy! OK that&#8217;s enough experimental proof for me.</p>

<h3 id='so_what'>So what?</h3>

<p>Experimental analysis leans heavily on the Central Limit Theorem. A common question in experimental analysis is whether a sample is likely to have been drawn from a particular probability distribution. Since you can always treat sample means as normally distributed, you don&#8217;t need to perform a different analysis for every type of distribution you might encounter. All you need to know is how to work with the normal distribution.</p>

<p>You&#8217;ve probably seen <a href='http://en.wikipedia.org/wiki/Standard_deviation'>this diagram</a> before:</p>

<p><img alt='the normal distribution' src='http://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Standard_deviation_diagram.svg/325px-Standard_deviation_diagram.svg.png' /></p>

<p>This is what we&#8217;re working with. A value drawn from a normal distribution will be within 2 standard deviations of the mean 96% of the time. Since sample means are normally distributed around the true mean, sample means will be within 2 standard errors of the true mean 96% of the time. If a sample mean is more than 2 standard deviations away from the true mean, the sample is unlikely to have been drawn from that distribution.</p>

<h3 id='an_example'>An example</h3>

<p>Let&#8217;s look at an example where we can put the Central Limit Theorem to good use. Suppose your friend tells you he has a fair coin and offers to play a game. You pay him $1 to play, and he flips his coin until it comes up heads. He gives you $1 for every time the coin comes up tails until that happens.</p>

<p>After 100 rounds of this, you notice that you&#8217;ve lost $30. Did your friend cheat you?</p>

<p>In standard experimental analysis terms, the null hypothesis is that your friend has a fair coin. You can reject the null hypothesis if you can show that there is less than, say, a 5% chance of losing $30 after 100 rounds.</p>

<p>You can model the distribution of outcomes for a single round of the game as follows:</p>
<div class='highlight'><pre><code class='scala'><span class='k'>val</span> <span class='n'>d</span> <span class='k'>=</span> <span class='n'>geometric</span><span class='o'>(</span><span class='mf'>0.5</span><span class='o'>).</span><span class='n'>map</span><span class='o'>(</span><span class='k'>_</span> <span class='o'>-</span> <span class='mf'>1.0</span><span class='o'>)</span>
</code></pre></div>
<p><code>geometric(0.5)</code> models your winnings and <code>- 1.0</code> represents the cost to play the round. The expected value and standard deviation of this distribution are:</p>

<pre><code>scala&gt; d.ev
res102: Double = -0.0093

scala&gt; d.stdev
res105: Double = 1.406354208583263</code></pre>

<p>We&#8217;ll call that 0 and 1.4. You have a sample of 100 rounds and an average loss of $0.30 per round. What is the probability that 100 samples from <code>d</code> would have a mean of -0.3? Well, the distribution of sample means has mean 0 and standard deviation <script type='math/tex'>1.4 / \sqrt{100} = 0.14</script>. So your sample mean of -0.3 is more than 2 standard deviations away from the average sample mean, which we know will happen less than 5% of the time. So we can reject the null hypothesis.</p>

<p>We can also calculate the probability directly against the distribution of sample means.</p>

<pre><code>scala&gt; sampleMean(d, n = 100).pr(_ &lt; -0.3)
res0: Double = 0.0104</code></pre>

<p>It&#8217;s worth pointing out that this is a one-tailed test (I&#8217;m not considering the possibility that we&#8217;d see a gain of $0.30 per round) because I have no reason to suspect that my friend has rigged the game in my favor.</p>

<h3 id='another_example'>Another example</h3>

<p>Let&#8217;s say you have a website, and you want to know whether making your big green &#8220;Sign Up!&#8221; button red instead of green would increase the percent of people who click the button. Historically, you know that 5.8% of visitors to your site click the (green) button.</p>

<p>So one day you make the button red and keep track of the fraction of visitors who click on it. After some period of time you observe that 53 out of 810 visitors clicked the (red) button. That&#8217;s 6.5%, a decent improvement! (Some would say it&#8217;s a 12% improvement; others a 0.7% improvement. Potato, potato.) But is this difference something we&#8217;re likely to observe just by chance, or was making the button red a meaningful change?</p>

<p>We can model the number of clicks as a Bernoulli distribution with a 5.8% success probability. In order to do this I&#8217;ll have to translate <code>true</code> to 1 click and <code>false</code> to 0 clicks.</p>

<pre><code>scala&gt; val d = bernoulli(0.058).map(b =&gt; if (b) 1.0 else 0.0)
d: Distribution[Double] = &lt;distribution&gt;

scala&gt; d.stdev
res0: Double = 0.24062616233485523</code></pre>

<p>We want to know the probability of seeing a 6.5% success rate in a sample of 810 visitors. Since we know the standard deviation of <code>d</code>, we can apply the Central Limit Theorem to find the standard error for a sample of size 810, just by dividing by <script type='math/tex'>\sqrt{810} = 28.5</script>.</p>

<pre><code>scala&gt; d.stdev / math.sqrt(810)
res2: Double = 0.008313053843054703</code></pre>

<p>Well, OK. Our difference of 0.7% is less than one standard deviation from the mean, which means it&#8217;s pretty likely to happen just by chance. In other words, we can&#8217;t reject the null hypothesis that making the button red did not change the rate at which people click on it.</p>

<p>To illustrate this further, here&#8217;s what the distribution of sample means for samples of size 810 looks like:</p>

<pre><code>scala&gt; sampleMean(d, 810).hist
0.0350  0.15% 
0.0375  0.41% 
0.0400  1.20% #
0.0425  2.01% ##
0.0450  3.58% ###
0.0475  5.96% #####
0.0500  7.55% #######
0.0525  9.62% #########
0.0550 11.49% ###########
0.0575 12.08% ############
0.0600 11.23% ###########
0.0625 10.42% ##########
0.0650  8.32% ########
0.0675  5.70% #####
0.0700  3.97% ###
0.0725  2.90% ##
0.0750  1.56% #
0.0775  0.84% 
0.0800  0.47% </code></pre>

<p>Seeing 6.5% as the mean of a sample of size 810 is totally within the fat part of the distribution. Let&#8217;s see just how likely a difference of 0.7% really is:</p>

<pre><code>scala&gt; sampleMean(d, 810).pr(c =&gt; c &lt; 0.051 || c &gt; 0.065)
res3: Double = 0.4074</code></pre>

<p>Pretty likely! (A difference of -0.7% would be just as surprising, so we have to count that too.)</p>

<p>OK, let&#8217;s say instead we had observed 530 clicks out of 8100 visitors &#8211; it&#8217;s the same 6.5% success rate, just with 10 times as many samples. What does that do to our analysis? Well, first of all, we can expect the standard error to be much smaller, since we&#8217;re dividing by <script type='math/tex'>\sqrt{8100} = 90</script> instead of <script type='math/tex'>\sqrt{810} = 28.5</script>.</p>

<pre><code>scala&gt; d.stdev / 90
res0: Double = 0.0026411931460171042</code></pre>

<p>The standard error is 0.26%, putting our 0.7% difference at more than 2.5 standard deviations from the mean, and so in this case we can reject the null hypothesis and conclude that making the button red was a meaningful change.</p>

<p>To further illustrate this, here is the distribution of sample means for samples of size 8100, on the same scale as before:</p>

<pre><code>scala&gt; sampleMean(d, 8100).bucketedHist(0.035, 0.08, 18)
0.0350  0.00% 
0.0375  0.00% 
0.0400  0.00% 
0.0425  0.00% 
0.0450  0.00% 
0.0475  0.01% 
0.0500  0.44% 
0.0525  4.50% ####
0.0550 19.90% ###################
0.0575 36.42% ####################################
0.0600 28.55% ############################
0.0625  8.70% ########
0.0650  1.41% #
0.0675  0.06% 
0.0700  0.01% 
0.0725  0.00% 
0.0750  0.00% 
0.0775  0.00% 
0.0800  0.00% 

scala&gt; sampleMean(d, 8100).pr(p =&gt; p &lt; 0.051 || p &gt; 0.065)
res4: Double = 0.0062</code></pre>

<p>The range of the distribution of sample means is much narrower, so much so that a difference of 0.7% (in either direction) is expected to occur by chance less than 1% of the time.</p>

<h3 id='one_important_exception'>One important exception</h3>

<p>It turns out that the Central Limit Theorem doesn&#8217;t work with every distribution. This is due to one sneaky fact — sample means are clustered around the mean of the underlying distribution <em>if it exists</em>. But how can a distribution have no mean? Well, one common distribution that has no mean is the Pareto distribution. If you tried to calculate it using the usual methods, it would diverge to infinity.</p>

<p>The means of samples drawn from the Pareto distribution are not normally distributed:</p>

<pre><code>scala&gt; sampleMean(pareto(1)).bucketedHist(0, 20, 20)
 0.0  0.00% 
 1.0  0.00% 
 2.0  0.00% 
 3.0  3.04% ###
 4.0 16.03% ################
 5.0 20.08% ####################
 6.0 16.61% ################
 7.0 12.16% ############
 8.0  8.08% ########
 9.0  5.87% #####
10.0  4.42% ####
11.0  2.90% ##
12.0  2.72% ##
13.0  1.67% #
14.0  1.51% #
15.0  1.31% #
16.0  1.14% #
17.0  0.88% 
18.0  0.83% 
19.0  0.53% 
20.0  0.22% </code></pre>

<p>And the standard error is completely meaningless:</p>

<pre><code>scala&gt; sampleMean(pareto(1)).stdev
res0: Double = 157.6098722134558

scala&gt; sampleMean(pareto(1)).stdev
res1: Double = 477.9797744569662</code></pre>

<p>So the Central Limit Theorem doesn&#8217;t apply.</p>

<h3 id='conclusion'>Conclusion</h3>

<p>We were able to use the Central Limit Theorem to reason about a samples from various distributions, knowing that the mean of such a sample is expected to fall within a bell-shaped curve around the mean of the underlying distribution. This is great because we don&#8217;t need special analysis tools for each kind of distribution we might come across. No matter what the underlying distribution is, you can always treat sample means as normally distributed.</p>

<p>&#8230; unless the underlying distribution has no mean.</p>

<p>We actually run into this all the time at Foursquare. Certain things like, say, the distribution of the number of friends users have is Pareto-distributed (the vast majority of users have a small number of friends, but some users have thousands of friends). So if you&#8217;re running an experiment that is intended to increase the average number of friends users have, you&#8217;re going to run into trouble. You aren&#8217;t going to be able to use standard statistical techniques to analyze the results of the experiment. Well, actually, you can try, and you&#8217;ll get some convincing-looking numbers out, but those numbers will be completely meaningless!</p>
    </div>

    

  


  <div style="float: right; margin-top: 5px">
  <a href="https://twitter.com/share" class="twitter-share-button" data-url="http://jliszka.github.io/2013/08/26/a-programmers-guide-to-the-central-limit-theorem.html" data-text="A Programmer's Guide to the Central Limit Theorem" data-via="jliszka">Tweet</a>
  <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
</div>





  
    <ul class="tag_box inline">
      <li><i class="icon-tags"></i></li>
      
      


  
     
    	<li><a href="/tags.html#probability-ref">probability <span>5</span></a></li>
    
  



    </ul>
    

    <hr>
    <div class="navigation">
    
      <span class="prev"><a href="/2013/08/19/climbing-the-probability-distribution-ladder.html" title="Climbing the probability distribution ladder">&larr; Climbing the probability distribution ladder</a></span>
    
    
      <span class="next"><a href="/2013/09/03/fun-with-bayesian-priors.html" title="Fun with Bayesian Priors">Fun with Bayesian Priors &rarr;</a></span>
    
      <div style="clear: both"></div>
    </div>
    <hr>
    


  <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'jliszka'; // required: replace example with your forum shortname
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>




  </div>
</div>


      </div>
      <hr>
      <footer>
        <p>&copy; 2013 Jason Liszka
          with help from <a href="http://jekyllbootstrap.com" target="_blank" title="The Definitive Jekyll Blogging Framework">Jekyll Bootstrap</a>
          and <a href="http://twitter.github.com/bootstrap/" target="_blank">Twitter Bootstrap</a>
        </p>
      </footer>

    </div>

    


  <script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-42567355-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>



  </body>
</html>

