<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
<channel>
        <title>A Gentleman and a Scala</title>
        <description>A Gentleman and a Scala - Jason Liszka</description>
        <link>http://jliszka.github.io</link>
        <link>http://jliszka.github.io</link>
        <lastBuildDate>2013-10-01T22:52:40-04:00</lastBuildDate>
        <pubDate>2013-10-01T22:52:40-04:00</pubDate>
        <ttl>1800</ttl>


        <item>
                <title>How traffic actually works</title>
                <description>&lt;p&gt;Every so often &lt;a href='http://www.smartmotorist.com/traffic-and-safety-guideline/traffic-jams.html'&gt;this article&lt;/a&gt; makes the rounds and it annoys me. That isn&amp;#8217;t how traffic works and the proposed solutions won&amp;#8217;t fix anything. Maybe you can eliminate the annoying stop-and-go, but no one gets home any faster. In fact you can prove that you and everyone behind you get home strictly later than if you had just gone along with the stop-and-go traffic.&lt;/p&gt;

&lt;h3 id='the_facts'&gt;The facts&lt;/h3&gt;

&lt;p&gt;Here&amp;#8217;s how traffic works. First, we know from &lt;a href='http://www.fhwa.dot.gov/publications/research/operations/tft/chap2.pdf'&gt;empirical studies&lt;/a&gt; that drivers tend to maintain a minimum following distance, measured in seconds. It varies per driver, but typically it&amp;#8217;s somewhere between 1.5 and 2 seconds. That works out to a maximum theoretical flow rate of between 1,800 and 2,400 vehicles per lane per hour passing by a given point on the highway. Studies of actual highway traffic have measured flow rates as high as 2,000 vehicles per lane per hour, which works out to a following distance of 1.8 seconds. (I&amp;#8217;m just going to call it 2 seconds for the sake of round numbers.)&lt;/p&gt;

&lt;p&gt;The important fact: &lt;strong&gt;there is a limit to the number of cars that can pass by a given point on the highway in a given amount of time, and that limit is one car every 2 seconds, per lane&lt;/strong&gt;. So imagine you are in heavy traffic during rush hour. There are a certain number of cars in line in front of you. Let&amp;#8217;s pick a point on the road to call the front of the line — say, the point at which you plan to exit the highway. The line gets shorter by one car every 2 seconds. If there are 1,000 cars in front of you, it&amp;#8217;s going to take a minimum of 2,000 seconds for you to get to the front of the line. It doesn&amp;#8217;t matter whether people are kind and let cars merge in front of them, zipper-style. It doesn&amp;#8217;t matter how much stop-and-go there is. The simple fact is that it takes 2 seconds per car for you to get to the front of the line, and there are some cars in front of you that have to get there before you do.&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h3 id='merging'&gt;Merging&lt;/h3&gt;

&lt;p&gt;Say there are some cars that are trying to merge into your lane a mile or so in front of you. Every time a car merges in, that adds 2 seconds to your trip. If one car merges in every 2 seconds, your trip gets longer by 2 seconds every 2 seconds, which means you are not moving (or will soon not be moving).&lt;/p&gt;

&lt;p&gt;Leaving space in front of your car for people who are trying to merge won&amp;#8217;t solve anything. Let&amp;#8217;s say you slow down to leave some room for an upcoming merge. Now you are 4 seconds behind the car in front of you instead of 2. You&amp;#8217;ve just added 2 seconds to the commute of everyone behind you in line. Now a car merges in front of you. At the next merge, you have to leave more space. That&amp;#8217;s another 2 seconds for everyone behind you. There is no &amp;#8220;simple cure&amp;#8221; for this!&lt;/p&gt;

&lt;p&gt;If &lt;a href='http://www.smartmotorist.com/traffic-and-safety-guideline/traffic-jams.html'&gt;anyone tries to tell you&lt;/a&gt; that if only drivers left space in front of them and took turns merging, traffic would flow smoothly, and it&amp;#8217;s only because of jerks that there are any traffic jams at all, just ask them what&amp;#8217;s going to happen at the &lt;em&gt;next&lt;/em&gt; merge. Where is that extra space going to come from? You cannot keep 2 seconds back from the car that has just merged in front of you without, um, &lt;em&gt;slowing down&lt;/em&gt;. If the car in front of you is also slowing down for the same reason, you have to slow down even more. This is basically the definition of a traffic jam.&lt;/p&gt;

&lt;p&gt;Zipper merging is only beneficial insofar as it reduces confusion on the road, the way any convention does — like who gets to go next at a 4-way stop. Confusion leads to delay, delay leads to anger, etc., etc.&lt;/p&gt;

&lt;h3 id='bottlenecks'&gt;Bottlenecks&lt;/h3&gt;

&lt;p&gt;Suppose you&amp;#8217;re on a 2-lane (each way) highway and one lane is closed up ahead due to construction. Now the flow rate of your lane is cut in half (or there are twice as many cars in line in front of you, depending on how you want to look at it). Road signs commonly ask you to use both lanes up to the point of the bottleneck. That&amp;#8217;s reasonable advice, but it&amp;#8217;s not going to get anyone home faster. Remember only so many cars are going to clear the bottleneck per second, no matter what happens upstream. The only thing this does is shorten the length of the backup on the highway — it&amp;#8217;s a 2 mile backup instead of a 4 mile backup. This is good because it is less likely to affect other traffic by spilling out onto onramps and surface roads. Also maybe there&amp;#8217;s someone on the highway who&amp;#8217;s planning to exit 3 miles before the bottleneck. If the backup is 2 miles instead of 4 miles, that person doesn&amp;#8217;t have to wait in traffic.&lt;/p&gt;

&lt;p&gt;As an aside, whenever I see a line of cars on the highway (or, for that matter, any line of anythings anywhere), I make sure I know what it&amp;#8217;s for before I get in it. If you see a line of cars in the right lane, and the left lane is completely empty, what do you do? Maybe the left lane is closed up ahead, and everyone decided to merge early. Or maybe the people in the right lane are trying to exit, and there&amp;#8217;s a backup on the offramp, one mile ahead. I&amp;#8217;m not getting in that line if that&amp;#8217;s the case! One time I was driving and happened upon just such a situation. So I stayed in the left lane. Someone ahead of me pulled out into the left lane and kept speed with the right lane, blocking me from passing. I was pissed!&lt;/p&gt;

&lt;h3 id='catastrophe'&gt;Catastrophe&lt;/h3&gt;

&lt;p&gt;It&amp;#8217;s worth noting that the 2 second following distance is measured front bumper to front bumper — if you were sitting by the side of the highway, that&amp;#8217;s how you&amp;#8217;d count the time between cars going by. But drivers generally like to keep a 2 second following distance between their front bumper and the &lt;em&gt;rear&lt;/em&gt; bumper of the car in front of them. The difference between these is negligible at high speeds, but at a low enough speed, it becomes difficult to maintain a 2 second following distance from the &lt;em&gt;front&lt;/em&gt; bumper of the car in front of you without impinging on the &lt;em&gt;rear&lt;/em&gt; bumper of the car in front of you, especially if said car is more than 0 feet long. So under these circumstances the flow rate of the highway decreases below 1 car every 2 seconds — maybe to 1 car every 5 seconds. So now you have to wait 5 seconds for every car in front of you in line.&lt;/p&gt;

&lt;p&gt;The situation is modeled pretty well by &lt;a href='http://en.wikipedia.org/wiki/Catastrophe_theory'&gt;catastrophe theory&lt;/a&gt;, something I never thought would be useful.&lt;/p&gt;
&lt;center&gt;&lt;img class='spacer' src='/assets/img/traffic/catastrophe.png' /&gt;&lt;/center&gt;
&lt;p&gt;At low occupancy (cars per mile), drivers can go as fast as they&amp;#8217;d like. As occupancy increases, so does flow rate, even though speed decreases somewhat due to everyone trying to maintain following distance. At a certain point, when occupancy becomes high enough, speed dips low enough to where drivers are unable to maintain their minimum following distance, and — catastrophe! — the flow rate decreases dramatically.&lt;/p&gt;

&lt;h3 id='some_code'&gt;Some code&lt;/h3&gt;

&lt;p&gt;Let&amp;#8217;s see how well this model predicts reality. Here&amp;#8217;s some code that determines the flow rate and the speed of traffic as a function of occupancy (cars per km), according to the following-distance model:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;traffic&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;carsPerKm&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;carLength&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='mf'&gt;5.0&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;secondsBetweenCars&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='mf'&gt;1.8&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;metersPerSecondToKmPerHour&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='mf'&gt;3600.0&lt;/span&gt; &lt;span class='o'&gt;/&lt;/span&gt; &lt;span class='mf'&gt;1000.0&lt;/span&gt;
  &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;metersBetweenCars&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='mf'&gt;1000.0&lt;/span&gt; &lt;span class='o'&gt;/&lt;/span&gt; &lt;span class='n'&gt;carsPerKm&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt; &lt;span class='n'&gt;carLength&lt;/span&gt;
  &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;maxSpeed&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;min&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;120&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;metersBetweenCars&lt;/span&gt; &lt;span class='o'&gt;/&lt;/span&gt; &lt;span class='n'&gt;secondsBetweenCars&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;metersPerSecondToKmPerHour&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
  &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;carsPerHour&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;carsPerKm&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;maxSpeed&lt;/span&gt;
  &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;maxSpeed&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;carsPerHour&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Evaluating &lt;code&gt;traffic&lt;/code&gt; with values of &lt;code&gt;carsPerKm&lt;/code&gt; between 1 and 200 produces the following output:&lt;/p&gt;
&lt;center&gt;&lt;img class='spacer' src='/assets/img/traffic/model.png' /&gt;&lt;/center&gt;
&lt;p&gt;Each dot represents a different value of &lt;code&gt;carsPerKm&lt;/code&gt; and is plotted as the maximum speed and flow rate it implies. Below an occupancy of about 16 cars per km, the maximum speed that still allows everyone to keep a 1.8-second following distance is well above a reasonable speed limit, so I just capped it at 120 kph. Obviously real highway traffic is going to &lt;a href='http://books.google.com/books?id=4g7f1h4BfYsC&amp;amp;printsec=frontcover#v=onepage&amp;amp;q&amp;amp;f=false'&gt;behave in more subtle ways than that&lt;/a&gt;. But it doesn&amp;#8217;t matter because the congested part is all I care about, and this model matches observed data pretty well. Here&amp;#8217;s some data from a meta-analysis by the &lt;a href='http://www.fhwa.dot.gov/publications/research/operations/tft/chap2.pdf'&gt;Federal Highway Administration&lt;/a&gt;:&lt;/p&gt;
&lt;center&gt;&lt;img class='spacer' src='/assets/img/traffic/speed_vs_flow.png' /&gt;&lt;/center&gt;
&lt;p&gt;For another comparison, here&amp;#8217;s what the model predicts for flow rate vs. occupancy:&lt;/p&gt;
&lt;center&gt;&lt;img class='spacer' src='/assets/img/traffic/inverted-v-model.png' /&gt;&lt;/center&gt;
&lt;p&gt;And here&amp;#8217;s the data, from &lt;a href='http://trid.trb.org/view.aspx?id=308654'&gt;Freeway Speed-Flow Concentration Relationships&lt;/a&gt;:&lt;/p&gt;
&lt;center&gt;&lt;img class='spacer' src='/assets/img/traffic/inverted-v-actual.png' /&gt;&lt;/center&gt;
&lt;p&gt;And a quote from the same source:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;#8220;The inverted-V model implies that drivers maintain a roughly constant average time gap between their front bumper and the back bumper of the vehicle in front of them, provided their speed is less than some critical value. Once their speed reaches this critical value (which is as fast as they want to go), they cease to be sensitive to vehicle spacing.&amp;#8221;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Parameter tuning aside, this simple model predicts actual traffic so well that any reasonable discussion of the physics of traffic jams has to take it into account.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: Lots of great comments in the &lt;a href='https://news.ycombinator.com/item?id=6476836'&gt;hacker news thread&lt;/a&gt;. One thing one of the commenters rightly points out is that this model does not account for the variance in flow rates in congested traffic. I don&amp;#8217;t have any data to back this up, but it&amp;#8217;s possible that variance in car length (% of trucks on the road) might account for it. Here&amp;#8217;s the same model but with car length ranging between 5 and 8 meters:&lt;/p&gt;
&lt;center&gt;
  &lt;img class='spacer' src='/assets/img/traffic/speed-vs-occupancy-2.png' /&gt;
  &lt;img class='spacer' src='/assets/img/traffic/flow-vs-occupancy-2.png' /&gt;
&lt;/center&gt;
&lt;p&gt;But it&amp;#8217;s also possible that driver behavior is responsible for it. Pathological driving could certainly create pockets of low-flow traffic, as human response times and vehicle acceleration times aren&amp;#8217;t instantaneous.&lt;/p&gt;

&lt;h3 id='antitraffic'&gt;&amp;#8220;Anti-traffic&amp;#8221;&lt;/h3&gt;

&lt;p&gt;Since occupancy determines flow rate, there&amp;#8217;s not much benefit to trying to &amp;#8220;cancel out&amp;#8221; a traffic wave by leaving a ton of space in front of you. No matter what you do, you&amp;#8217;re not going to get to the front of the line before the car in front of you. Worse, by leaving space in front of you, you&amp;#8217;re artificially reducing the occupancy of that part of the road, but since you haven&amp;#8217;t changed the number of cars trying to use the road, &lt;em&gt;this comes at the expense of increased occupancy somewhere else&lt;/em&gt;. Sure, there&amp;#8217;s a beautiful line of cars behind you driving at a constant 35 mph, but behind &lt;em&gt;that&lt;/em&gt; there&amp;#8217;s a ridiculous traffic jam that didn&amp;#8217;t need to be there.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update #2&lt;/strong&gt;: Drew R. makes a great point in the comments that there doesn&amp;#8217;t always have to be a traffic jam at the back of that line of cars — specifically, when there is excess capacity in the road behind you, i.e., the flow rate is below the theoretical maximum and the occupancy is below the critical threshold where people start caring about following distance. You might know whether such a region exists behind you because you just drove through it. So in that situation, it is actually beneficial to try to smooth out the traffic wave by driving slower as you approach it.&lt;/p&gt;

&lt;h3 id='conclusion'&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;At the risk of being helpful, here are some things YOU can do that are actually guaranteed to improve commute times for everyone:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Drive a shorter car.&lt;/li&gt;

&lt;li&gt;Don&amp;#8217;t let people merge in front of you, ever.&lt;/li&gt;

&lt;li&gt;Don&amp;#8217;t drive during rush hour.&lt;/li&gt;

&lt;li&gt;Move to New York. Seriously, no one owns a car here. It&amp;#8217;s great. I don&amp;#8217;t even know why I&amp;#8217;m writing this.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Bye!&lt;/p&gt;</description>
                <link>http://jliszka.github.io/2013/10/01/how-traffic-actually-works.html</link>
                <guid>http://jliszka.github.io/2013/10/01/how-traffic-actually-works</guid>
                <pubDate>2013-10-01T00:00:00-04:00</pubDate>
        </item>

        <item>
                <title>More backwards functions: Unevaluating polynomials</title>
                <description>&lt;p&gt;I have a function that evaluates polynomials with integer coefficients. To evaluate &lt;script type='math/tex; mode=display'&gt;
%&lt;![CDATA[
f(x) = 6 + 5x + 2x^3
%]]&gt;
&lt;/script&gt; at &lt;script type='math/tex'&gt;f(8)&lt;/script&gt;, for example, you do this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; evalPoly(8, List(6, 5, 0, 2))
res0: (Int, Int) = (8, 1070)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For some reason it echoes the input back out to you. Here&amp;#8217;s the code you might write:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;evalPoly&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;coeffs&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;eval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;cs&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;cs&lt;/span&gt; &lt;span class='k'&gt;match&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='nc'&gt;Nil&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='mi'&gt;0&lt;/span&gt;
      &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='n'&gt;h&lt;/span&gt; &lt;span class='o'&gt;::&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;x&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;eval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;t&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;+&lt;/span&gt; &lt;span class='n'&gt;h&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
  &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;eval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;coeffs&lt;/span&gt;&lt;span class='o'&gt;))&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This should not be surprising.&lt;/p&gt;

&lt;p&gt;But I also have a function that un-evaluates polynomials. To un-evaluate &lt;script type='math/tex'&gt;f(8) = 1070&lt;/script&gt;, you do this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; unevalPoly(8, 1070)
res1: (Int, List[Int]) = (8, List(6, 5, 0, 2))&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and it echoes your input and gives you back the coefficients of the polynomial.&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;Wait, what? I thought you needed &lt;script type='math/tex'&gt;N+1&lt;/script&gt; points to determine an &lt;script type='math/tex'&gt;N&lt;/script&gt;-degree polynomial. Here I&amp;#8217;ve seemingly done it with just one point. To spoil the surprise a little, &lt;code&gt;unevalPoly&lt;/code&gt; doesn&amp;#8217;t always work. But how does it work even some of the time? How would you go about coding this up?&lt;/p&gt;

&lt;p&gt;Having noticed that the input to &lt;code&gt;unevalPoly&lt;/code&gt; is the output of &lt;code&gt;evalPoly&lt;/code&gt;, and vice versa, one tack we can try is to write &lt;code&gt;evalPoly&lt;/code&gt; backwards. First let me rewrite it slightly:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;evalPoly&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;coeffs&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;eval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;cs&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;cs&lt;/span&gt; &lt;span class='k'&gt;match&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='nc'&gt;Nil&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='mi'&gt;0&lt;/span&gt;
      &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='n'&gt;h&lt;/span&gt; &lt;span class='o'&gt;::&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;plustimes&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;eval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;t&lt;/span&gt;&lt;span class='o'&gt;),&lt;/span&gt; &lt;span class='n'&gt;h&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
  &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;eval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;coeffs&lt;/span&gt;&lt;span class='o'&gt;))&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I&amp;#8217;ve just replaced &lt;code&gt;x * eval(t) + h&lt;/code&gt; with a call to this function:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;plustimes&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;n&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;q&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;n&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;q&lt;/span&gt; &lt;span class='o'&gt;+&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now here&amp;#8217;s &lt;code&gt;eval&lt;/code&gt; as a data flow diagram. I&amp;#8217;ve threaded through &lt;code&gt;x&lt;/code&gt; as a &amp;#8220;context&amp;#8221; variable because it isn&amp;#8217;t an input to &lt;code&gt;eval&lt;/code&gt; per se.&lt;/p&gt;

&lt;p&gt;&lt;img alt='eval' src='/assets/img/poly/eval.png' /&gt;&lt;/p&gt;

&lt;p&gt;Following the arrows backwards from the outputs to the inputs we can write the following code:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;unevalPoly&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;y&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;uneval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;y&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;y&lt;/span&gt; &lt;span class='k'&gt;match&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='mi'&gt;0&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='nc'&gt;Nil&lt;/span&gt;
      &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='n'&gt;y&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
        &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;q&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;unplustimes&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;y&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
        &lt;span class='n'&gt;r&lt;/span&gt; &lt;span class='o'&gt;::&lt;/span&gt; &lt;span class='n'&gt;uneval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;q&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
      &lt;span class='o'&gt;}&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
  &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;uneval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;y&lt;/span&gt;&lt;span class='o'&gt;))&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now this should work as long as we can write &lt;code&gt;unplustimes&lt;/code&gt;, which is possible only when &lt;code&gt;plustimes&lt;/code&gt; doesn&amp;#8217;t destroy information. So given &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;m = n * q + r&lt;/code&gt;, when can we recover &lt;code&gt;q&lt;/code&gt; and &lt;code&gt;r&lt;/code&gt;?&lt;/p&gt;

&lt;p&gt;Well, if &lt;code&gt;r&lt;/code&gt; happens to be less than &lt;code&gt;n&lt;/code&gt;, this is just like doing long division — &lt;code&gt;q&lt;/code&gt; and &lt;code&gt;r&lt;/code&gt; are the quotient and remainder when dividing &lt;code&gt;m&lt;/code&gt; by &lt;code&gt;n&lt;/code&gt;:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;unplustimes&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;n&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;m&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;q&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;m&lt;/span&gt; &lt;span class='o'&gt;/&lt;/span&gt; &lt;span class='n'&gt;n&lt;/span&gt;
  &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;m&lt;/span&gt; &lt;span class='o'&gt;%&lt;/span&gt; &lt;span class='n'&gt;n&lt;/span&gt;
  &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;q&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This works because for a given positive integer &lt;script type='math/tex'&gt;n&lt;/script&gt;, any integer &lt;script type='math/tex'&gt;m&lt;/script&gt; can be written uniquely as &lt;script type='math/tex'&gt;m = nq + r&lt;/script&gt;, where &lt;script type='math/tex'&gt;q&lt;/script&gt; and &lt;script type='math/tex'&gt;r&lt;/script&gt; are nonnegative integers and &lt;script type='math/tex'&gt;r \lt n&lt;/script&gt;. Since this formulation is unique, it&amp;#8217;s easy to reverse the process and recover &lt;script type='math/tex'&gt;q&lt;/script&gt; and &lt;script type='math/tex'&gt;r&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;So what does that mean for &lt;code&gt;unevalPoly&lt;/code&gt;? It will only work if&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is a positive integer, and&lt;/li&gt;

&lt;li&gt;all of the coefficients are nonnegative integers less than &lt;code&gt;x&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let&amp;#8217;s try it out. This works:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; evalPoly(8, List(1, 3))
res0: (Int, Int) = (8, 25)

scala&amp;gt; unevalPoly(8, 25)
res1: (Int, List[Int]) = (8, List(1, 3))&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But this doesn&amp;#8217;t, as expected:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; evalPoly(2, List(1, 4, 2))
res2: (Int, Int) = (2, 17)

scala&amp;gt; unevalPoly(2, 17)
res3: (Int, List[Int]) = (2, List(1, 0, 0, 0, 1))&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And neither does this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; evalPoly(5, List(1, -2, 1))
res4: (Int, Int) = (5, 16)

scala&amp;gt; unevalPoly(5, 16)
res5: (Int, List[Int]) = (5, List(1, 3))&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Neat though!&lt;/p&gt;

&lt;h3 id='a_puzzle'&gt;A puzzle&lt;/h3&gt;

&lt;p&gt;This all came to me through a puzzle I heard: Your friend has a secret polynomial, which you know has nonnegative integer coefficients. She challenges you to determine the coefficients of the polynomial, offering to evaluate the polynomial for you on any two numbers you choose.&lt;/p&gt;

&lt;p&gt;From the above, you know need to evaluate the polynomial at a number that is larger than all of the coefficients. So all that&amp;#8217;s left to the solution is finding some number that satisfies that description.&lt;/p&gt;

&lt;h3 id='whats_really_going_on'&gt;What&amp;#8217;s really going on&lt;/h3&gt;

&lt;p&gt;You might have noticed that all &lt;code&gt;unevalPoly(n, m)&lt;/code&gt; is doing is converting &lt;code&gt;m&lt;/code&gt; to its representation in base &lt;code&gt;n&lt;/code&gt;. Here it is converting 42 to base 2:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; unevalPoly(2, 42)
res6: (Int, List[Int]) = (2,List(0, 1, 0, 1, 0, 1))&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And oh, look:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; unevalPoly(10, 12345)
res7: (Int, List[Int]) = (10, List(5, 4, 3, 2, 1))&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This all makes sense now. The polynomial&lt;/p&gt;
&lt;script type='math/tex; mode=display'&gt;
%&lt;![CDATA[
f(x) = ax^4 + bx^3 + cx^2 + dx + e
%]]&gt;
&lt;/script&gt;
&lt;p&gt;is what you mean when you write &lt;script type='math/tex'&gt;abcde_x&lt;/script&gt;, which is the unique representation of that number in base &lt;script type='math/tex'&gt;x&lt;/script&gt; provided that all of the coefficients are less than &lt;script type='math/tex'&gt;x&lt;/script&gt;. Recovering the coefficients of &lt;script type='math/tex'&gt;f(x) = y&lt;/script&gt; is the same as writing &lt;script type='math/tex'&gt;y&lt;/script&gt; in base &lt;script type='math/tex'&gt;x&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;So backwards programming is good for something! If this interests you, you should read my &lt;a href='/2013/09/18/insertion-sort-is-dual-to-bubble-sort.html'&gt;last post on backwards sorting algorithms&lt;/a&gt;.&lt;/p&gt;</description>
                <link>http://jliszka.github.io/2013/09/24/more-backwards-functions-unevaluating-polynomials.html</link>
                <guid>http://jliszka.github.io/2013/09/24/more-backwards-functions-unevaluating-polynomials</guid>
                <pubDate>2013-09-24T00:00:00-04:00</pubDate>
        </item>

        <item>
                <title>Insertion sort is dual to bubble sort</title>
                <description>&lt;p&gt;I noticed recently that &lt;a href='http://en.wikipedia.org/wiki/insertion_sort'&gt;insertion sort&lt;/a&gt; is &lt;a href='http://en.wikipedia.org/wiki/Bubble_sort'&gt;bubble sort&lt;/a&gt; backwards. Or inside out. Or something.&lt;/p&gt;

&lt;p&gt;Both algorithms can be expressed as a main function that calls a recursive helper. Let&amp;#8217;s take a look at the main functions first.&lt;/p&gt;

&lt;h3 id='the_main_functions'&gt;The main functions&lt;/h3&gt;

&lt;p&gt;Here&amp;#8217;s the code:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;insertionSort&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;:&lt;/span&gt; &lt;span class='kt'&gt;Ordered&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]](&lt;/span&gt;&lt;span class='n'&gt;xs&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;xs&lt;/span&gt; &lt;span class='k'&gt;match&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='nc'&gt;Nil&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='nc'&gt;Nil&lt;/span&gt;
    &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='n'&gt;h&lt;/span&gt; &lt;span class='o'&gt;::&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;s&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;insertionSort&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;t&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
      &lt;span class='n'&gt;insert&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;h&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;s&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;insertionSort&lt;/code&gt; sorts a list by inserting the head of the list into the recursively sorted tail, in such a way that it remains sorted.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;bubbleSort&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;:&lt;/span&gt; &lt;span class='kt'&gt;Ordered&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]](&lt;/span&gt;&lt;span class='n'&gt;xs&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;xs&lt;/span&gt; &lt;span class='k'&gt;match&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='nc'&gt;Nil&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='nc'&gt;Nil&lt;/span&gt;
    &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='n'&gt;xs&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;h&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;bubble&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;xs&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
      &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;s&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;bubbleSort&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;t&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
      &lt;span class='n'&gt;h&lt;/span&gt; &lt;span class='o'&gt;::&lt;/span&gt; &lt;span class='n'&gt;s&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;  
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;bubbleSort&lt;/code&gt; sorts a list by bubbling the smallest element to the front of the list and recursively sorting the tail.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s not obvious from the code that these functions are backwards versions of each other, but look at their data flow diagrams:&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;&lt;img alt='main function data flow' src='/assets/img/main.png' /&gt;&lt;/p&gt;

&lt;p&gt;They are exactly the same, except with the arrows going the other way.&lt;/p&gt;

&lt;p&gt;Of course you would also want each box to be the &amp;#8220;backwards&amp;#8221; version of its corresponding box in the other function. This is pretty obviously true for &lt;code&gt;cons&lt;/code&gt; and &lt;code&gt;decons&lt;/code&gt; — one constructs a list from a head and a tail, and the other deconstructs a list into a head and a tail. And by assumption, the recursive call to &lt;code&gt;bubble sort&lt;/code&gt; is the &amp;#8220;backwards&amp;#8221; version of the corresponding recursive call to &lt;code&gt;insertion sort&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;All that&amp;#8217;s left is to show that this is true for helper functions — that &lt;code&gt;bubble&lt;/code&gt; is &lt;code&gt;insert&lt;/code&gt; backwards.&lt;/p&gt;

&lt;h3 id='the_helper_functions'&gt;The helper functions&lt;/h3&gt;

&lt;p&gt;Here&amp;#8217;s the code:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;insert&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;:&lt;/span&gt; &lt;span class='kt'&gt;Ordered&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]](&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;xs&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;xs&lt;/span&gt; &lt;span class='k'&gt;match&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='nc'&gt;Nil&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;x&lt;/span&gt; &lt;span class='o'&gt;::&lt;/span&gt; &lt;span class='nc'&gt;Nil&lt;/span&gt;
    &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='n'&gt;h&lt;/span&gt; &lt;span class='o'&gt;::&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;a&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;b&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;sort&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;h&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
      &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;insert&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;b&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
      &lt;span class='n'&gt;a&lt;/span&gt; &lt;span class='o'&gt;::&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;insert&lt;/code&gt;&amp;#8217;s job is to insert an item into an already-sorted list in such a way that the list remains sorted.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;bubble&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;:&lt;/span&gt; &lt;span class='kt'&gt;Ordered&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]](&lt;/span&gt;&lt;span class='n'&gt;xs&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;xs&lt;/span&gt; &lt;span class='k'&gt;match&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='n'&gt;h&lt;/span&gt; &lt;span class='o'&gt;::&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;bubble&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;t&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
      &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;a&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;b&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;sort&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;h&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
      &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;a&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;b&lt;/span&gt; &lt;span class='o'&gt;::&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;bubble&lt;/code&gt;&amp;#8217;s job is to bubble the smallest item up to the front of the list and return that item plus the rest of the list.&lt;/p&gt;

&lt;p&gt;The data flow diagrams of these functions make it totally clear that they are the same function, only one has the arrows reversed and the boxes running backwards:&lt;/p&gt;

&lt;p&gt;&lt;img alt='helper function data flow' src='/assets/img/helper.png' /&gt;&lt;/p&gt;

&lt;p&gt;The only thing left is this &lt;code&gt;sort&lt;/code&gt; method, which takes 2 arguments and returns them in sorted order. Which I guess is the same if you run it backwards?&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;sort&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;:&lt;/span&gt; &lt;span class='kt'&gt;Ordered&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]](&lt;/span&gt;&lt;span class='n'&gt;a&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;b&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;if&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;a&lt;/span&gt; &lt;span class='o'&gt;&amp;lt;&lt;/span&gt; &lt;span class='n'&gt;b&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;a&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;b&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='k'&gt;else&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;b&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;a&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So, there you have it. Aside from the &lt;code&gt;Nil&lt;/code&gt; cases and the question of &lt;code&gt;sort&lt;/code&gt; being its own opposite, if you take a machine that does insertion sort and run it in reverse, you get a machine that does bubble sort.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m pretty sure this fits the categorical definition of a dual, but my Category theory isn&amp;#8217;t strong enough to say for sure.&lt;/p&gt;

&lt;h3 id='are_there_any_other_sorting_algorithms_that_are_duals'&gt;Are there any other sorting algorithms that are duals?&lt;/h3&gt;

&lt;p&gt;Yes, I believe &lt;a href='http://en.wikipedia.org/wiki/Merge_sort'&gt;Merge sort&lt;/a&gt; and &lt;a href='http://en.wikipedia.org/wiki/Quicksort'&gt;Quicksort&lt;/a&gt; are duals, but I haven&amp;#8217;t been able to get it to work out.&lt;/p&gt;

&lt;p&gt;Merge sort looks like this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Split the list in half&lt;/li&gt;

&lt;li&gt;Recursively sort each half&lt;/li&gt;

&lt;li&gt;Merge the two halves back together&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And Quicksort looks like this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Partition the list into a list of smaller items and a list of larger items&lt;/li&gt;

&lt;li&gt;Recursively sort each list&lt;/li&gt;

&lt;li&gt;Concatenate them back together&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Clearly, the &amp;#8220;split&amp;#8221; and &amp;#8220;concatenate&amp;#8221; steps are dual, and the recursive calls are dual by assumption. The &amp;#8220;merge&amp;#8221; and &amp;#8220;partition&amp;#8221; steps also seem like they should be dual to each other — one interleaves 2 lists to form 1 list, and the other distributes the elements of 1 list into 2 other lists. But I haven&amp;#8217;t been able to formulate them in such a way that they are really &amp;#8220;backwards&amp;#8221; versions of each other.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s possible that &amp;#8220;merge&amp;#8221; is actually dual to a function that extracts an increasing subsequence from a list, as in &lt;a href='http://en.wikipedia.org/wiki/Strand_sort'&gt;strand sort&lt;/a&gt;. If that turns out to be true, then strand sort is its own dual. That would be interesting to investigate.&lt;/p&gt;

&lt;h3 id='shouldnt_a_backwards_sort_unsort_a_list'&gt;Shouldn&amp;#8217;t a backwards sort&amp;#8230; unsort a list?&lt;/h3&gt;

&lt;p&gt;Well yeah, it should, but it can&amp;#8217;t. Sorting a list destroys information. You can pinpoint where that happens: the &lt;code&gt;sort&lt;/code&gt; function. If you give it &lt;code&gt;(4, 3)&lt;/code&gt; it will output &lt;code&gt;(3, 4)&lt;/code&gt;, but if you run it backwards, it can&amp;#8217;t know whether to turn &lt;code&gt;(3, 4)&lt;/code&gt; into &lt;code&gt;(4, 3)&lt;/code&gt; or &lt;code&gt;(3, 4)&lt;/code&gt; without some additional information. To think of it another way: a given list has only one sorted ordering but a very large number of unsorted orderings. One deterministic function can&amp;#8217;t turn a sorted list into each of the unsorted lists you could have started with.&lt;/p&gt;

&lt;p&gt;So these backwards functions are backwards in every respect except for the part that destroys information.&lt;/p&gt;

&lt;h3 id='can_this_be_done_automatically'&gt;Can this be done automatically?&lt;/h3&gt;

&lt;p&gt;Can you write a function that takes another function (or a suitable description thereof) as input and turns it inside out? Can we get new algorithms for free just by turning existing ones on their head? What happens if you run Dijkstra&amp;#8217;s algorithm backwards? I don&amp;#8217;t know!&lt;/p&gt;</description>
                <link>http://jliszka.github.io/2013/09/18/insertion-sort-is-dual-to-bubble-sort.html</link>
                <guid>http://jliszka.github.io/2013/09/18/insertion-sort-is-dual-to-bubble-sort</guid>
                <pubDate>2013-09-18T00:00:00-04:00</pubDate>
        </item>

        <item>
                <title>The 3 Things You Should Understand about Quantum Computation</title>
                <description>&lt;p&gt;I&amp;#8217;m working on a post about probablistic graphical models, but it&amp;#8217;s not done yet, so in the meantime here&amp;#8217;s a post about quantum probability.&lt;/p&gt;

&lt;h3 id='loaded_dice'&gt;Loaded dice&lt;/h3&gt;

&lt;p&gt;Let&amp;#8217;s say you have a loaded die with the following probability distribution:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th /&gt;&lt;th /&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;10%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;20%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;30%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;10%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;20%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;10%&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;How many pieces of information are encoded in a loaded die like this? It&amp;#8217;s weird to think of a probability distribution encoding information, but think of it this way: if you sent me this die in the mail, I could roll it a bunch of times to discover the probability for each face of the die. If you control how the die is weighted, you could send me a message that way.&lt;/p&gt;

&lt;p&gt;Anyway, the answer is that there are 5 pieces of information encoded in this distribution. (If you&amp;#8217;re not sure why it isn&amp;#8217;t 6, notice that once you specify 5 of the entries in the table, the 6th one is completely determined, since they all have to add up to 100%. So you can really only send me 5 numbers of your choosing this way.)&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h3 id='joint_probability_distributions'&gt;Joint probability distributions&lt;/h3&gt;

&lt;p&gt;How many pieces of information can you encode in 2 loaded dice? Obviously it&amp;#8217;s 10, you think, since each die can encode 5 pieces of information.&lt;/p&gt;

&lt;p&gt;But here&amp;#8217;s a (wrong) argument that it&amp;#8217;s 35. Instead of rolling each die separately to discover the probability distribution of each one, suppose I roll them together to discover their joint probability distribution. I&amp;#8217;ll get something like this:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th /&gt;&lt;th /&gt;&lt;th /&gt;&lt;th /&gt;&lt;th /&gt;&lt;th /&gt;&lt;th /&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;' /&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;5%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;10%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;3%&lt;/td&gt;&lt;td style='text-align: center;'&gt;3%&lt;/td&gt;&lt;td style='text-align: center;'&gt;3%&lt;/td&gt;&lt;td style='text-align: center;'&gt;3%&lt;/td&gt;&lt;td style='text-align: center;'&gt;3%&lt;/td&gt;&lt;td style='text-align: center;'&gt;15%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;5%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;10%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;5%&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Naïvely there are 35 pieces of information here (35 independent numbers that determine the 36th number, since they all add up to 100%). However, if you&amp;#8217;re clever enough you can &amp;#8220;factor&amp;#8221; this table and conclude that the first die has the distribution described above, and the second die has the following probabililty distribution:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th /&gt;&lt;th /&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;10%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;10%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;10%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;10%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;10%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;50%&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;You can see that the 6 x 6 table above is the outer product of the two single-column tables. So there really are only 10 numbers that determine that entire table.&lt;/p&gt;

&lt;p&gt;That&amp;#8217;s the nature of classical probability — joint probability distributions of independent events always &amp;#8220;factor&amp;#8221; into individual probability distributions for each event. You can&amp;#8217;t encode any 35 numbers you like into the joint probability distribution of two dice, because it might not factor.&lt;/p&gt;

&lt;p&gt;&amp;#8230; unless your dice happen to be quantum dice.&lt;/p&gt;

&lt;h3 id='quantum_dice'&gt;Quantum dice&lt;/h3&gt;

&lt;p&gt;With quantum dice, you &lt;em&gt;can&lt;/em&gt; actually construct a joint probability distribution that doesn&amp;#8217;t factor. For example:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th /&gt;&lt;th /&gt;&lt;th /&gt;&lt;th /&gt;&lt;th /&gt;&lt;th /&gt;&lt;th /&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;' /&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;5%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;10%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;3%&lt;/td&gt;&lt;td style='text-align: center;'&gt;3%&lt;/td&gt;&lt;td style='text-align: center;'&gt;5%&lt;/td&gt;&lt;td style='text-align: center;'&gt;3%&lt;/td&gt;&lt;td style='text-align: center;'&gt;3%&lt;/td&gt;&lt;td style='text-align: center;'&gt;15%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;5%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;2%&lt;/td&gt;&lt;td style='text-align: center;'&gt;10%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;1%&lt;/td&gt;&lt;td style='text-align: center;'&gt;5%&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Notice the 0% in the (2, 2) cell. This table won&amp;#8217;t factor because in order for that entry to be 0%, one of the dice has to have a 0% chance of landing on a 2, which means that entire row (or column) would be 0%.&lt;/p&gt;

&lt;p&gt;But think of the implications of a distribution like this. It means if you roll a 2 with one of the dice, you are guaranteed not to roll a 2 with the other — no matter what order you roll them in, or even you fly one of the dice to the opposite side of the world and roll them at the same time.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s almost as if there&amp;#8217;s a tiny mechanism inside each of the dice that detects when it has landed on a certain face, and transmits a message to the other die that causes it to adjust some tiny internal servos that change how it&amp;#8217;s weighted.&lt;/p&gt;

&lt;p&gt;Except that it has been demonstrated in a lab that if that were the case, that message would have to travel faster than the speed of light. In quantum mechanical terms, the two dice are &amp;#8220;entangled.&amp;#8221;&lt;/p&gt;

&lt;h3 id='3_things_that_make_quantum_computation_possible'&gt;3 things that make quantum computation possible&lt;/h3&gt;

&lt;p&gt;It&amp;#8217;s kind of irrelevant to the field of quantum computation what mechanism produces this weird behavior. The important things are:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. You can encode 35 numbers in the joint probability distribution of two quantum dice.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In reality, you deal with quantum bits (qubits), not quantum dice. A 10-qubit quantum computer has &lt;script type='math/tex'&gt;2^{10}-1&lt;/script&gt; slots to store values. (Think about the joint probability distribution table for 10 quantum bits — it has &lt;script type='math/tex'&gt;2^{10}&lt;/script&gt; entries, one for each possible outcome, the last one of which is constrained by all the others such that they add up to 100%.) Compare this with 10 classical bits, which provides only 10 slots to store either a 0 or a 1. This is where quantum computers get their reputation for the ability to store a huge amount of data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. A quantum computer performs operations on the entire joint probability distribution at once.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I don&amp;#8217;t really understand the mechanics of how this is actually done in a lab, but suffice it to say that in order to produce crazy non-factoring joint probabilty distributions like the one above, you essentially apply matrix operations called quantum gates on joint probability distribution tables. Each gate works in constant time, regardless of the size of the table. This is where quantum computers get their reputation for massively parallel processing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Quantum probabilities are not restricted to real numbers between 0 and 1.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Instead they are restricted to &lt;em&gt;complex&lt;/em&gt; numbers with modulus between 0 and 1. This allows interference effects to happen, which is what makes any interesting quantum algorithms possible. More on this later.&lt;/p&gt;

&lt;h3 id='the_catch'&gt;The catch&lt;/h3&gt;

&lt;p&gt;The annoying thing about quantum computers is that you can&amp;#8217;t actually &amp;#8220;roll the dice&amp;#8221; as many times as you want to discover what the entire joint probability distribution looks like. As soon as you roll them once (i.e., perform a measurement), the entire thing collapses into a single classical state — the dice show a 3 and a 4 (for example), and the entangled state you worked so hard to construct is gone. In its place you have this:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th /&gt;&lt;th /&gt;&lt;th /&gt;&lt;th /&gt;&lt;th /&gt;&lt;th /&gt;&lt;th /&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;' /&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;100%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;So even though quantum computers can technically represent a huge amount of information in a tiny number of qubits, you can&amp;#8217;t get at most of it! The way some quantum algorithms work is by contriving a joint probability distribution where most of the probability is concentrated in the &amp;#8220;answer&amp;#8221; you want to get out. When you perform the measurement, you can then observe (with high likelihood) where all the probability ended up. In a 10 qubit computer, for example, that measurement gives you a single 10-bit result.&lt;/p&gt;

&lt;h3 id='demo_time'&gt;Demo time&lt;/h3&gt;

&lt;p&gt;I actually have some code for this. It&amp;#8217;s mostly cribbed from &lt;a href='http://sigfpe.wordpress.com/2007/03/04/monads-vector-spaces-and-quantum-mechanics-pt-ii/'&gt;sigfpe&amp;#8217;s vector space monad&lt;/a&gt;. I put it together while taking the &lt;a href='https://class.coursera.org/qcomp-2012-001/class/index'&gt;Quantum Computation Coursera&lt;/a&gt;, just so I wouldn&amp;#8217;t have to do all the math by hand. It turned out to be pretty useful! Here&amp;#8217;s a quick demo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; s0
res0: Q[Basis.Std] = 1.0|0&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a very simple quantum state equivalent to the following probability distribution table:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th /&gt;&lt;th /&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;100%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;script type='math/tex'&gt;\newcommand{\ket}[1]{\left| #1 \right&gt;}&lt;/script&gt;
&lt;p&gt;State labels are written using &lt;em&gt;ket&lt;/em&gt; notaton. &lt;script type='math/tex'&gt;\ket{0}&lt;/script&gt; refers to the 0 row in the table above. The number in front of the label represents the probability for that row in the table — actually, it&amp;#8217;s a probability amplitude, which is a complex number whose squared absolute value gives the classical probability of that state. This will make more sense in a second.&lt;/p&gt;

&lt;p&gt;But first, let&amp;#8217;s apply a quantum gate to this state:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; s0 &amp;gt;&amp;gt;= H
res1: Q[Basis.Std] = 0.707107|0&amp;gt; + 0.707107|1&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is &lt;script type='math/tex'&gt;\frac{1}{\sqrt{2}}\ket{0} + \frac{1}{\sqrt{2}}\ket{1}&lt;/script&gt;. It corresponds to the following (classical) probability distribution table:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th /&gt;&lt;th /&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;50%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;50%&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;since &lt;script type='math/tex'&gt;|\frac{1}{\sqrt{2}}|^2 = \frac{1}{2}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Notice that &lt;script type='math/tex'&gt;\frac{1}{\sqrt{2}}\ket{0} - \frac{1}{\sqrt{2}}\ket{1}&lt;/script&gt; corresponds to the same table, and so does &lt;script type='math/tex'&gt;\frac{-i}{\sqrt{2}}\ket{0} - \frac{1}{\sqrt{2}}\ket{1}&lt;/script&gt;, since &lt;script type='math/tex'&gt;|\frac{-i}{\sqrt{2}}|^2 = |\frac{-1}{\sqrt{2}}|^2 = \frac{1}{2}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;One qubit only gets you so far. So let&amp;#8217;s create a 2 qubit state.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; tensor(s0, s0)
res2: Q[T[Basis.Std,Basis.Std]] = 1.0|00&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The state label now contains 2 bits. This state corresponds to this table:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th /&gt;&lt;th /&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;00&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;100%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;01&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;10&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;11&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Now we&amp;#8217;ll apply the H gate to both qubits:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; tensor(s0, s0) &amp;gt;&amp;gt;= lift12(H, H)
res3: Q[T[Basis.Std,Basis.Std]] = 0.5|00&amp;gt; + 0.5|01&amp;gt; + 0.5|10&amp;gt; + 0.5|11&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or just to the first qubit:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; tensor(s0, s0) &amp;gt;&amp;gt;= lift1(H)
res4: Q[T[Basis.Std,Basis.Std]] = 0.707107|00&amp;gt; + 0.707107|10&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are some gates that operate on two qubits at once. The CNOT gate, for example, flips the second qubit only if the first qubit is a 1.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val s = tensor(s0, s0) &amp;gt;&amp;gt;= lift1(H) &amp;gt;&amp;gt;= cnot
s: Q[T[Basis.Std,Basis.Std]] = 0.707107|00&amp;gt; + 0.707107|11&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That corresponds to this table:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th /&gt;&lt;th /&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;00&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;50%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;01&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;10&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;0%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: center;'&gt;&lt;strong&gt;11&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: center;'&gt;50%&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;There, wait! We now have a pair of entangled qubits. They&amp;#8217;re like 2 quantum coins that always land both heads or both tails, even if you flip them at the exact same time on opposite sides of the Earth. This is called the &lt;a href='http://en.wikipedia.org/wiki/Bell_state'&gt;Bell state&lt;/a&gt; and comes up all the time in quantum algorithms.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s see what happens when we measure the first qubit:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val (m, s2) = s.measure(_._1)
m: Basis.Std = |1&amp;gt;
s2: Q[T[Basis.Std,Basis.Std]] = 1.0|11&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The result of the measurement is 2 things: the outcome of the measurement itself — &lt;code&gt;m&lt;/code&gt;, &lt;script type='math/tex'&gt;\ket{1}&lt;/script&gt; — and the new state of the system — &lt;code&gt;s2&lt;/code&gt;, &lt;script type='math/tex'&gt;1.0\ket{11}&lt;/script&gt;. The measurement gave us one of the possible states, at random, according to its probability amplitude. The act of measuring changes the state, eliminating all states that are inconsistent with that outcome. So now if we measure the second qubit, we are guaranteed to get &lt;script type='math/tex'&gt;\ket{1}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s another example of that.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val s = tensor(s0, s0) &amp;gt;&amp;gt;= lift12(H, H)
s: Q[T[Basis.Std,Basis.Std]] = 0.5|00&amp;gt; + 0.5|01&amp;gt; + 0.5|10&amp;gt; + 0.5|11&amp;gt;

scala&amp;gt; val (m, s2) = s.measure(_._2)
m: Basis.Std = |0&amp;gt;
s2: Q[T[Basis.Std,Basis.Std]] = 0.707107|00&amp;gt; + 0.707107|10&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This time we measured the second qubit, getting &lt;script type='math/tex'&gt;\ket{0}&lt;/script&gt;, and you can see that the only states remaining are the ones where the second qubit is 0.&lt;/p&gt;

&lt;h3 id='interference'&gt;Interference&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m going to quickly show you how interference effects work. Suppose I have a quantum gate that performs the following transformation on states: &lt;script type='math/tex; mode=display'&gt;
%&lt;![CDATA[
\ket{0} \rightarrow \frac{1}{\sqrt{2}}\ket{0} + \frac{1}{\sqrt{2}}\ket{1} \\
\ket{1} \rightarrow \frac{-1}{\sqrt{2}}\ket{0} + \frac{1}{\sqrt{2}}\ket{1} \\
%]]&gt;
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m going to call this gate &lt;code&gt;sqrtNot&lt;/code&gt; for reasons that will soon become apparent. Let&amp;#8217;s see it in action.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; s0 &amp;gt;&amp;gt;= sqrtNot
res0: Q[Basis.Std] = 0.707107|0&amp;gt; + 0.707107|1&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK, we&amp;#8217;ve turned a pure &lt;script type='math/tex'&gt;\ket{0}&lt;/script&gt; state into an even mix of &lt;script type='math/tex'&gt;\ket{0}&lt;/script&gt; and &lt;script type='math/tex'&gt;\ket{1}&lt;/script&gt;. In other words, we took a coin that always lands heads and &amp;#8220;randomized&amp;#8221; it into a completely fair coin.&lt;/p&gt;

&lt;p&gt;Now let&amp;#8217;s run it through the &lt;code&gt;sqrtNot&lt;/code&gt; gate again and see what happens.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; s0 &amp;gt;&amp;gt;= sqrtNot &amp;gt;&amp;gt;= sqrtNot
res1: Q[Basis.Std] = 1.0|1&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Weird! We now have a coin that always lands tails. (That&amp;#8217;s why it&amp;#8217;s called &lt;code&gt;sqrtNot&lt;/code&gt; — applying it twice inverts the state.) How does that work? Let&amp;#8217;s do the math.&lt;/p&gt;
&lt;script type='math/tex; mode=display'&gt;
%&lt;![CDATA[
\begin{align}
    \text{sqrtNot}(\frac{1}{\sqrt{2}}\ket{0} + \frac{1}{\sqrt{2}}\ket{1})
    &amp;= \frac{1}{\sqrt{2}}(\frac{1}{\sqrt{2}}\ket{0} + \frac{1}{\sqrt{2}}\ket{1}) + \frac{1}{\sqrt{2}}(\frac{-1}{\sqrt{2}}\ket{0} + \frac{1}{\sqrt{2}}\ket{1})
    \\ &amp;= \frac{1}{2}\ket{0} + \frac{1}{2}\ket{1} - \frac{1}{2}\ket{0} + \frac{1}{2}\ket{1}
    \\ &amp;= 1\ket{1}
\end{align}
%]]&gt;
&lt;/script&gt;
&lt;p&gt;The &lt;script type='math/tex'&gt;\ket{0}&lt;/script&gt; got cancelled out. That would never happen in classical probability!&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s keep going:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; s0 &amp;gt;&amp;gt;= sqrtNot &amp;gt;&amp;gt;= sqrtNot &amp;gt;&amp;gt;= sqrtNot
res2: Q[Basis.Std] = -0.707107|0&amp;gt; + 0.707107|1&amp;gt;

scala&amp;gt; s0 &amp;gt;&amp;gt;= sqrtNot &amp;gt;&amp;gt;= sqrtNot &amp;gt;&amp;gt;= sqrtNot &amp;gt;&amp;gt;= sqrtNot
res3: Q[Basis.Std] = -1.0|0&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we&amp;#8217;re back to a coin that always lands heads. (We flipped the sign, but remember only the squared absolute value really matters.)&lt;/p&gt;

&lt;p&gt;For kicks, let&amp;#8217;s see what happens when we introduce another qubit into the mix:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; bell
res4: Q[T[Basis.Std,Basis.Std]] = 0.707107|00&amp;gt; + 0.707107|11&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These qubits happen to be entangled, but that shouldn&amp;#8217;t affect our application of &lt;code&gt;sqrtNot&lt;/code&gt; to the first qubit, should it?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; bell &amp;gt;&amp;gt;= lift1(sqrtNot)
res5: Q[T[Basis.Std,Basis.Std]] = 0.5|00&amp;gt; + -0.5|01&amp;gt; + 0.5|10&amp;gt; + 0.5|11&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Oops! The interference effects disappeared. The first qubit now behaves like a classical fair coin — no matter what we do to it, we can&amp;#8217;t recover those interference effects and get things to cancel. I think this is called decoherence (although some sources I&amp;#8217;ve read says this is not the same as decoherence) and is what makes building actual quantum computers difficult — preventing stray particles from coming in, accidentally getting entangled with the qubits in your quantum computer, and flying off to Pluto where you can&amp;#8217;t do anything to unentangle it.&lt;/p&gt;

&lt;p&gt;Anyway, this is kind of fun to play with! If you&amp;#8217;re interested in checking it out, the code is available &lt;a href='https://github.com/jliszka/quantum-probability-monad'&gt;on github&lt;/a&gt;.&lt;/p&gt;</description>
                <link>http://jliszka.github.io/2013/09/09/the-3-things-you-should-understand-about-quantum-computing.html</link>
                <guid>http://jliszka.github.io/2013/09/09/the-3-things-you-should-understand-about-quantum-computing</guid>
                <pubDate>2013-09-09T00:00:00-04:00</pubDate>
        </item>

        <item>
                <title>Fun with Bayesian Priors</title>
                <description>&lt;p&gt;Say you have a biased coin, but you don&amp;#8217;t know what the &amp;#8220;true&amp;#8221; bias is. You flip the coin 10 times and observe 8 heads. What can you say now about the true bias?&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s easy to say that the most likely bias is 0.8. That&amp;#8217;s accurate, but maybe you also want to know what other biases are likely. How likely is it that you have a fair coin? Can you rule out having a bias as low as 0.4?&lt;/p&gt;

&lt;p&gt;This sounds like a classic problem in Bayesian inference, but I&amp;#8217;m going to take a different tack — simulation. To make this a little easier I&amp;#8217;ll use a &lt;a href='/2013/08/12/a-frequentist-approach-to-probability.html'&gt;Scala library based on the Monte Carlo method&lt;/a&gt; that I&amp;#8217;ve been working on as an exercise in trying to better understand &lt;a href='/2013/08/19/climbing-the-probability-distribution-ladder.html'&gt;some&lt;/a&gt; &lt;a href='/2013/08/26/a-programmers-guide-to-the-central-limit-theorem.html'&gt;ideas&lt;/a&gt; in probability and statistics.&lt;/p&gt;

&lt;h3 id='the_simulation'&gt;The simulation&lt;/h3&gt;

&lt;p&gt;A single trial in the simulation will look like this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Choose a bias at random&lt;/li&gt;

&lt;li&gt;Flip a coin with that bias 10 times&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After, say, 10,000 trials, you look at all the times you got 8 heads, and see what the bias happened to be in each of those trials. The percent of the time each bias comes up in this subset of trials gives the probability (the &amp;#8220;posterior&amp;#8221; probability) that that bias is the &amp;#8220;true&amp;#8221; bias.&lt;/p&gt;

&lt;p&gt;When you start to code this up, one question jumps out: In step 1, when you choose a bias &amp;#8220;at random,&amp;#8221; what distribution do you draw it from?&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;The most reasonable choice is the uniform distribution between 0 and 1. This makes sense if you want to assume no particular prior knowledge about what the true bias is — all biases are equally likely. Later on we&amp;#8217;ll see what happens to the posterior distribution when you start with different distributions representing prior knowledge about the bias (commonly just called the &amp;#8220;prior&amp;#8221;).&lt;/p&gt;

&lt;p&gt;So first, we&amp;#8217;ll need a case class that represents the outcome of one trial:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='k'&gt;class&lt;/span&gt; &lt;span class='nc'&gt;Trial&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;bias&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;heads&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And here&amp;#8217;s the simulation itself:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;experiment&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Trial&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;bias&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;uniform&lt;/span&gt;
    &lt;span class='n'&gt;heads&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;binomial&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;bias&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt; &lt;span class='k'&gt;yield&lt;/span&gt; &lt;span class='nc'&gt;Trial&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;bias&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;heads&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;(I recommend reading &lt;a href='/2013/08/12/a-frequentist-approach-to-probability.html'&gt;this writeup of the probability distribution monad&lt;/a&gt; if you haven&amp;#8217;t seen this before.)&lt;/p&gt;

&lt;p&gt;The bias is drawn from the uniform distribution, and a &lt;a href='/2013/08/19/climbing-the-probability-distribution-ladder.html'&gt;binomial distribution&lt;/a&gt; represents the number of heads you will see in 10 coin flips when the probability of seeing a head on a single coin flip is determined by the value of &lt;code&gt;bias&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now let&amp;#8217;s analyze the experiment. Remember we only care about the trials that resulted in 8 heads.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;posterior&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;experiment&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;given&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;heads&lt;/span&gt; &lt;span class='o'&gt;==&lt;/span&gt; &lt;span class='mi'&gt;8&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;bias&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let&amp;#8217;s see what it looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; posterior.bucketedHist(0, 1, 20)
0.00  0.00% 
0.05  0.00% 
0.10  0.00% 
0.15  0.00% 
0.20  0.01% 
0.25  0.03% 
0.30  0.09% 
0.35  0.18% 
0.40  0.63% 
0.45  1.37% #
0.50  2.32% ##
0.55  3.85% ###
0.60  6.69% ######
0.65  9.35% #########
0.70 12.73% ############
0.75 15.49% ###############
0.80 16.91% ################
0.85 15.08% ###############
0.90 10.60% ##########
0.95  4.48% ####
1.00  0.19% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That looks pretty good! It&amp;#8217;s clear that 0.8 is the most likely bias, as expected.&lt;/p&gt;

&lt;h3 id='chaining_posteriors'&gt;Chaining posteriors&lt;/h3&gt;

&lt;p&gt;Alright, now suppose you flip the same coin 10 more times and get only 6 heads. This can be modeled the same way, only this time, instead of using &lt;code&gt;uniform&lt;/code&gt; as the prior distribution for the bias, you can use &lt;code&gt;posterior&lt;/code&gt;, essentially building on top of our new belief about what the bias is from the first experiment.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;experiment2&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;bias&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;posterior&lt;/span&gt;
    &lt;span class='n'&gt;heads&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;binomial&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;bias&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt; &lt;span class='k'&gt;yield&lt;/span&gt; &lt;span class='nc'&gt;Trial&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;bias&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;heads&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;posterior2&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;experiment2&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;given&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;heads&lt;/span&gt; &lt;span class='o'&gt;==&lt;/span&gt; &lt;span class='mi'&gt;6&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;bias&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;scala&amp;gt; posterior2.bucketedHist(0, 1, 20)
0.00  0.00% 
0.05  0.00% 
0.10  0.00% 
0.15  0.00% 
0.20  0.00% 
0.25  0.00% 
0.30  0.00% 
0.35  0.18% 
0.40  0.55% 
0.45  1.84% #
0.50  4.25% ####
0.55  7.79% #######
0.60 12.91% ############
0.65 17.86% #################
0.70 19.41% ###################
0.75 17.66% #################
0.80 11.37% ###########
0.85  4.98% ####
0.90  1.14% #
0.95  0.06% 
1.00  0.00% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great, exactly what you would expect. The distribution has shifted towards 0.7 (14/20) and has narrowed a bit.&lt;/p&gt;

&lt;h3 id='i_cant_not_abstract_this_out_into_a_method'&gt;I can&amp;#8217;t not abstract this out into a method&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;ve written almost the exact same code twice, so I basically have to do this now. Here&amp;#8217;s my attempt, as an instance method of the &lt;code&gt;Distribution&lt;/code&gt; interface:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;trait&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='c1'&gt;// ...&lt;/span&gt;
  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;posterior&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;B&lt;/span&gt;&lt;span class='o'&gt;](&lt;/span&gt;&lt;span class='n'&gt;experiment&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;A&lt;/span&gt; &lt;span class='o'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;B&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt;
                   &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;observed&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;B&lt;/span&gt; &lt;span class='o'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='nc'&gt;Boolean&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='k'&gt;class&lt;/span&gt; &lt;span class='nc'&gt;Trial&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;a&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;outcome&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;B&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
    &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;d&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='n'&gt;a&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='k'&gt;this&lt;/span&gt;
      &lt;span class='n'&gt;e&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;experiment&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;a&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt; &lt;span class='k'&gt;yield&lt;/span&gt; &lt;span class='nc'&gt;Trial&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;a&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;e&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
    &lt;span class='n'&gt;d&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;given&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;t&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;observed&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;t&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;outcome&lt;/span&gt;&lt;span class='o'&gt;)).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;a&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The idea is that &lt;code&gt;posterior&lt;/code&gt; updates a prior distribution according to the outcome of some experiment. It returns the new posterior after running the provided &lt;code&gt;experiment&lt;/code&gt;, which depends on values sampled from the prior, and in which only certain outcomes are observed. The &lt;code&gt;observed&lt;/code&gt; parameter is a function that indicates what outcomes were actually observed in the experiment and which were not.&lt;/p&gt;

&lt;p&gt;So now our first two experiments become:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;p1&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;uniform&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;posterior&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;bias&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;binomial&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;bias&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt;&lt;span class='o'&gt;))(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt; &lt;span class='o'&gt;==&lt;/span&gt; &lt;span class='mi'&gt;8&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;p2&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;p1&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;posterior&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;bias&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;binomial&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;bias&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt;&lt;span class='o'&gt;))(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt; &lt;span class='o'&gt;==&lt;/span&gt; &lt;span class='mi'&gt;6&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Pretty clean!&lt;/p&gt;

&lt;p&gt;We can eyeball that &lt;code&gt;p2&lt;/code&gt; gives the same result as flipping a coin 20 times and observing 14 heads:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; uniform.posterior(bias =&amp;gt; binomial(bias, 20))(_ == 14).bucketedHist(0, 1, 20)
0.00  0.00% 
0.05  0.00% 
0.10  0.00% 
0.15  0.00% 
0.20  0.00% 
0.25  0.00% 
0.30  0.06% 
0.35  0.13% 
0.40  0.53% 
0.45  1.83% #
0.50  3.88% ###
0.55  8.20% ########
0.60 13.40% #############
0.65 17.61% #################
0.70 19.28% ###################
0.75 18.30% ##################
0.80 11.33% ###########
0.85  4.40% ####
0.90  0.98% 
0.95  0.07% 
1.00  0.00% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that more trials gives a narrower distribution:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; uniform.posterior(bias =&amp;gt; binomial(bias, 100))(_ == 72).bucketedHist(0, 1, 20)
0.00  0.00% 
0.05  0.00% 
0.10  0.00% 
0.15  0.00% 
0.20  0.00% 
0.25  0.00% 
0.30  0.00% 
0.35  0.00% 
0.40  0.00% 
0.45  0.00% 
0.50  0.01% 
0.55  0.14% 
0.60  2.35% ##
0.65 15.58% ###############
0.70 39.42% #######################################
0.75 33.53% #################################
0.80  8.60% ########
0.85  0.37% 
0.90  0.00% 
0.95  0.00% 
1.00  0.00% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Neat!&lt;/p&gt;

&lt;h3 id='does_the_posterior_distribution_have_a_memory'&gt;Does the posterior distribution have a memory?&lt;/h3&gt;

&lt;p&gt;My intuition says that if I flip a coin 10 times and get 2 heads (20%), and then flip it 30 times and get 3 heads (10%), the combined posterior should say that the most likely bias is somewhere between 20% and 10%, but closer to 10% because the 30 flips should count more than the 10 flips. In fact it should be exactly 12.5% since we observed 5 total heads in 40 total flips.&lt;/p&gt;

&lt;p&gt;But does this technique of chaining posteriors actually give that result? After I generate the first posterior distribution from the 10-flip experiment, isn&amp;#8217;t the information that I flipped it 10 times lost somehow? Let&amp;#8217;s see.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val p1 = uniform.posterior(bias =&amp;gt; binomial(bias, 10))(_ == 2)
p1: Distribution[Double] = &amp;lt;distribution&amp;gt;

scala&amp;gt; p1.bucketedHist(0, 1, 20)
0.00  0.28% 
0.05  4.14% ####
0.10 10.45% ##########
0.15 15.02% ###############
0.20 16.38% ################
0.25 15.37% ###############
0.30 12.85% ############
0.35  9.65% #########
0.40  6.68% ######
0.45  4.30% ####
0.50  2.54% ##
0.55  1.37% #
0.60  0.58% 
0.65  0.27% 
0.70  0.10% 
0.75  0.02% 
0.80  0.00% 
0.85  0.00% 
0.90  0.00% 
0.95  0.00% 
1.00  0.00% 

scala&amp;gt; val p2 = p1.posterior(bias =&amp;gt; binomial(bias, 30))(_ == 3)
p2: Distribution[Double] = &amp;lt;distribution&amp;gt;

scala&amp;gt; p2.bucketedHist(0, 0.5, 20)
0.000  0.00% 
0.025  0.41% 
0.050  3.89% ###
0.075 10.21% ##########
0.100 16.69% ################
0.125 18.80% ##################
0.150 17.77% #################
0.175 12.85% ############
0.200  9.02% #########
0.225  5.06% #####
0.250  2.93% ##
0.275  1.46% #
0.300  0.44% 
0.325  0.37% 
0.350  0.04% 
0.375  0.04% 
0.400  0.02% 
0.425  0.00% 
0.450  0.00% 
0.475  0.00% 
0.500  0.00% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hm, yeah, it sure looks like it worked! The most likely bias is 12.5%.&lt;/p&gt;

&lt;p&gt;So how does this work? Well, &lt;code&gt;p1&lt;/code&gt; actually does encode how many flips went into it — more flips translates into a narrower distribution, and fewer flips will produce a distribution that is more spread out. This is pretty easily illustrated: if instead we had done 20 flips and gotten 4 heads, or 40 flips and gotten 8 heads, the resulting posterior distributions would have looked different, even though these each encode the same 20% bias.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; uniform.posterior(bias =&amp;gt; binomial(bias, 20))(_ == 4).bucketedHist(0, 1, 20)
0.00  0.00% 
0.05  1.61% #
0.10  9.57% #########
0.15 18.74% ##################
0.20 22.51% ######################
0.25 19.40% ###################
0.30 14.06% ##############
0.35  8.03% ########
0.40  3.84% ###
0.45  1.57% #
0.50  0.48% 
0.55  0.13% 
0.60  0.06% 
0.65  0.00% 
0.70  0.00% 
0.75  0.00% 
0.80  0.00% 
0.85  0.00% 
0.90  0.00% 
0.95  0.00% 
1.00  0.00% 

scala&amp;gt; uniform.posterior(bias =&amp;gt; binomial(bias, 40))(_ == 8).bucketedHist(0, 1, 20)
0.00  0.00% 
0.05  0.39% 
0.10  6.15% ######
0.15 22.06% ######################
0.20 30.70% ##############################
0.25 23.72% #######################
0.30 12.06% ############
0.35  3.85% ###
0.40  0.90% 
0.45  0.16% 
0.50  0.01% 
0.55  0.00% 
0.60  0.00% 
0.65  0.00% 
0.70  0.00% 
0.75  0.00% 
0.80  0.00% 
0.85  0.00% 
0.90  0.00% 
0.95  0.00% 
1.00  0.00% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The shape of the prior distribution naturally affects the posteriors that result from it.&lt;/p&gt;

&lt;h3 id='fun_with_priors'&gt;Fun with priors&lt;/h3&gt;

&lt;p&gt;Let&amp;#8217;s see exactly how that plays out by feeding in different priors and see what posterior distributions come out.&lt;/p&gt;

&lt;p&gt;Suppose we start with some knowledge that coin favors tails over heads. So we know the bias is less than 0.5. We&amp;#8217;ll model this with a uniform distribution between 0 and 0.5.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val prior = uniform.given(_ &amp;lt; 0.5)
prior: Distribution[Double] = &amp;lt;distribution&amp;gt;

scala&amp;gt; prior.posterior(bias =&amp;gt; binomial(bias, 10))(_ == 8).bucketedHist(0, 1, 20, roundDown = true)
0.00  0.00% 
0.05  0.00% 
0.10  0.00% 
0.15  0.01% 
0.20  0.33% 
0.25  1.47% #
0.30  4.42% ####
0.35 11.61% ###########
0.40 26.93% ##########################
0.45 55.23% #######################################################
0.50  0.00% 
0.55  0.00% 
0.60  0.00% 
0.65  0.00% 
0.70  0.00% 
0.75  0.00% 
0.80  0.00% 
0.85  0.00% 
0.90  0.00% 
0.95  0.00% 
1.00  0.00% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Makes sense, all the probabily mass crowds as close to 0.5 as it can.&lt;/p&gt;

&lt;p&gt;Now let&amp;#8217;s try something a little silly — say someone tells us that they don&amp;#8217;t know what the bias is, but it is definitely &lt;em&gt;not&lt;/em&gt; between 0.7 and 0.8.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val prior = uniform.given(x =&amp;gt; x &amp;lt;= 0.7 || x &amp;gt;= 0.8)
prior: Distribution[Double] = &amp;lt;distribution&amp;gt;

scala&amp;gt; prior.posterior(bias =&amp;gt; binomial(bias, 10))(_ == 8).bucketedHist(0, 1, 20, roundDown = true)
0.00  0.00% 
0.05  0.00% 
0.10  0.00% 
0.15  0.00% 
0.20  0.03% 
0.25  0.02% 
0.30  0.20% 
0.35  0.39% 
0.40  1.21% #
0.45  2.79% ##
0.50  4.67% ####
0.55  7.52% #######
0.60 11.68% ###########
0.65 16.20% ################
0.70  0.00% 
0.75  0.00% 
0.80 23.96% #######################
0.85 18.53% ##################
0.90 10.52% ##########
0.95  2.28% ##
1.00  0.00% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fun! Makes perfect sense though, the prior distribution isn&amp;#8217;t generating any biases between 0.7 and 0.8, so it&amp;#8217;s not going to show up in the results.&lt;/p&gt;

&lt;p&gt;Now let&amp;#8217;s say we know the bias is either 0.5 or 0.9 (we either have a perfectly fair coin or a very biased coin). Our prior is then:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val prior = discreteUniform(List(0.5, 0.9))
prior: Distribution[Double] = &amp;lt;distribution&amp;gt;

scala&amp;gt; prior.bucketedHist(0, 1, 10)
0.0  0.00% 
0.1  0.00% 
0.2  0.00% 
0.3  0.00% 
0.4  0.00% 
0.5 49.76% #################################################
0.6  0.00% 
0.7  0.00% 
0.8  0.00% 
0.9 50.24% ##################################################
1.0  0.00% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now after flipping the coin 10 times and observing 8 heads, the posterior becomes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; prior.posterior(bias =&amp;gt; binomial(bias, 10))(_ == 8).bucketedHist(0, 1, 10)
0.0  0.00% 
0.1  0.00% 
0.2  0.00% 
0.3  0.00% 
0.4  0.00% 
0.5 18.40% ##################
0.6  0.00% 
0.7  0.00% 
0.8  0.00% 
0.9 81.60% #################################################################################
1.0  0.00% &lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='bayes_theorem'&gt;Bayes&amp;#8217; theorem&lt;/h3&gt;

&lt;p&gt;It&amp;#8217;s pretty easy to use Bayes&amp;#8217; theorem to analyze that last example, so let&amp;#8217;s walk through it and compare.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the formula applied to this example: &lt;script type='math/tex; mode=display'&gt;
%&lt;![CDATA[
\begin{align}
  P(A|B) &amp;= \frac{P(B|A)P(A)}{P(B)}
  \\ P(\text{fair coin}|\text{8 heads})
     &amp;= \frac{P(\text{8 heads}|\text{fair coin})P(\text{fair coin})}{P(\text{8 heads})}
  \\ &amp;= \frac{P(\text{8 heads}|\text{fair coin})P(\text{fair coin})}{P(\text{8 heads}|\text{fair coin})P(\text{fair coin}) + P(\text{8 heads}|\text{biased coin})P(\text{biased coin})}
  \\ &amp;= \frac{ {10 \choose 8} (\frac{1}{2})^{10} \cdot \frac{1}{2}}
          { {10 \choose 8} (\frac{1}{2})^{10} \cdot \frac{1}{2} + {10 \choose 8} (\frac{9}{10})^8 (\frac{1}{10})^2 \cdot \frac{1}{2}}
  \\ &amp;= \frac{0.022}{0.022 + 0.097} = 0.18
\end{align}
%]]&gt;
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;So we got the same result. Written out, you can see the correspondence between Bayes&amp;#8217; Theorem and our simulation. &lt;script type='math/tex'&gt;P(\text{fair coin})&lt;/script&gt; and &lt;script type='math/tex'&gt;P(\text{biased coin})&lt;/script&gt; in the denominator play the same role as our prior in regulating how often we&amp;#8217;re using each bias. Then it becomes a simple fraction to determine the probability that the coin is fair — it&amp;#8217;s just the fraction of the number of times you observe 8 heads that are accounted for by using a fair coin. There&amp;#8217;s a slight mismatch here, in that this formula deals with probabilities, whereas I&amp;#8217;m talking about the number of times you observe certain outcomes. But this is easily enough explained — if you multiply the numerator and denominator by the number of trials you plan on running, you will have converted the probabilities into numbers of successes in that many trials, without changing the value of the fraction.&lt;/p&gt;

&lt;h3 id='conclusion'&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;In Bayesian probability, the prior distribution reflects your degree of belief that an unknown quantity takes on particular values. It represents your uncertainty, rather than the relative frequencies of observing particular events, as is the case with the frequentist interpretation of probability.&lt;/p&gt;

&lt;p&gt;However, we&amp;#8217;ve seen that the prior can acquiesce to a frequentist interpretation. We&amp;#8217;ve essentially turned the prior into a machine that regulates how often we&amp;#8217;re allowed to see certain values of an unknown quantity in our experiments, and the observed outcomes of experiments will be used to refine the output of the machine.&lt;/p&gt;</description>
                <link>http://jliszka.github.io/2013/09/03/fun-with-bayesian-priors.html</link>
                <guid>http://jliszka.github.io/2013/09/03/fun-with-bayesian-priors</guid>
                <pubDate>2013-09-03T00:00:00-04:00</pubDate>
        </item>

        <item>
                <title>A Programmer's Guide to the Central Limit Theorem</title>
                <description>&lt;p&gt;This post is a continuation of a series of posts about exploring probability distributions through code. The first post is &lt;a href='/2013/08/12/a-frequentist-approach-to-probability.html'&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this post I&amp;#8217;m going to look at the Central Limit Theorem.&lt;/p&gt;

&lt;h3 id='sample_means'&gt;Sample means&lt;/h3&gt;

&lt;p&gt;Suppose I have a random variable whose underlying distribution is unknown to me. I take sample of a reasonable size (say 100) and find the mean of the sample. What can I say about the relationship between the true mean and the mean of the sample?&lt;/p&gt;

&lt;p&gt;The most comprehensive answer to this is to look at the distribution of the sample mean.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;sampleMean&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;d&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;],&lt;/span&gt; &lt;span class='n'&gt;n&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='mi'&gt;100&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;d&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;repeat&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;n&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;sum&lt;/span&gt; &lt;span class='o'&gt;/&lt;/span&gt; &lt;span class='n'&gt;n&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This method takes a probability distribution and returns the distribution of means of samples from that distribution. You can specify the sample size, but by default we&amp;#8217;ll use 100.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s try it on some of the distributions we&amp;#8217;ve &lt;a href='/2013/08/19/climbing-the-probability-distribution-ladder.html'&gt;created&lt;/a&gt;.&lt;/p&gt;
&lt;!-- more --&gt;
&lt;pre&gt;&lt;code&gt;scala&amp;gt; sampleMean(uniform).hist
0.40  0.01% 
0.41  0.06% 
0.42  0.36% 
0.43  0.79% 
0.44  1.63% #
0.45  2.95% ##
0.46  5.18% #####
0.47  8.33% ########
0.48 11.43% ###########
0.49 12.80% ############
0.50 14.22% ##############
0.51 12.47% ############
0.52 10.74% ##########
0.53  8.00% ########
0.54  5.47% #####
0.55  2.78% ##
0.56  1.60% #
0.57  0.70% 
0.58  0.32% 
0.59  0.07% 
0.60  0.06% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not surprising. All the sample means are clustered around the true mean (0.5).&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s try a couple more.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; sampleMean(exponential(1)).hist
0.60  0.00% 
0.65  0.02% 
0.70  0.16% 
0.75  0.69% 
0.80  2.38% ##
0.85  6.68% ######
0.90 13.12% #############
0.95 17.93% #################
1.00 19.21% ###################
1.05 17.27% #################
1.10 11.26% ###########
1.15  6.53% ######
1.20  3.01% ###
1.25  1.28% #
1.30  0.36% 
1.35  0.07% 
1.40  0.02% 
1.45  0.00% 
1.50  0.01% 

scala&amp;gt; sampleMean(chi2(5)).hist
3.90  0.02% 
4.00  0.08% 
4.10  0.14% 
4.20  0.40% 
4.30  0.95% 
4.40  1.89% #
4.50  3.63% ###
4.60  5.68% #####
4.70  8.52% ########
4.80 10.25% ##########
4.90 12.23% ############
5.00 13.18% #############
5.10 11.19% ###########
5.20 10.37% ##########
5.30  7.61% #######
5.40  5.84% #####
5.50  3.67% ###
5.60  2.04% ##
5.70  1.23% #
5.80  0.64% 
5.90  0.29% 
6.00  0.10% 
6.10  0.03% 
6.20  0.02% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK, starting to see a pattern here. Let&amp;#8217;s look at some discrete distributions.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; sampleMean(bernoulli(0.8).map(b =&amp;gt; if (b) 1.0 else 0.0)).hist
0.68  0.33% 
0.70  0.85% 
0.72  2.14% ##
0.74  5.29% #####
0.76  9.96% #########
0.78 15.81% ###############
0.80 19.27% ###################
0.82 18.74% ##################
0.84 14.75% ##############
0.86  8.14% ########
0.88  3.32% ###
0.90  1.10% #
0.92  0.21% 
0.94  0.03% 

scala&amp;gt; sampleMean(binomial(0.2, 10).map(_.toDouble)).hist
1.50  0.00% 
1.55  0.03% 
1.60  0.08% 
1.65  0.32% 
1.70  1.00% #
1.75  2.35% ##
1.80  4.61% ####
1.85  7.88% #######
1.90 11.20% ###########
1.95 14.62% ##############
2.00 16.07% ################
2.05 14.82% ##############
2.10 11.42% ###########
2.15  7.43% #######
2.20  4.60% ####
2.25  2.15% ##
2.30  0.97% 
2.35  0.34% 
2.40  0.09% 
2.45  0.02% 
2.50  0.00% 

scala&amp;gt; sampleMean(geometric(0.2).map(_.toDouble)).hist
2.40  0.01% 
2.60  0.09% 
2.80  0.29% 
3.00  1.14% #
3.20  3.59% ###
3.40  7.31% #######
3.60 12.92% ############
3.80 16.75% ################
4.00 17.69% #################
4.20 15.31% ###############
4.40 11.16% ###########
4.60  7.03% #######
4.80  3.84% ###
5.00  1.70% #
5.20  0.79% 
5.40  0.29% 
5.60  0.08% 
5.80  0.00% 
6.00  0.01% 

scala&amp;gt; sampleMean(poisson(5).map(_.toDouble)).hist
4.30  0.15% 
4.40  0.43% 
4.50  1.34% #
4.60  3.42% ###
4.70  7.19% #######
4.80 12.04% ############
4.90 15.49% ###############
5.00 18.02% ##################
5.10 15.82% ###############
5.20 11.99% ###########
5.30  7.37% #######
5.40  4.03% ####
5.50  1.81% #
5.60  0.64% 
5.70  0.16% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All of these distributions look vaguely normal and they&amp;#8217;re all clustered around the mean of the underlying distribution.&lt;/p&gt;

&lt;h3 id='the_central_limit_theorem'&gt;The Central Limit Theorem&lt;/h3&gt;

&lt;p&gt;Surprise! That little observation was basically a statement of the Central Limit Theorem — means samples of a reasonable size drawn from any probability distribution will be normally distributed around the mean of the distribution. The Central Limit Theorem even tells you how to compute the standard deviation of this distribution: it&amp;#8217;s just the standard deviation of the underlying distribution divided by the square root of the sample size.&lt;/p&gt;
&lt;script type='math/tex; mode=display'&gt;
%&lt;![CDATA[
\bar{\sigma} = \frac{\sigma}{\sqrt{n}}
%]]&gt;
&lt;/script&gt;
&lt;p&gt;This quantity, the standard deviation of the distribution of sample means, is also known as the &lt;a href='http://en.wikipedia.org/wiki/Standard_error'&gt;standard error&lt;/a&gt;. It&amp;#8217;s not a terribly suggestive name, but it might help to think of the &amp;#8220;error&amp;#8221; as the difference between the sample mean and the true mean.&lt;/p&gt;

&lt;p&gt;Terminology aside, the most remarkable fact is that this works no matter what distribution you try it on.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s revisit each of the examples above and see if it pans out.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; uniform.ev
res0: Double = 0.49596431533522234

scala&amp;gt; uniform.stdev
res1: Double = 0.290545289200811&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So the Central Limit Theorem would predict that &lt;code&gt;sampleMean(uniform)&lt;/code&gt; will have mean 0.5 and stdev &lt;script type='math/tex'&gt;0.29 / \sqrt{100} = 0.029&lt;/script&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; sampleMean(uniform).ev
res2: Double = 0.49968258747065275

scala&amp;gt; sampleMean(uniform).stdev
res3: Double = 0.028763987024078164&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wow, OK! Let&amp;#8217;s keep going. (I&amp;#8217;m going to omit the mean calculations because it seems like an obvious fact. So I&amp;#8217;m just looking to see that the standard error is 1/10th the standard deviation of the underlying distribution.)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; exponential(1).stdev
res0: Double = 0.9971584111946743

scala&amp;gt; sampleMean(exponential(1)).stdev
res2: Double = 0.09987372019328666

scala&amp;gt; chi2(5).stdev
res3: Double = 3.1542391941582766

scala&amp;gt; sampleMean(chi2(5)).stdev
res4: Double = 0.3180622311083607

scala&amp;gt; binomial(0.2, 10).map(_.toDouble).stdev
res5: Double = 1.2733502267640227

scala&amp;gt; sampleMean(binomial(0.2, 10).map(_.toDouble)).stdev
res6: Double = 0.12688793641635224

scala&amp;gt; poisson(5).map(_.toDouble).stdev
res7: Double = 2.2423514867210077

scala&amp;gt; sampleMean(poisson(5).map(_.toDouble)).stdev
res8: Double = 0.2251131007715896

scala&amp;gt; geometric(0.2).map(_.toDouble).stdev
res9: Double = 4.439230239579939

scala&amp;gt; sampleMean(geometric(0.2).map(_.toDouble)).stdev
res10: Double = 0.4428929231078312&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And one more with a different sample size:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; sampleMean(geometric(0.2).map(_.toDouble), n = 625).stdev
res11: Double = 0.17952533894556522

scala&amp;gt; geometric(0.2).stdev / 25
res12: Double = 0.17756920958319758&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Crazy! OK that&amp;#8217;s enough experimental proof for me.&lt;/p&gt;

&lt;h3 id='so_what'&gt;So what?&lt;/h3&gt;

&lt;p&gt;Experimental analysis leans heavily on the Central Limit Theorem. A common question in experimental analysis is whether a sample is likely to have been drawn from a particular probability distribution. Since you can always treat sample means as normally distributed, you don&amp;#8217;t need to perform a different analysis for every type of distribution you might encounter. All you need to know is how to work with the normal distribution.&lt;/p&gt;

&lt;p&gt;You&amp;#8217;ve probably seen &lt;a href='http://en.wikipedia.org/wiki/Standard_deviation'&gt;this diagram&lt;/a&gt; before:&lt;/p&gt;

&lt;p&gt;&lt;img alt='the normal distribution' src='http://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Standard_deviation_diagram.svg/325px-Standard_deviation_diagram.svg.png' /&gt;&lt;/p&gt;

&lt;p&gt;This is what we&amp;#8217;re working with. A value drawn from a normal distribution will be within 2 standard deviations of the mean 96% of the time. Since sample means are normally distributed around the true mean, sample means will be within 2 standard errors of the true mean 96% of the time. If a sample mean is more than 2 standard deviations away from the true mean, the sample is unlikely to have been drawn from that distribution.&lt;/p&gt;

&lt;h3 id='an_example'&gt;An example&lt;/h3&gt;

&lt;p&gt;Let&amp;#8217;s look at an example where we can put the Central Limit Theorem to good use. Suppose your friend tells you he has a fair coin and offers to play a game. You pay him $1 to play, and he flips his coin until it comes up heads. He gives you $1 for every time the coin comes up tails until that happens.&lt;/p&gt;

&lt;p&gt;After 100 rounds of this, you notice that you&amp;#8217;ve lost $30. Did your friend cheat you?&lt;/p&gt;

&lt;p&gt;In standard experimental analysis terms, the null hypothesis is that your friend has a fair coin. You can reject the null hypothesis if you can show that there is less than, say, a 5% chance of losing $30 after 100 rounds.&lt;/p&gt;

&lt;p&gt;You can model the distribution of outcomes for a single round of the game as follows:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;d&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;geometric&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='mf'&gt;0.5&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt; &lt;span class='mf'&gt;1.0&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;geometric(0.5)&lt;/code&gt; models your winnings and &lt;code&gt;- 1.0&lt;/code&gt; represents the cost to play the round. The expected value and standard deviation of this distribution are:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; d.ev
res102: Double = -0.0093

scala&amp;gt; d.stdev
res105: Double = 1.406354208583263&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;#8217;ll call that 0 and 1.4. You have a sample of 100 rounds and an average loss of $0.30 per round. What is the probability that 100 samples from &lt;code&gt;d&lt;/code&gt; would have a mean of -0.3? Well, the distribution of sample means has mean 0 and standard deviation &lt;script type='math/tex'&gt;1.4 / \sqrt{100} = 0.14&lt;/script&gt;. So your sample mean of -0.3 is more than 2 standard deviations away from the average sample mean, which we know will happen less than 5% of the time. So we can reject the null hypothesis.&lt;/p&gt;

&lt;p&gt;We can also calculate the probability directly against the distribution of sample means.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; sampleMean(d, n = 100).pr(_ &amp;lt; -0.3)
res0: Double = 0.0104&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;#8217;s worth pointing out that this is a one-tailed test (I&amp;#8217;m not considering the possibility that we&amp;#8217;d see a gain of $0.30 per round) because I have no reason to suspect that my friend has rigged the game in my favor.&lt;/p&gt;

&lt;h3 id='another_example'&gt;Another example&lt;/h3&gt;

&lt;p&gt;Let&amp;#8217;s say you have a website, and you want to know whether making your big green &amp;#8220;Sign Up!&amp;#8221; button red instead of green would increase the percent of people who click the button. Historically, you know that 5.8% of visitors to your site click the (green) button.&lt;/p&gt;

&lt;p&gt;So one day you make the button red and keep track of the fraction of visitors who click on it. After some period of time you observe that 53 out of 810 visitors clicked the (red) button. That&amp;#8217;s 6.5%, a decent improvement! (Some would say it&amp;#8217;s a 12% improvement; others a 0.7% improvement. Potato, potato.) But is this difference something we&amp;#8217;re likely to observe just by chance, or was making the button red a meaningful change?&lt;/p&gt;

&lt;p&gt;We can model the number of clicks as a Bernoulli distribution with a 5.8% success probability. In order to do this I&amp;#8217;ll have to translate &lt;code&gt;true&lt;/code&gt; to 1 click and &lt;code&gt;false&lt;/code&gt; to 0 clicks.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val d = bernoulli(0.058).map(b =&amp;gt; if (b) 1.0 else 0.0)
d: Distribution[Double] = &amp;lt;distribution&amp;gt;

scala&amp;gt; d.stdev
res0: Double = 0.24062616233485523&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We want to know the probability of seeing a 6.5% success rate in a sample of 810 visitors. Since we know the standard deviation of &lt;code&gt;d&lt;/code&gt;, we can apply the Central Limit Theorem to find the standard error for a sample of size 810, just by dividing by &lt;script type='math/tex'&gt;\sqrt{810} = 28.5&lt;/script&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; d.stdev / math.sqrt(810)
res2: Double = 0.008313053843054703&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Well, OK. Our difference of 0.7% is less than one standard deviation from the mean, which means it&amp;#8217;s pretty likely to happen just by chance. In other words, we can&amp;#8217;t reject the null hypothesis that making the button red did not change the rate at which people click on it.&lt;/p&gt;

&lt;p&gt;To illustrate this further, here&amp;#8217;s what the distribution of sample means for samples of size 810 looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; sampleMean(d, 810).hist
0.0350  0.15% 
0.0375  0.41% 
0.0400  1.20% #
0.0425  2.01% ##
0.0450  3.58% ###
0.0475  5.96% #####
0.0500  7.55% #######
0.0525  9.62% #########
0.0550 11.49% ###########
0.0575 12.08% ############
0.0600 11.23% ###########
0.0625 10.42% ##########
0.0650  8.32% ########
0.0675  5.70% #####
0.0700  3.97% ###
0.0725  2.90% ##
0.0750  1.56% #
0.0775  0.84% 
0.0800  0.47% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Seeing 6.5% as the mean of a sample of size 810 is totally within the fat part of the distribution. Let&amp;#8217;s see just how likely a difference of 0.7% really is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; sampleMean(d, 810).pr(c =&amp;gt; c &amp;lt; 0.051 || c &amp;gt; 0.065)
res3: Double = 0.4074&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pretty likely! (A difference of -0.7% would be just as surprising, so we have to count that too.)&lt;/p&gt;

&lt;p&gt;OK, let&amp;#8217;s say instead we had observed 530 clicks out of 8100 visitors &amp;#8211; it&amp;#8217;s the same 6.5% success rate, just with 10 times as many samples. What does that do to our analysis? Well, first of all, we can expect the standard error to be much smaller, since we&amp;#8217;re dividing by &lt;script type='math/tex'&gt;\sqrt{8100} = 90&lt;/script&gt; instead of &lt;script type='math/tex'&gt;\sqrt{810} = 28.5&lt;/script&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; d.stdev / 90
res0: Double = 0.0026411931460171042&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The standard error is 0.26%, putting our 0.7% difference at more than 2.5 standard deviations from the mean, and so in this case we can reject the null hypothesis and conclude that making the button red was a meaningful change.&lt;/p&gt;

&lt;p&gt;To further illustrate this, here is the distribution of sample means for samples of size 8100, on the same scale as before:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; sampleMean(d, 8100).bucketedHist(0.035, 0.08, 18)
0.0350  0.00% 
0.0375  0.00% 
0.0400  0.00% 
0.0425  0.00% 
0.0450  0.00% 
0.0475  0.01% 
0.0500  0.44% 
0.0525  4.50% ####
0.0550 19.90% ###################
0.0575 36.42% ####################################
0.0600 28.55% ############################
0.0625  8.70% ########
0.0650  1.41% #
0.0675  0.06% 
0.0700  0.01% 
0.0725  0.00% 
0.0750  0.00% 
0.0775  0.00% 
0.0800  0.00% 

scala&amp;gt; sampleMean(d, 8100).pr(p =&amp;gt; p &amp;lt; 0.051 || p &amp;gt; 0.065)
res4: Double = 0.0062&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The range of the distribution of sample means is much narrower, so much so that a difference of 0.7% (in either direction) is expected to occur by chance less than 1% of the time.&lt;/p&gt;

&lt;h3 id='one_important_exception'&gt;One important exception&lt;/h3&gt;

&lt;p&gt;It turns out that the Central Limit Theorem doesn&amp;#8217;t work with every distribution. This is due to one sneaky fact — sample means are clustered around the mean of the underlying distribution &lt;em&gt;if it exists&lt;/em&gt;. But how can a distribution have no mean? Well, one common distribution that has no mean is the Pareto distribution. If you tried to calculate it using the usual methods, it would diverge to infinity.&lt;/p&gt;

&lt;p&gt;The means of samples drawn from the Pareto distribution are not normally distributed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; sampleMean(pareto(1)).bucketedHist(0, 20, 20)
 0.0  0.00% 
 1.0  0.00% 
 2.0  0.00% 
 3.0  3.04% ###
 4.0 16.03% ################
 5.0 20.08% ####################
 6.0 16.61% ################
 7.0 12.16% ############
 8.0  8.08% ########
 9.0  5.87% #####
10.0  4.42% ####
11.0  2.90% ##
12.0  2.72% ##
13.0  1.67% #
14.0  1.51% #
15.0  1.31% #
16.0  1.14% #
17.0  0.88% 
18.0  0.83% 
19.0  0.53% 
20.0  0.22% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And the standard error is completely meaningless:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; sampleMean(pareto(1)).stdev
res0: Double = 157.6098722134558

scala&amp;gt; sampleMean(pareto(1)).stdev
res1: Double = 477.9797744569662&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So the Central Limit Theorem doesn&amp;#8217;t apply.&lt;/p&gt;

&lt;h3 id='conclusion'&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;We were able to use the Central Limit Theorem to reason about a samples from various distributions, knowing that the mean of such a sample is expected to fall within a bell-shaped curve around the mean of the underlying distribution. This is great because we don&amp;#8217;t need special analysis tools for each kind of distribution we might come across. No matter what the underlying distribution is, you can always treat sample means as normally distributed.&lt;/p&gt;

&lt;p&gt;&amp;#8230; unless the underlying distribution has no mean.&lt;/p&gt;

&lt;p&gt;We actually run into this all the time at Foursquare. Certain things like, say, the distribution of the number of friends users have is Pareto-distributed (the vast majority of users have a small number of friends, but some users have thousands of friends). So if you&amp;#8217;re running an experiment that is intended to increase the average number of friends users have, you&amp;#8217;re going to run into trouble. You aren&amp;#8217;t going to be able to use standard statistical techniques to analyze the results of the experiment. Well, actually, you can try, and you&amp;#8217;ll get some convincing-looking numbers out, but those numbers will be completely meaningless!&lt;/p&gt;</description>
                <link>http://jliszka.github.io/2013/08/26/a-programmers-guide-to-the-central-limit-theorem.html</link>
                <guid>http://jliszka.github.io/2013/08/26/a-programmers-guide-to-the-central-limit-theorem</guid>
                <pubDate>2013-08-26T00:00:00-04:00</pubDate>
        </item>

        <item>
                <title>Climbing the probability distribution ladder</title>
                <description>&lt;p&gt;In the &lt;a href='/2013/08/12/a-frequentist-approach-to-probability.html'&gt;last post&lt;/a&gt; I created a simple library for constructing probability distributions, based on the &lt;a href='http://en.wikipedia.org/wiki/Monte_Carlo_method'&gt;Monte Carlo method&lt;/a&gt;. I started with the uniform distribution and derived the Bernoulli and normal distributions from it.&lt;/p&gt;

&lt;p&gt;In this post I&amp;#8217;ll construct some more common distributions in the same manner.&lt;/p&gt;

&lt;h3 id='the_exponential_distribution'&gt;The exponential distribution&lt;/h3&gt;

&lt;p&gt;If &lt;script type='math/tex'&gt;X&lt;/script&gt; is a uniformly distributed random variable, then &lt;script type='math/tex'&gt;-log(X)/\lambda&lt;/script&gt; is distributed according to the &lt;a href='http://en.wikipedia.org/wiki/Exponential_distribution'&gt;exponential distribution&lt;/a&gt;. The parameter &lt;script type='math/tex'&gt;\lambda&lt;/script&gt; is just a scaling factor. In code:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;exponential&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;l&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;x&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;uniform&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt; &lt;span class='k'&gt;yield&lt;/span&gt; &lt;span class='n'&gt;math&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;log&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='o'&gt;(-&lt;/span&gt;&lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='o'&gt;/&lt;/span&gt;&lt;span class='n'&gt;l&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It looks like this:&lt;/p&gt;
&lt;!-- more --&gt;
&lt;pre&gt;&lt;code&gt;scala&amp;gt; exponential(1).bucketedHist(0, 8, 16, roundDown = true)
 0.0 39.40% #######################################
 0.5 23.15% #######################
 1.0 15.11% ###############
 1.5  9.13% #########
 2.0  4.93% ####
 2.5  3.32% ###
 3.0  1.84% #
 3.5  1.19% #
 4.0  0.71% 
 4.5  0.53% 
 5.0  0.32% 
 5.5  0.15% 
 6.0  0.07% 
 6.5  0.07% 
 7.0  0.03% 
 7.5  0.03% 
 8.0  0.01% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It seems backwards that the exponential distribution is implemented using a logarithm. It probably has something to do with this particular technique of constructing distributions. I&amp;#8217;m describing where to put each piece of probability mass (here, by taking the log of each sample) rather than describing how much probability mass lives at each value of &lt;script type='math/tex'&gt;x&lt;/script&gt; (for the exponential distribution, &lt;script type='math/tex'&gt;\lambda e^{-\lambda x}&lt;/script&gt; lives at &lt;script type='math/tex'&gt;x&lt;/script&gt;, so from that definition it&amp;#8217;s clear why it&amp;#8217;s called the exponential distribution).&lt;/p&gt;

&lt;p&gt;This distribution is the continuous analog of the geometric distribution, and plays an interesting role on the construction of the Poisson distribution, both of which I&amp;#8217;ll get to in a minute.&lt;/p&gt;

&lt;h3 id='the_pareto_distribution'&gt;The Pareto distribution&lt;/h3&gt;

&lt;p&gt;You can construct the &lt;a href='http://en.wikipedia.org/wiki/Pareto_distribution'&gt;Pareto distribution&lt;/a&gt; from the uniform distribution in a similar way. If &lt;script type='math/tex'&gt;X&lt;/script&gt; is a uniformly distributed random variable, then &lt;script type='math/tex'&gt;x_m X^{-1/\alpha}&lt;/script&gt; is a Pareto-distributed random variable. The parameter &lt;script type='math/tex'&gt;x_m&lt;/script&gt; is the minimum value the distribution can take, and &lt;script type='math/tex'&gt;\alpha&lt;/script&gt; is a factor that determines how spread out the distribution is. In code:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;pareto&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;a&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;xm&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='mf'&gt;1.0&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;x&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;uniform&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt; &lt;span class='k'&gt;yield&lt;/span&gt; &lt;span class='n'&gt;xm&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;math&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;pow&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt;&lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='o'&gt;/&lt;/span&gt;&lt;span class='n'&gt;a&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; pareto(1).bucketedHist(1, 10, 18, roundDown = true)
 1.0 37.60% #####################################
 1.5 17.59% #################
 2.0 10.52% ##########
 2.5  7.61% #######
 3.0  5.43% #####
 3.5  4.04% ####
 4.0  2.85% ##
 4.5  2.64% ##
 5.0  1.61% #
 5.5  1.77% #
 6.0  1.37% #
 6.5  1.10% #
 7.0  1.06% #
 7.5  0.98% 
 8.0  0.90% 
 8.5  0.83% 
 9.0  0.68% 
 9.5  0.56% 
10.0  0.39% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hm, the implementations of &lt;code&gt;pareto&lt;/code&gt; and &lt;code&gt;exponential&lt;/code&gt; look pretty similar. It&amp;#8217;s more obvious if I rewrite &lt;code&gt;exponential&lt;/code&gt; slightly, moving the product inside the log.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;exponential&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;l&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;x&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;uniform&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt; &lt;span class='k'&gt;yield&lt;/span&gt; &lt;span class='n'&gt;math&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;log&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;math&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;pow&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt;&lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='o'&gt;/&lt;/span&gt;&lt;span class='n'&gt;l&lt;/span&gt;&lt;span class='o'&gt;))&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And now it looks like &lt;code&gt;exponential&lt;/code&gt; is just the log of &lt;code&gt;pareto&lt;/code&gt;. Let&amp;#8217;s check.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; pareto(1).map(math.log).bucketedHist(0, 8, 16, roundDown = true)
 0.0 38.76% ######################################
 0.5 24.28% ########################
 1.0 14.47% ##############
 1.5  9.09% #########
 2.0  5.10% #####
 2.5  3.29% ###
 3.0  1.92% #
 3.5  1.29% #
 4.0  0.77% 
 4.5  0.43% 
 5.0  0.22% 
 5.5  0.14% 
 6.0  0.09% 
 6.5  0.04% 
 7.0  0.02% 
 7.5  0.04% 
 8.0  0.04% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yep, pretty close! But you wouldn&amp;#8217;t know how closely they are related by looking at the probabily density functions.&lt;/p&gt;

&lt;p&gt;Pareto:&lt;/p&gt;
&lt;script type='math/tex; mode=display'&gt;
%&lt;![CDATA[
f_\alpha(x) = \frac{\alpha}{x^{\alpha+1}}
%]]&gt;
&lt;/script&gt;
&lt;p&gt;Exponential:&lt;/p&gt;
&lt;script type='math/tex; mode=display'&gt;
%&lt;![CDATA[
f_\lambda(x) = \lambda e^{-\lambda x}
%]]&gt;
&lt;/script&gt;
&lt;p&gt;Hm, interesting!&lt;/p&gt;

&lt;p&gt;Anyway, this distribution shows up a lot in &amp;#8220;rich get richer&amp;#8221; scenarios — distribution of income, the population of cities, file sizes on your computer, etc. But I don&amp;#8217;t have a good explanation as to why.&lt;/p&gt;

&lt;h3 id='the_chisquared_distribution'&gt;The chi-squared distribution&lt;/h3&gt;

&lt;p&gt;A &lt;a href='http://en.wikipedia.org/wiki/Chi-squared_distribution'&gt;chi-squared distribution&lt;/a&gt; can be constructed by squaring and then summing several normal distributions. It is parameterized by the number of degrees of freedom, &lt;code&gt;df&lt;/code&gt;, which just indicates how many squared normal distributions to sum up. Here&amp;#8217;s the code:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;chi2&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;df&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;normal&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;repeat&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;df&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;sum&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Its probability density function is a lot easier to understand, though:&lt;/p&gt;
&lt;script type='math/tex; mode=display'&gt;
%&lt;![CDATA[
f_k(x) = \frac{x^{(k/2)-1}e^{-x/2}}{2^{k/2}\Gamma(\frac{k}{2})}
%]]&gt;
&lt;/script&gt;
&lt;p&gt;Just kidding! This is gross. I&amp;#8217;m not going to even get into what &lt;script type='math/tex'&gt;\Gamma&lt;/script&gt; is.&lt;/p&gt;

&lt;p&gt;OK here&amp;#8217;s what it looks like for different degrees of freedom:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; chi2(1).bucketedHist(0, 10, 10, roundDown = true)
 0.0 68.20% ####################################################################
 1.0 15.49% ###############
 2.0  7.67% #######
 3.0  3.83% ###
 4.0  2.07% ##
 5.0  1.30% #
 6.0  0.66% 
 7.0  0.42% 
 8.0  0.26% 
 9.0  0.10% 
10.0  0.00% 

scala&amp;gt; chi2(5).bucketedHist(0, 15, 15, roundDown = true)
 0.0  3.84% ###
 1.0 11.48% ###########
 2.0 14.71% ##############
 3.0 15.07% ###############
 4.0 13.67% #############
 5.0 10.83% ##########
 6.0  8.75% ########
 7.0  6.43% ######
 8.0  4.99% ####
 9.0  3.51% ###
10.0  2.43% ##
11.0  1.64% #
12.0  1.22% #
13.0  0.85% 
14.0  0.59% 
15.0  0.00% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This distribution is useful in &lt;a href='http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test'&gt;analyzing&lt;/a&gt; whether an observed sample is likely to have been drawn from a given theoretical distribution, where you construct a &amp;#8220;test statistic&amp;#8221; by summing the squares of the deviations of the observed values from their theoretical values. It&amp;#8217;s this sum of squared differences that makes the chi-squared distribution an appropriate tool here. Why the chi-squared distribution is the sum of squared &lt;em&gt;normal&lt;/em&gt; distributions is a topic for another post.&lt;/p&gt;

&lt;h3 id='students_tdistribution'&gt;Student&amp;#8217;s &lt;em&gt;t&lt;/em&gt;-distribution&lt;/h3&gt;

&lt;p&gt;If &lt;script type='math/tex'&gt;Z&lt;/script&gt; is a normally distributed random variable and &lt;script type='math/tex'&gt;V&lt;/script&gt; is a chi-squared random variable with &lt;script type='math/tex'&gt;k&lt;/script&gt; degrees of freedom, then &lt;script type='math/tex'&gt;Z / \sqrt{V/k}&lt;/script&gt; is a random variable distributed according to the &lt;a href='http://en.wikipedia.org/wiki/Student&amp;apos;s_t-distribution'&gt;Student&amp;#8217;s &lt;em&gt;t&lt;/em&gt;-distribution&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the code:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;students_t&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;k&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;z&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;normal&lt;/span&gt;
    &lt;span class='n'&gt;v&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;chi2&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;k&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt; &lt;span class='k'&gt;yield&lt;/span&gt; &lt;span class='n'&gt;z&lt;/span&gt; &lt;span class='o'&gt;/&lt;/span&gt; &lt;span class='n'&gt;math&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;sqrt&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;v&lt;/span&gt; &lt;span class='o'&gt;/&lt;/span&gt; &lt;span class='n'&gt;k&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The closed-form probability density function is too gross to even consider. Here&amp;#8217;s a plot though:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; students_t(3).bucketedHist(-5, 5, 20)
-5.0  0.12% 
-4.5  0.38% 
-4.0  0.51% 
-3.5  0.63% 
-3.0  1.41% #
-2.5  2.24% ##
-2.0  3.72% ###
-1.5  5.89% #####
-1.0 10.03% ##########
-0.5 15.90% ###############
 0.0 18.38% ##################
 0.5 15.88% ###############
 1.0 11.01% ###########
 1.5  5.83% #####
 2.0  3.37% ###
 2.5  2.07% ##
 3.0  1.13% #
 3.5  0.62% 
 4.0  0.49% 
 4.5  0.26% 
 5.0  0.14% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This distribution arises by modeling the location of the true mean of a distribution with unknown mean and unknown standard deviation, when all you have is a small sample from the distribution. &lt;script type='math/tex'&gt;k&lt;/script&gt; represents the sample size. &lt;script type='math/tex'&gt;Z&lt;/script&gt; represents the distribution of the sample mean around the true mean (why it&amp;#8217;s a normal distribution is a subject for another post). &lt;script type='math/tex'&gt;V/k&lt;/script&gt; represents the variance of the sample — as the sum of squared differences of samples from the sample mean, it is naturally modeled as a chi-squared distribution. Its square root represents the standard deviation of the sample. So basically we&amp;#8217;re scaling a normal distribution (representing the sample mean) by the standard deviation of the sample.&lt;/p&gt;

&lt;p&gt;It looks a lot like the normal distribution, and in fact as the degrees of freedom goes up, it becomes a better and better approximation to it. At smaller degrees of freedom, though, there is more probability mass in the tails (it has &amp;#8220;fatter tails&amp;#8221; as some people say).&lt;/p&gt;

&lt;h3 id='the_geometric_distribution'&gt;The geometric distribution&lt;/h3&gt;

&lt;p&gt;The &lt;a href='http://en.wikipedia.org/wiki/Geometric_distribution'&gt;geometric distribution&lt;/a&gt; is a discrete distribution that can be constructed from the Bernoulli distribution (a biased coin flip). Although recall that the Bernoulli distribution itself can be &lt;a href='/2013/08/12/a-frequentist-approach-to-probability.html'&gt;constructed from the uniform distribution&lt;/a&gt; pretty easily.&lt;/p&gt;

&lt;p&gt;The geometric distribution describes the number of failures you will see before seeing your first success in repeated Bernoulli trials with bias &lt;code&gt;p&lt;/code&gt;. In other words, if I flip a coin repeatedly, how many tails will I see before get my first head?&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;geometric&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;p&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;bernoulli&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;p&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;until&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;headOption&lt;/span&gt; &lt;span class='o'&gt;==&lt;/span&gt; &lt;span class='nc'&gt;Some&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kc'&gt;true&lt;/span&gt;&lt;span class='o'&gt;)).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;size&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt; &lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;scala&amp;gt; geometric(0.5).hist
 0 49.56% #################################################
 1 25.83% #########################
 2 12.06% ############
 3  6.23% ######
 4  3.08% ###
 5  1.68% #
 6  0.75% 
 7  0.40% 
 8  0.21% 
 9  0.10% 
10  0.04% 
11  0.04% 
12  0.02% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Half the time heads comes up on the 1st flip, a quarter of the time it comes up on the 2nd flip, an eighth of the time it comes up on the 3rd flip, etc. &lt;script type='math/tex'&gt;\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, ..., (\frac{1}{2})^n&lt;/script&gt; is a geometric sequence and that&amp;#8217;s where this distribution gets its name. If you used a biased coin, you would get a different (but still geometric) sequence.&lt;/p&gt;

&lt;h3 id='the_binomial_distribution'&gt;The binomial distribution&lt;/h3&gt;

&lt;p&gt;The &lt;a href='http://en.wikipedia.org/wiki/Binomial_distribution'&gt;binomial distribution&lt;/a&gt; can be modeled as the number of successes you will see in &lt;code&gt;n&lt;/code&gt; Bernoulli trials with bias &lt;code&gt;p&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For example: I flip a fair coin 20 times, how many times will it come up heads? Let&amp;#8217;s see:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;binomial&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;p&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;n&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;bernoulli&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;p&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;repeat&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;n&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;count&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt; &lt;span class='o'&gt;==&lt;/span&gt; &lt;span class='kc'&gt;true&lt;/span&gt;&lt;span class='o'&gt;))&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;scala&amp;gt; binomial(0.5, 20).hist
 2  0.02% 
 3  0.11% 
 4  0.46% 
 5  1.33% #
 6  3.81% ###
 7  7.35% #######
 8 11.73% ###########
 9 15.84% ###############
10 18.05% ##################
11 16.19% ################
12 11.75% ###########
13  7.50% #######
14  3.71% ###
15  1.51% #
16  0.48% 
17  0.13% 
18  0.03% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;10 is the most likely result, as you would expect, although other outcomes are possible too. This distribution spells out exactly how probable each outcome is.&lt;/p&gt;

&lt;p&gt;This distribution also looks a lot like the normal distribution, and in fact as &lt;code&gt;n&lt;/code&gt; increases, the binomial distribution better approximates the normal distribution.&lt;/p&gt;

&lt;p&gt;The probability density function involves some combinatorics, which is not entirely surprising.&lt;/p&gt;
&lt;script type='math/tex; mode=display'&gt;
%&lt;![CDATA[
f(k) = {n \choose k}p^k(1-p)^{n-k}
%]]&gt;
&lt;/script&gt;
&lt;h3 id='the_negative_binomial_distribution'&gt;The negative binomial distribution&lt;/h3&gt;

&lt;p&gt;The &lt;a href='http://en.wikipedia.org/wiki/Negative_binomial_distribution'&gt;negative binomial distribution&lt;/a&gt; is a relative of the binomial distribution. It counts the number of successes you will see in repeated Bernoulli trials (with bias &lt;code&gt;p&lt;/code&gt;) before you see &lt;code&gt;r&lt;/code&gt; failures.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the code:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;negativeBinomial&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;p&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;bernoulli&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;p&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;until&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;count&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt; &lt;span class='o'&gt;==&lt;/span&gt; &lt;span class='kc'&gt;false&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;==&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;size&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Straightforward stuff at this point.&lt;/p&gt;

&lt;h3 id='the_poisson_distribution'&gt;The Poisson distribution&lt;/h3&gt;

&lt;p&gt;A &lt;a href='http://en.wikipedia.org/wiki/Poisson_distribution'&gt;Poisson distribution&lt;/a&gt; with parameter &lt;script type='math/tex'&gt;\lambda&lt;/script&gt; gives the distribution of the number of discrete events that will occur during a given time period if &lt;script type='math/tex'&gt;\lambda&lt;/script&gt; events are expected to occur on average.&lt;/p&gt;

&lt;p&gt;Wikipedia gives the following &lt;a href='http://en.wikipedia.org/wiki/Poisson_distribution#Generating_Poisson-distributed_random_variables'&gt;algorithm&lt;/a&gt; for generating values from a Poisson distribution:&lt;/p&gt;

&lt;p&gt;Sample values from a uniform distribution one at a time until their cumulative product is less than &lt;script type='math/tex'&gt;e^{-\lambda}&lt;/script&gt;. The number of samples this requires (minus 1) will be Poisson-distributed.&lt;/p&gt;

&lt;p&gt;In code:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;poisson&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;lambda&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;m&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;math&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;exp&lt;/span&gt;&lt;span class='o'&gt;(-&lt;/span&gt;&lt;span class='n'&gt;lambda&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
  &lt;span class='n'&gt;uniform&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;until&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;product&lt;/span&gt; &lt;span class='o'&gt;&amp;lt;&lt;/span&gt; &lt;span class='n'&gt;m&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;size&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt; &lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To me this obscures what&amp;#8217;s really going on. If you take the negative log of everything, this algorithm becomes:&lt;/p&gt;

&lt;p&gt;Sample values from a uniform distribution, take the negative log, and keep a running sum until the sum is greater than &lt;script type='math/tex'&gt;\lambda&lt;/script&gt;. The number of samples this requires (minus 1) will be Poisson-distributed.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;poisson&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;lambda&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;d&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;uniform&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt;&lt;span class='n'&gt;math&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;log&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='o'&gt;))&lt;/span&gt;
  &lt;span class='n'&gt;d&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;until&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;sum&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;lambda&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;size&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt; &lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This sounds more complicated until you remember that the negative log of the uniform distribution is the exponential distribution.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;poisson&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;lambda&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;d&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;exponential&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
  &lt;span class='n'&gt;d&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;until&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;sum&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;lambda&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;size&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt; &lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now this is what the Poisson distribution is really about. Why? The time between events in a Poisson process follows the exponential distribution. So if you wanted to know how many events will happen in, say &lt;script type='math/tex'&gt;\lambda&lt;/script&gt; seconds, you could add up inter-event timings drawn from the exponential distribution (which has mean 1) until you get to &lt;script type='math/tex'&gt;\lambda&lt;/script&gt;. That&amp;#8217;s exactly what the code above does.&lt;/p&gt;

&lt;p&gt;But why does the exponential distribution model the time between events in the first place? In a rigorous sense, the exponential distribution is the most natural choice. First of all, it produces values between 0 and &lt;script type='math/tex'&gt;\infty&lt;/script&gt; (in the parlance, it has &amp;#8220;support&amp;#8221; &lt;script type='math/tex'&gt;[0, \infty)&lt;/script&gt;), which makes sense for modeling timings between events — you don&amp;#8217;t want any negative values, but otherwise there is no limit to the amount of time that could elapse between events.&lt;/p&gt;

&lt;p&gt;And second, of all the distributions with support &lt;script type='math/tex'&gt;[0, \infty)&lt;/script&gt;, the exponential distribution is the one that makes the fewest additional assumptions — that is, it contains the least extra information, which is the same as saying that it has the highest &lt;a href='http://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution'&gt;entropy&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, with a little rewriting, you can see how the negative binomial distribution is sort of the discrete counterpart to the Poisson distribution. Here is &lt;code&gt;negativeBinomial&lt;/code&gt; rewritten to show the similarity:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;negativeBinomial&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;p&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;d&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;bernoulli&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;p&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;b&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;if&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;b&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='mi'&gt;0&lt;/span&gt; &lt;span class='k'&gt;else&lt;/span&gt; &lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
  &lt;span class='n'&gt;d&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;until&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;sum&lt;/span&gt; &lt;span class='o'&gt;==&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;size&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you squint, sorta? If you squint even harder, or you are drunk, you can probably even convince yourself that &lt;code&gt;if (b) 0 else 1&lt;/code&gt; is a discrete analog of &lt;code&gt;-math.log(x)&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id='conclusion'&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Obviously there is a lot more to say about each of these distributions, but I hope this has removed some of the mystery around how various probability distributions arise and how they are related to one another.&lt;/p&gt;

&lt;p&gt;All of this is going somewhere, I promise! In the next post I&amp;#8217;ll take a look at the Central Limit Theorem, which sounds scary but I promise you is not.&lt;/p&gt;

&lt;p&gt;The code in this post is available on &lt;a href='http://github.com/jliszka/probability-monad'&gt;github&lt;/a&gt;.&lt;/p&gt;</description>
                <link>http://jliszka.github.io/2013/08/19/climbing-the-probability-distribution-ladder.html</link>
                <guid>http://jliszka.github.io/2013/08/19/climbing-the-probability-distribution-ladder</guid>
                <pubDate>2013-08-19T00:00:00-04:00</pubDate>
        </item>

        <item>
                <title>A Frequentist Approach to Probability</title>
                <description>&lt;p&gt;One thing that always confused me in my intro stats classes was the concept of a random variable. A random variable is not a variable like I&amp;#8217;m used to thinking about, like a thing that has one value at a time. A random variable is instead an object that you can sample values from, and the values you get will be distributed according to some underlying probability distribution.&lt;/p&gt;

&lt;p&gt;In that way it sort of acts like a container, where the only operation is to sample a value from the container. In Scala it might look something like:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;trait&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;get&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;A&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The idea is that &lt;code&gt;get&lt;/code&gt; returns a different value (of type &lt;code&gt;A&lt;/code&gt;) from the distribution every time you call it.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m going to add a &lt;code&gt;sample&lt;/code&gt; method that lets me draw a sample of any size I want from the distribution.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;trait&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;get&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;A&lt;/span&gt;

  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;sample&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;n&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='nc'&gt;List&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;fill&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;n&lt;/span&gt;&lt;span class='o'&gt;)(&lt;/span&gt;&lt;span class='k'&gt;this&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;get&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now to create a simple distribution. Here&amp;#8217;s one whose samples are uniformly distributed between 0 and 1.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;uniform&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;private&lt;/span&gt; &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;rand&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='n'&gt;java&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;util&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='nc'&gt;Random&lt;/span&gt;&lt;span class='o'&gt;()&lt;/span&gt;
  &lt;span class='k'&gt;override&lt;/span&gt; &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;get&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;rand&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;nextDouble&lt;/span&gt;&lt;span class='o'&gt;()&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And sampling it gives&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; uniform.sample(10).foreach(println)
0.15738645964157327
0.7827120503875181
0.8787176537434814
0.38506604599728245
0.9469681837641953
0.20822217752687067
0.8229649049912187
0.7767540566158817
0.4133782959276152
0.8152378840945975&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='transforming_distributions'&gt;Transforming distributions&lt;/h3&gt;

&lt;p&gt;Every good container should have a &lt;code&gt;map&lt;/code&gt; method. &lt;code&gt;map&lt;/code&gt; will transform values produced by the distribution according to some function you pass it.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;trait&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;self&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt;
  &lt;span class='c1'&gt;// ...&lt;/span&gt;
  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;B&lt;/span&gt;&lt;span class='o'&gt;](&lt;/span&gt;&lt;span class='n'&gt;f&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;A&lt;/span&gt; &lt;span class='o'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;B&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;B&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;B&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='k'&gt;override&lt;/span&gt; &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;get&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;f&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;self&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;get&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- more --&gt;
&lt;p&gt;(Quick technical note: I added a self-type annotation that makes &lt;code&gt;self&lt;/code&gt; an alias for &lt;code&gt;this&lt;/code&gt; so that it&amp;#8217;s easier to refer to in anonymous inner classes.)&lt;/p&gt;

&lt;p&gt;Now I can map &lt;code&gt;* 2&lt;/code&gt; over the uniform distribution, giving a uniform distribution between 0 and 2:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; uniform.map(_ * 2).sample(10).foreach(println)
1.608298200368093
0.14423181179528677
0.31844160650777886
1.6299535560273648
1.0188592816936894
1.9150473071752487
0.9324757358322544
0.5287503566916676
1.35497977515358
0.5874386820078819&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;map&lt;/code&gt; also lets you create distributions of different types:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val tf = uniform.map(_ &amp;lt; 0.5)
tf: Distribution[Boolean] = &amp;lt;distribution&amp;gt;

scala&amp;gt; tf.sample(10)
res2: List[Boolean] = List(true, true, true, true, false, false, false, false, true, false)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;tf&lt;/code&gt; is a &lt;code&gt;Distribution[Boolean]&lt;/code&gt; that should give &lt;code&gt;true&lt;/code&gt; and &lt;code&gt;false&lt;/code&gt; with equal probability. Actually, it would be a bit more useful to be able to create distributions giving &lt;code&gt;true&lt;/code&gt; and &lt;code&gt;false&lt;/code&gt; with arbitrary probabilities. This kind of distribution is called the Bernoulli distribution.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;bernoulli&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;p&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Boolean&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;uniform&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt; &lt;span class='o'&gt;&amp;lt;&lt;/span&gt; &lt;span class='n'&gt;p&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Trying it out:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; bernoulli(0.8).sample(10)
res0: List[Boolean] = List(true, false, true, true, true, true, true, true, true, true)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cool. Now I want to measure the probability that a random variable will take on certain values. This is easy to do empirically by pulling 10,000 sample values and counting how many of the values satisfy the given predicate.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;trait&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='c1'&gt;// ...&lt;/span&gt;
  &lt;span class='k'&gt;private&lt;/span&gt; &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;N&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='mi'&gt;10000&lt;/span&gt;
  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;pr&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;predicate&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;A&lt;/span&gt; &lt;span class='o'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='nc'&gt;Boolean&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='k'&gt;this&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;sample&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;N&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;count&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;predicate&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;toDouble&lt;/span&gt; &lt;span class='o'&gt;/&lt;/span&gt; &lt;span class='n'&gt;N&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;scala&amp;gt; uniform.pr(_ &amp;lt; 0.4)
res2: Double = 0.4015&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It works! It&amp;#8217;s not exact, but it&amp;#8217;s close enough.&lt;/p&gt;

&lt;p&gt;Now I need two ways to transform a distribution.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;trait&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='c1'&gt;// ...&lt;/span&gt;
  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;given&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;predicate&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;A&lt;/span&gt; &lt;span class='o'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='nc'&gt;Boolean&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='nd'&gt;@tailrec&lt;/span&gt;
    &lt;span class='k'&gt;override&lt;/span&gt; &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;get&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;a&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;self&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;get&lt;/span&gt;
      &lt;span class='k'&gt;if&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;predicate&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;a&lt;/span&gt;&lt;span class='o'&gt;))&lt;/span&gt; &lt;span class='n'&gt;a&lt;/span&gt; &lt;span class='k'&gt;else&lt;/span&gt; &lt;span class='k'&gt;this&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;get&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;

  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;repeat&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;n&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='k'&gt;override&lt;/span&gt; &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;get&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='nc'&gt;List&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;fill&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;n&lt;/span&gt;&lt;span class='o'&gt;)(&lt;/span&gt;&lt;span class='n'&gt;self&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;get&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;given&lt;/code&gt; creates a new distribution by sampling from the original distribution and discarding values that don&amp;#8217;t match the given predicate. &lt;code&gt;repeat&lt;/code&gt; creates a &lt;code&gt;Distribution[List[A]]&lt;/code&gt; from a &lt;code&gt;Distribution[A]&lt;/code&gt; by producing samples that are lists of samples from the original distributions.&lt;/p&gt;

&lt;p&gt;OK, now one more distribution:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;discreteUniform&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;](&lt;/span&gt;&lt;span class='n'&gt;values&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Iterable&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;vec&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;values&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;toVector&lt;/span&gt;
  &lt;span class='n'&gt;uniform&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;vec&lt;/span&gt;&lt;span class='o'&gt;((&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;vec&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;length&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;toInt&lt;/span&gt;&lt;span class='o'&gt;))&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let&amp;#8217;s see how all this works.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val die = discreteUniform(1 to 6)
die: Distribution[Int] = &amp;lt;distribution&amp;gt;

scala&amp;gt; die.sample(10)
res0: List[Int] = List(1, 5, 6, 5, 4, 3, 5, 4, 1, 1)

scala&amp;gt; die.pr(_ == 4)
res1: Double = 0.1668

scala&amp;gt; die.given(_ % 2 == 0).pr(_ == 4)
res2: Double = 0.3398

scala&amp;gt; val dice = die.repeat(2).map(_.sum)
dice: Distribution[Int] = &amp;lt;distribution&amp;gt;

scala&amp;gt; dice.pr(_ == 7)
res3: Double = 0.1653

scala&amp;gt; dice.pr(_ == 11)
res4: Double = 0.0542

scala&amp;gt; dice.pr(_ &amp;lt; 4)
res5: Double = 0.0811&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Neat! This is getting useful.&lt;/p&gt;

&lt;p&gt;OK I&amp;#8217;m tired of looking at individual probabilities. What I really want is a way to visualize the entire distribution.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; dice.hist
 2  2.67% ##
 3  5.21% #####
 4  8.48% ########
 5 11.52% ###########
 6 13.78% #############
 7 16.61% ################
 8 13.47% #############
 9 11.17% ###########
10  8.66% ########
11  5.64% #####
12  2.79% ##&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;#8217;s better. &lt;code&gt;hist&lt;/code&gt; pulls 10,000 samples from the distribution, buckets them, counts the size of the buckets, and finds a good way to display it. (The code is tedious so I&amp;#8217;m not going to reproduce it here.)&lt;/p&gt;

&lt;h3 id='dont_tell_anyone_its_a_monad'&gt;Don&amp;#8217;t tell anyone it&amp;#8217;s a monad&lt;/h3&gt;

&lt;p&gt;Another way to represent two die rolls is to sample from &lt;code&gt;die&lt;/code&gt; twice and add the samples.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val dice = die.map(d1 =&amp;gt; die.map(d2 =&amp;gt; d1 + d2))
dice: Distribution[Distribution[Int]] = &amp;lt;distribution&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But wait, that gives me a &lt;code&gt;Distribution[Distribution[Int]]&lt;/code&gt;, which is nonsense. Fortunately there&amp;#8217;s an easy fix.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;trait&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='c1'&gt;// ...&lt;/span&gt;
  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;flatMap&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;B&lt;/span&gt;&lt;span class='o'&gt;](&lt;/span&gt;&lt;span class='n'&gt;f&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;A&lt;/span&gt; &lt;span class='o'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;B&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;B&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;B&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='k'&gt;override&lt;/span&gt; &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;get&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;f&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;self&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;get&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;get&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let&amp;#8217;s try it now.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val dice = die.flatMap(d1 =&amp;gt; die.map(d2 =&amp;gt; d1 + d2))
dice: Distribution[Int] = &amp;lt;distribution&amp;gt;

scala&amp;gt; dice.hist
 2  2.71% ##
 3  5.17% #####
 4  8.23% ########
 5 11.54% ###########
 6 14.04% ##############
 7 16.67% ################
 8 13.53% #############
 9 10.97% ##########
10  8.81% ########
11  5.62% #####
12  2.71% ##&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It worked!&lt;/p&gt;

&lt;p&gt;The definition of &lt;code&gt;dice&lt;/code&gt; can be re-written using Scala&amp;#8217;s &lt;code&gt;for&lt;/code&gt;-comprehension syntax:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;dice&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;d1&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;die&lt;/span&gt;
  &lt;span class='n'&gt;d2&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;die&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt; &lt;span class='k'&gt;yield&lt;/span&gt; &lt;span class='n'&gt;d1&lt;/span&gt; &lt;span class='o'&gt;+&lt;/span&gt; &lt;span class='n'&gt;d2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is really nice. The &lt;code&gt;&amp;lt;-&lt;/code&gt; notation can be read as sampling a value from a distribution. &lt;code&gt;d1&lt;/code&gt; and &lt;code&gt;d2&lt;/code&gt; are samples from &lt;code&gt;die&lt;/code&gt; and both have type &lt;code&gt;Int&lt;/code&gt;. &lt;code&gt;d1 + d2&lt;/code&gt; is a sample from &lt;code&gt;dice&lt;/code&gt;, the distribution I&amp;#8217;m creating.&lt;/p&gt;

&lt;p&gt;In other words, I&amp;#8217;m creating a new distribution by writing code that constructs a single sample of the distribution from individual samples of other distributions. This is pretty handy! Lots of common distributions can be constructed this way. (More on that soon!)&lt;/p&gt;

&lt;h3 id='monty_hall'&gt;Monty Hall&lt;/h3&gt;

&lt;p&gt;I think it would be fun to model the &lt;a href='http://en.wikipedia.org/wiki/Monty_Hall_problem'&gt;Monty Hall problem&lt;/a&gt;.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;montyHall&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[(&lt;/span&gt;&lt;span class='kt'&gt;Int&lt;/span&gt;, &lt;span class='kt'&gt;Int&lt;/span&gt;&lt;span class='o'&gt;)]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;doors&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;1&lt;/span&gt; &lt;span class='n'&gt;to&lt;/span&gt; &lt;span class='mi'&gt;3&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;toSet&lt;/span&gt;
  &lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;prize&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;discreteUniform&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;doors&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;   &lt;span class='c1'&gt;// The prize is placed randomly&lt;/span&gt;
    &lt;span class='n'&gt;choice&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;discreteUniform&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;doors&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;  &lt;span class='c1'&gt;// You choose randomly&lt;/span&gt;
    &lt;span class='n'&gt;opened&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;discreteUniform&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;doors&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt; &lt;span class='n'&gt;prize&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt; &lt;span class='n'&gt;choice&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;   &lt;span class='c1'&gt;// Monty opens one of the other doors&lt;/span&gt;
    &lt;span class='n'&gt;switch&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;discreteUniform&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;doors&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt; &lt;span class='n'&gt;choice&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt; &lt;span class='n'&gt;opened&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;  &lt;span class='c1'&gt;// You switch to the unopened door&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt; &lt;span class='k'&gt;yield&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;prize&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;switch&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This code constructs a distribution of pairs representing the door the prize is behind and the door you switched to. Let&amp;#8217;s see how often those are the same door:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; montyHall.pr{ case (prize, switch) =&amp;gt; prize == switch }
res0: Double = 0.6671&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just as expected. Lots of people have a hard time believing the explanation behind why this is correct, but there&amp;#8217;s no arguing with just trying it 10,000 times!&lt;/p&gt;

&lt;h3 id='hth_vs_htt'&gt;HTH vs HTT&lt;/h3&gt;

&lt;p&gt;Another fun problem: if you flip a coin repeatedly, which pattern do you expect to see first, heads-tails-heads or heads-tails-tails?&lt;/p&gt;

&lt;p&gt;First I need the following method:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;trait&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='c1'&gt;// ...&lt;/span&gt;
  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;until&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;pred&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='nc'&gt;Boolean&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='k'&gt;override&lt;/span&gt; &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;get&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='nd'&gt;@tailrec&lt;/span&gt;
      &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;helper&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;sofar&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;List&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
        &lt;span class='k'&gt;if&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;pred&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;sofar&lt;/span&gt;&lt;span class='o'&gt;))&lt;/span&gt; &lt;span class='n'&gt;sofar&lt;/span&gt;
        &lt;span class='k'&gt;else&lt;/span&gt; &lt;span class='n'&gt;helper&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;self&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;get&lt;/span&gt; &lt;span class='o'&gt;::&lt;/span&gt; &lt;span class='n'&gt;sofar&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
      &lt;span class='o'&gt;}&lt;/span&gt;
      &lt;span class='n'&gt;helper&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='nc'&gt;Nil&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;until&lt;/code&gt; samples from the distribution, adding the samples to the &lt;em&gt;front&lt;/em&gt; of the list until the list satisfies some predicate. A single sample from the resulting distribution is a list that satisfies the predicate.&lt;/p&gt;

&lt;p&gt;Now I can do:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;hth&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;bernoulli&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='mf'&gt;0.5&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;until&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;take&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;3&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;==&lt;/span&gt; &lt;span class='nc'&gt;List&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kc'&gt;true&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='kc'&gt;false&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='kc'&gt;true&lt;/span&gt;&lt;span class='o'&gt;)).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;length&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;htt&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;bernoulli&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='mf'&gt;0.5&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;until&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;take&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;3&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;==&lt;/span&gt; &lt;span class='nc'&gt;List&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kc'&gt;false&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='kc'&gt;false&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='kc'&gt;true&lt;/span&gt;&lt;span class='o'&gt;)).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;length&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Looking at the distributions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; hth.hist
 3 11.63% ###########
 4 12.43% ############
 5  9.50% #########
 6  7.82% #######
 7  7.31% #######
 8  6.51% ######
 9  5.41% #####
10  4.57% ####
11  4.56% ####
12  3.78% ###
13  3.44% ###
14  3.04% ###
15  2.52% ##
16  2.08% ##
17  1.76% #
18  1.70% #
19  1.34% #
20  1.34% #

scala&amp;gt; htt.hist
 3 12.94% ############
 4 12.18% ############
 5 12.48% ############
 6 11.29% ###########
 7  9.88% #########
 8  7.67% #######
 9  6.07% ######
10  5.32% #####
11  4.18% ####
12  3.51% ###
13  2.78% ##
14  2.23% ##
15  1.75% #
16  1.40% #
17  1.21% #
18  0.92% 
19  0.78% 
20  0.60% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Eyeballing it, it appears that HTT is likely to occur earlier than HTH. (How can this be? Excercise for the reader!) But I&amp;#8217;d like to get a more concrete answer than that. What I want to know is how many flips you expect to see before seeing either pattern. So let me add a method to compute the expected value of a distribution:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;trait&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='c1'&gt;// ...&lt;/span&gt;
  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;ev&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='nc'&gt;Stream&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;fill&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;N&lt;/span&gt;&lt;span class='o'&gt;)(&lt;/span&gt;&lt;span class='n'&gt;self&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;get&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;sum&lt;/span&gt; &lt;span class='o'&gt;/&lt;/span&gt; &lt;span class='n'&gt;N&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Hm, that &lt;code&gt;.sum&lt;/code&gt; is not going to work for all &lt;code&gt;A&lt;/code&gt;s. I mean, &lt;code&gt;A&lt;/code&gt; could certainly be &lt;code&gt;Boolean&lt;/code&gt;, as in the case of the &lt;code&gt;bernoulli&lt;/code&gt; distribution (what is the expected value of a coin flip?). So I need to constrain &lt;code&gt;A&lt;/code&gt; to &lt;code&gt;Double&lt;/code&gt; for the purposes of this method.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;trait&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='c1'&gt;// ...&lt;/span&gt;
  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;ev&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;implicit&lt;/span&gt; &lt;span class='n'&gt;toDouble&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;A&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;:&lt;/span&gt;&lt;span class='kt'&gt;&amp;lt;&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='nc'&gt;Stream&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;fill&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;N&lt;/span&gt;&lt;span class='o'&gt;)(&lt;/span&gt;&lt;span class='n'&gt;toDouble&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;self&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;get&lt;/span&gt;&lt;span class='o'&gt;)).&lt;/span&gt;&lt;span class='n'&gt;sum&lt;/span&gt; &lt;span class='o'&gt;/&lt;/span&gt; &lt;span class='n'&gt;N&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;scala&amp;gt; hth.ev
&amp;lt;console&amp;gt;:15: error: Cannot prove that Int &amp;lt;:&amp;lt; Double.
              hth.ev&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Perfect. You know, it really bothered me when I first learned that the expected value of a die roll is 3.5. Requiring an explicit conversion to &lt;code&gt;Double&lt;/code&gt; before computing the expected value of any distribution makes that fact a lot more palatable.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; hth.map(_.toDouble).ev
res0: Double = 9.9204

scala&amp;gt; htt.map(_.toDouble).ev
res1: Double = 7.9854&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There we go, empirical confirmation that HTT is expected to appear after 8 flips and HTH after 10 flips.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m curious. Suppose you and I played a game where we each flipped a coin until I got HTH and you got HTT. Then whoever took more flips pays the other person the difference. What is the expected value of this game? Is it 2? It doesn&amp;#8217;t have to be 2, does it? Maybe the distributions are funky in some way that makes the difference in expected value 2 but the expected difference something else.&lt;/p&gt;

&lt;p&gt;Well, easy enough to try it.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;diff&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;me&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;hth&lt;/span&gt;
  &lt;span class='n'&gt;you&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;htt&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt; &lt;span class='k'&gt;yield&lt;/span&gt; &lt;span class='n'&gt;me&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt; &lt;span class='n'&gt;you&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;scala&amp;gt; diff.map(_.toDouble).ev
res3: Double = 1.9976&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Actually, it does have to be 2. Expectation is linear!&lt;/p&gt;

&lt;h3 id='unbiased_rounding'&gt;Unbiased rounding&lt;/h3&gt;

&lt;p&gt;At Foursquare we have some code that computes how much our customers owe us, and charges them for it. Our payments provider, &lt;a href='http://www.stripe.com'&gt;Stripe&lt;/a&gt;, only allows us to charge in whole cents, but for complicated business reasons sometimes a customer owes us fractional cents. (No, this is not an Office Space or Superman III reference.) So we just round to the nearest whole cent (actually we use unbiased rounding, or &lt;a href='http://en.wikipedia.org/wiki/Rounding#Round_half_to_even'&gt;banker&amp;#8217;s rounding&lt;/a&gt;, which rounds 0.5 cents up half the time and down half the time).&lt;/p&gt;

&lt;p&gt;Because we&amp;#8217;re paranoid and also curious, we want to know how much money we are losing or gaining due to rounding. Let&amp;#8217;s say that during some period of time we saw that we rounded 125 times, and the sum of all the roundings totaled +8.5 cents. That kinda seems like a lot, but it could happen by chance. If fractional cents are uniformly distributed, what is the probability that you would see a difference that big after 125 roundings?&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s find out.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val d = uniform.map(x =&amp;gt; if (x &amp;lt; 0.5) -x else 1.0-x).repeat(125).map(_.sum)
d: Distribution[Double] = &amp;lt;distribution&amp;gt;

scala&amp;gt; d.hist
-10.0  0.02% 
 -9.0  0.20% 
 -8.0  0.57% 
 -7.0  1.32% #
 -6.0  2.15% ##
 -5.0  3.75% ###
 -4.0  5.12% #####
 -3.0  7.83% #######
 -2.0 10.58% ##########
 -1.0 11.44% ###########
  0.0 12.98% ############
  1.0 11.57% ###########
  2.0 10.68% ##########
  3.0  7.73% #######
  4.0  5.70% #####
  5.0  3.88% ###
  6.0  2.32% ##
  7.0  1.21% #
  8.0  0.65% 
  9.0  0.25% 
 10.0  0.06% &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s the distribution. Each instance is either a loss of &lt;code&gt;x&lt;/code&gt; if &lt;code&gt;x &amp;lt; 0.5&lt;/code&gt; or a gain of &lt;code&gt;1.0-x&lt;/code&gt;. Repeat 125 times and sum it all up to get the total gain or loss from rounding.&lt;/p&gt;

&lt;p&gt;Now what&amp;#8217;s the probability that we&amp;#8217;d see a total greater than 8.5 cents? (Or less than -8.5 cents — a loss of 8.5 cents would be equally surprising.)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; d.pr(x =&amp;gt; math.abs(x) &amp;gt; 8.5)
res0: Double = 0.0098&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pretty unlikely, about 1%! So the distribution of fractional cents is probably not uniform. We should maybe look into that.&lt;/p&gt;

&lt;h3 id='the_normal_distribution'&gt;The normal distribution&lt;/h3&gt;

&lt;p&gt;One last example. It turns out the &lt;a href='http://en.wikipedia.org/wiki/Normal_distribution'&gt;normal distribution&lt;/a&gt; can be approximated pretty well by summing 12 uniformly distributed random variables and subtracting 6. In code:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;normal&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;uniform&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;repeat&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;12&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;_&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;sum&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt; &lt;span class='mi'&gt;6&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here&amp;#8217;s what it looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; normal.hist
-3.50  0.04% 
-3.00  0.18% 
-2.50  0.80% 
-2.00  2.54% ##
-1.50  6.62% ######
-1.00 12.09% ############
-0.50 17.02% #################
 0.00 20.12% ####################
 0.50 17.47% #################
 1.00 12.63% ############
 1.50  6.85% ######
 2.00  2.61% ##
 2.50  0.82% 
 3.00  0.29% 
 3.50  0.01% 

scala&amp;gt; normal.pr(x =&amp;gt; math.abs(x) &amp;lt; 1)
res0: Double = 0.6745

scala&amp;gt; normal.pr(x =&amp;gt; math.abs(x) &amp;lt; 2)
res1: Double = 0.9566

scala&amp;gt; normal.pr(x =&amp;gt; math.abs(x) &amp;lt; 3)
res2: Double = 0.9972&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I believe it! One more check though.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;trait&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;A&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='c1'&gt;// ...&lt;/span&gt;
  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;variance&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;implicit&lt;/span&gt; &lt;span class='n'&gt;toDouble&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;A&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;:&lt;/span&gt;&lt;span class='kt'&gt;&amp;lt;&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;mean&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;this&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;ev&lt;/span&gt;
    &lt;span class='k'&gt;this&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='n'&gt;math&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;pow&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;toDouble&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt; &lt;span class='n'&gt;mean&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;2&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
    &lt;span class='o'&gt;}).&lt;/span&gt;&lt;span class='n'&gt;ev&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;

  &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;stdev&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;implicit&lt;/span&gt; &lt;span class='n'&gt;toDouble&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;A&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;:&lt;/span&gt;&lt;span class='kt'&gt;&amp;lt;&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Double&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;math&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;sqrt&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;this&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;variance&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The variance &lt;script type='math/tex'&gt;\sigma^2&lt;/script&gt; of a random variable &lt;script type='math/tex'&gt;X&lt;/script&gt; with mean &lt;script type='math/tex'&gt;\mu&lt;/script&gt; is &lt;script type='math/tex'&gt;E[(X-\mu)^2]&lt;/script&gt;, and the standard deviation &lt;script type='math/tex'&gt;\sigma&lt;/script&gt; is just the square root of the variance.&lt;/p&gt;

&lt;p&gt;And now:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; normal.stdev
res0: Double = 0.9990012220368588&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Perfect.&lt;/p&gt;

&lt;p&gt;This is a great approximation and all, but &lt;code&gt;java.util.Random&lt;/code&gt; actually provides a &lt;code&gt;nextGaussian&lt;/code&gt; method, so for the sake of performance I&amp;#8217;m just going to use that.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;normal&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nc'&gt;Distribution&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Double&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;override&lt;/span&gt; &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;get&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;rand&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;nextGaussian&lt;/span&gt;&lt;span class='o'&gt;()&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id='conclusion'&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;The frequentist approach lines up really well with my intuitions about probability. And Scala&amp;#8217;s &lt;code&gt;for&lt;/code&gt;-comprehensions provide a suggestive syntax for constructing new random variables from existing ones. So I&amp;#8217;m going to continue to explore various concepts in probability and statistics using these tools.&lt;/p&gt;

&lt;p&gt;In later posts I&amp;#8217;ll try to model Bayesian inference, Markov chains, the Central Limit Theorem, probablistic graphical models, and a bunch of related distributions.&lt;/p&gt;

&lt;p&gt;All of the code for this is on &lt;a href='http://github.com/jliszka/probability-monad'&gt;github&lt;/a&gt;.&lt;/p&gt;</description>
                <link>http://jliszka.github.io/2013/08/12/a-frequentist-approach-to-probability.html</link>
                <guid>http://jliszka.github.io/2013/08/12/a-frequentist-approach-to-probability</guid>
                <pubDate>2013-08-12T00:00:00-04:00</pubDate>
        </item>


</channel>
</rss>
